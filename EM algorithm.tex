\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,amsfonts,dsfont,bm} 
\usepackage[margin=2.5cm]{geometry}

%opening
\title{EM algorithm}
\author{}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Context}

We have observed data $Y$ and unobserved data $Z$. The goal is to compute the likelihood of the data, $p_\theta (Y)$.
\[ \log(p_\theta (Y)) = \log (p_\theta (Y,Z)) - \log(p_\theta (Z|Y)).\]

The advantage of this is to link $p_\theta(Y)$ with $p_\theta (Y,Z)$ which is easier to compute in general. We now take the expectation,
conditioned on the data $Y$ :
\[ \log(p_\theta (Y) = \mathds{E}_\theta \left(\log(p_\theta (Y,Z))|Y \right) \underbrace{- \mathds{E}_\theta \left(\log(p_\theta (Y|Z))|Y \right)}_{\text{\normalsize{$\mathcal{H}(p_\theta (Y|Z))$}}} \]

\paragraph{E step :}
In this step, we must be very cautious on to what is varying. During this computation, data $Y$ is fixed, leading the entropy term to be fixed as well.
We compute this expectation with fixed parameters, only the hidden part is varying. So from now on we are interested in the first term, which is the conditional expectation of the complete log-likelihood. \\

\paragraph{M step :}
We consider that  $\theta$ is varying and we want to maximise the expectation with respect to these parameters.


\section{Example for Gaussian mixture models}
The data Y is an array of dimension $n\times d$, being for example $n$ samples of $d$ different species. Let
$Y_i$ be the $i^{th}$ row (i.e. sample) of Y. We then assume that data from the species follow a mixture of K multivariate Gaussians :
\[\forall k\in\{1,..K\}, f_k(Y_i) = \frac{1}{\sqrt{(2\pi)^d\det(\Sigma_k)}}\exp\left(-\frac{1}{2}(Y_i - \mu_k)^T\Sigma_k^{-1}(Y_i - \mu_k)\right)\]
With $d$ being the size of both $Y_i$ and $\mu_k$. The covariance matrix $\Sigma_k$ has size $d\times d$.\\

\paragraph{E step :}



\begin{align*}
\mathds{E}_\theta (\log(p_\theta(Y,Z)|Y) &= \sum_{i,k} \mathds{1}_{\{Z_i = k\}} \times \log(\pi_k f_k(Y_i)|Y)\\
&= \sum_{i,k} \mathds{E}_\theta \left( \mathds{1}_{\{Z_i = k\}} |Y_i\right)[\log(\pi_k) + \log(f_k(Y_i)) ]\\
\intertext{With $\tau_{ik} = \frac{\pi_k f_k(Y_i)}{\sum_{l} \pi_l f_l(Y_i)}$ :}
&= \sum_{i,k}\tau_{ik}[\log(\pi_k) + \log(f_k(Y_i)) ] \\
&= \sum_{i,k}\tau_{ik}\left[\log(\pi_k)-\frac{1}{2}\log\left((2\pi)^d\det(\Sigma_k)\right) - \frac{1}{2}(Y_i - \mu_k)^T\Sigma_k^{-1}(Y_i - \mu_k)\right] \\
\end{align*}


\paragraph{M step :}
Maximising the last expression, we get after some algebraic manipulations :
\begin{itemize}
\item \large{$\hat{\mu}_k = \frac{\sum_i \tau_{ik} y_i}{\sum_i \tau_{ik}}$}\normalsize
\item \large{$\hat{\Sigma}_k = \frac{\sum_i \tau_{ik} (y_i-\mu_k)^T(y_i-\mu_k)}{\sum_i \tau_{ik}}$}\normalsize
\item \large{$\hat{\pi}_k = \frac{1}{n} \sum_i \tau_{ik}$}
\end{itemize}
\end{document}
