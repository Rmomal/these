\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,amsfonts,dsfont,bm} 
\usepackage[margin=2.5cm]{geometry}

%opening
\title{EM algorithm}
\author{}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\maketitle



\section{Context}

We have observed data $Y$ and unobserved data $Z$. The goal is to compute the likelihood of the data, $p_\theta (Y)$.
\[ \log(p_\theta (Y)) = \log (p_\theta (Y,Z)) - \log(p_\theta (Z|Y)).\]

The advantage of this is to link $p_\theta(Y)$ with $p_\theta (Y,Z)$ which is easier to compute in general. We now take the expectation,
conditioned on the data $Y$ :
\[ \log(p_\theta (Y) = \mathds{E}_\theta \left(\log(p_\theta (Y,Z))|Y \right) \underbrace{- \mathds{E}_\theta \left(\log(p_\theta (Y|Z))|Y \right)}_{\text{\normalsize{$\mathcal{H}(p_\theta (Y|Z))$}}} \]

\paragraph{E step :}
In this step, we must be very cautious on to what is varying. During this computation, data $Y$ is fixed, leading the entropy term to be fixed as well.
We compute this expectation with fixed parameters, only the hidden part is varying. So from now on we are interested in the first term, which is the conditional expectation of the complete log-likelihood. \\

\paragraph{M step :}
We consider that  $\theta$ is varying and we want to maximise the expectation with respect to these parameters.


\section{Example for Gaussian mixture models}
The data Y is an array of dimension $n\times d$, being for example $n$ samples of $d$ different species. Let
$Y_i$ be the $i^{th}$ row (i.e. sample) of Y. We then assume that data from the species follow a mixture of K multivariate Gaussians :
\[\forall k\in\{1,..K\}, f_k(Y_i) = \frac{1}{\sqrt{(2\pi)^d\det(\Sigma_k)}}\exp\left(-\frac{1}{2}(Y_i - \mu_k)^T\Sigma_k^{-1}(Y_i - \mu_k)\right)\]
With $d$ being the size of both $Y_i$ and $\mu_k$. The covariance matrix $\Sigma_k$ has size $d\times d$.\\

\paragraph{E step :}



\begin{align*}
\mathds{E}_\theta (\log(p_\theta(Y,Z)|Y) &= \sum_{i,k} \mathds{1}_{\{Z_i = k\}} \times \log(\pi_k f_k(Y_i)|Y)\\
&= \sum_{i,k} \mathds{E}_\theta \left( \mathds{1}_{\{Z_i = k\}} |Y_i\right)[\log(\pi_k) + \log(f_k(Y_i)) ]\\
\intertext{With $\tau_{ik} = \frac{\pi_k f_k(Y_i)}{\sum_{l} \pi_l f_l(Y_i)}$ :}
&= \sum_{i,k}\tau_{ik}[\log(\pi_k) + \log(f_k(Y_i)) ] \\
&= \sum_{i,k}\tau_{ik}\left[\log(\pi_k)-\frac{1}{2}\log\left((2\pi)^d\det(\Sigma_k)\right) - \frac{1}{2}(Y_i - \mu_k)^T\Sigma_k^{-1}(Y_i - \mu_k)\right] \\
\end{align*}


\paragraph{M step :}
Maximising the last expression, we get after some algebraic manipulations :
\begin{itemize}
\item \large{$\hat{\mu}_k = \frac{\sum_i \tau_{ik} y_i}{\sum_i \tau_{ik}}$}\normalsize
\item \large{$\hat{\Sigma}_k = \frac{\sum_i \tau_{ik} (y_i-\mu_k)^T(y_i-\mu_k)}{\sum_i \tau_{ik}}$}\normalsize
\item \large{$\hat{\pi}_k = \frac{1}{n} \sum_i \tau_{ik}$}
\end{itemize}

\section{Example for mixtures of Gaussian Dependence Trees}
Let $T$ be a gaussian dependence tree. We are considering a mixture of hidden trees, $k$ an $l$  are nodes of the trees (
i.e. variables or species).\\

\[ \mathds{P}(T) = \frac{1}{B}\prod_{k,l\in T} \beta_{kl} \text{ , with } B = \sum_T \prod_{k,l\in T} \beta_{kl} \]
\subsection{E step :}
\[ \mathds{P}(Y,T) = \mathds{P}(T)\times\mathds{P}(Y|T)\]
\begin{align*}
 \log(\mathds{P}(Y,T)) &= \sum_{(k,l)\in T} \log(\beta_{kl})  + \sum_{(k,l)\in T} \psi_{kl}(Y)- \log (B)\\
 &=\sum_{k,l} \mathds{1}_{\{(k,l) \in T\}} \left(\log(\beta_{kl})  +  \psi_{kl}(Y)\right)- \log (B)
\end{align*}
 
\[ \mathcal{Q} = \mathds{E}_\theta[\log(\mathds{P}(Y,T))] = \sum_{k,l}  \mathds{P}((k,l)\in T | Y) \times \left( \log(\beta_{kl}) + \psi_{kl}(Y) \right)
 -\log(B)\]
 
Keeping in mind that only the hidden part varies for now, we want to maximise $\mathcal{Q}$. We use the following result :

\begin{equation}
 \label{res}
q^ * =  \argmax_{q_1,...q_n}  \sum_i q_i \log(p_i) \Rightarrow q^ * = (p_1,...p_n) \\
\end{equation}

We define $\tau_{kl} = \beta_{kl}\exp(\psi_{kl}(Y))$, such as :

\[\mathds{P}((k,l)\in T | Y) \times( \log(\beta_{kl}) + \psi_{kl}(Y)) = 
\mathds{P}((k,l)\in T | Y) \times \log(\tau_{kl}) \]

Following \ref{res}, we have :
\[\argmax_{\mathds{P}((k,l)\in T | Y)}  \sum_{k,l}  \mathds{P}((k,l)\in T | Y)\times \log(\tau_{kl}) =  \tau_{kl}.\]
\subsection{M step :\\}


\subsubsection{B}
Defining $\Delta$ as the Laplacian of the matrix of the weights $\beta_{kl}$, in can be shown that :

\begin{align*}
B &= \sum_{T \in \mathcal{T}} \prod_{k,l\in T} \beta_{kl}\\
 &=\beta_{kl}|\Delta^{\{k,l\}}|
\end{align*}
Where $|\Delta^{\{k,l\}}|$ is the determinant of the Laplacien where we have removed the columns and rows corresponding to
the edges $k$ an $l$.

\[\mathcal{Q} = \sum_{k,l} \beta_{kl} \exp(\psi_{kl}(Y))\left[\log(\beta_{kl}) + \psi_{kl}(Y)\right] - \log(B) \]


\[ \frac{\partial \mathcal{Q}}{\partial \beta_{kl}} = \beta_{kl}\left[\log(\beta_{kl}) + \psi_{kl}(Y)\right] + \exp(\psi_{kl}) -\frac{1}{\beta_{kl}}\]\\

Therefore, we get :
\[ \frac{\partial \mathcal{Q}}{\partial \beta_{kl}}  = 0 \Leftrightarrow \beta_{kl}^2\left[\log(\beta_{kl}) + \psi_{kl}(Y)\right] + \beta_{kl} \exp(\psi_{kl}(Y)) -1 =0\]


\end{document}
