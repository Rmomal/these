\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,amsfonts,dsfont,bm} 
\usepackage[margin=2.5cm]{geometry}

%opening
\title{EM algorithm}
\author{}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\maketitle



\section{Context}

We have observed data $Y$ and unobserved data $Z$. The goal is to compute the likelihood of the data, $p_\theta (Y)$.
\[ \log(p_\theta (Y)) = \log (p_\theta (Y,Z)) - \log(p_\theta (Z|Y)).\]

The advantage of this is to link $p_\theta(Y)$ with $p_\theta (Y,Z)$ which is easier to compute in general. We now take the expectation,
conditioned on the data $Y$ :
\[ \log(p_\theta (Y) = \mathds{E}_\theta \left(\log(p_\theta (Y,Z))|Y \right) \underbrace{- \mathds{E}_\theta \left(\log(p_\theta (Y|Z))|Y \right)}_{\text{\normalsize{$\mathcal{H}(p_\theta (Y|Z))$}}} \]

\paragraph{E step :}
In this step, we must be very cautious on to what is varying. During this computation, data $Y$ is fixed, leading the entropy term to be fixed as well.
We compute this expectation with fixed parameters, only the hidden part is varying. So from now on we are interested in the first term, which is the conditional expectation of the complete log-likelihood. \\

\paragraph{M step :}
We consider that  $\theta$ is varying and we want to maximise the expectation with respect to these parameters.


\section{Example for Gaussian mixture models}
The data Y is an array of dimension $n\times d$, being for example $n$ samples of $d$ different species. Let
$Y_i$ be the $i^{th}$ row (i.e. sample) of Y. We then assume that data from the species follow a mixture of K multivariate Gaussians :
\[\forall k\in\{1,..K\}, f_k(Y_i) = \frac{1}{\sqrt{(2\pi)^d\det(\Sigma_k)}}\exp\left(-\frac{1}{2}(Y_i - \mu_k)^T\Sigma_k^{-1}(Y_i - \mu_k)\right)\]
With $d$ being the size of both $Y_i$ and $\mu_k$. The covariance matrix $\Sigma_k$ has size $d\times d$.\\

\paragraph{E step :}



\begin{align*}
\log(p_\theta(Y,Z)) &= \sum_{i,k} \mathds{1}_{\{Z_i = k\}} \times \log(\pi_k f_k(Y_i)|Y)\\
\mathds{E}_\theta (\log(p_\theta(Y,Z))|Y)&= \sum_{i,k} \mathds{E}_\theta \left( \mathds{1}_{\{Z_i = k\}} |Y_i\right)[\log(\pi_k) + \log(f_k(Y_i)) ]\\
\intertext{We can estimate the expectation with $\tau_{ik} = \frac{\pi_k f_k(Y_i)}{\sum_{l} \pi_l f_l(Y_i)}$ :}
&= \sum_{i,k}\tau_{ik}[\log(\pi_k) + \log(f_k(Y_i)) ] \\
&= \sum_{i,k}\tau_{ik}\left[\log(\pi_k)-\frac{1}{2}\log\left((2\pi)^d\det(\Sigma_k)\right) - \frac{1}{2}(Y_i - \mu_k)^T\Sigma_k^{-1}(Y_i - \mu_k)\right] \\
\end{align*}


\paragraph{M step :}
Maximising the last expression, we get after some algebraic manipulations :
\begin{itemize}
\item \large{$\hat{\mu}_k = \frac{\sum_i \tau_{ik} y_i}{\sum_i \tau_{ik}}$}\normalsize
\item \large{$\hat{\Sigma}_k = \frac{\sum_i \tau_{ik} (y_i-\mu_k)^T(y_i-\mu_k)}{\sum_i \tau_{ik}}$}\normalsize
\item \large{$\hat{\pi}_k = \frac{1}{n} \sum_i \tau_{ik}$}
\end{itemize}

\section{Example for mixtures of Gaussian Dependence Trees}
Let $T$ be a gaussian dependence tree. We are considering a mixture of hidden trees, $k$ an $l$  are nodes of the trees (
i.e. variables or species).\\

\[ \mathds{P}(T) = \frac{1}{B}\prod_{k,l\in T} \beta_{kl} \text{ , with } B = \sum_T \prod_{k,l\in T} \beta_{kl} \]
\begin{align*}
\mathds{P}(Y|T) &=\mathds{P}(y_1|T)\prod_{i=2}^n \frac{\mathds{P}(y_i,y_{a_i}|T)}{\mathds{P}(y_{a_i}|T)}\\
&=\prod_{i=2}^n \mathds{P}(y_i|T)\prod_{i=2}^n \frac{\mathds{P}(y_i,y_{a_i}|T)}{\mathds{P}(y_i|T)\times \mathds{P}(y_{a_i}|T)}\\
&=\underbrace{\prod_{i=2}^n \mathds{P}(y_i|T)}_{\text{A}}\prod_{k,l \in T}\psi_{kl}(Y)
\end{align*}

We know (cf. Chow gaussian document) that \[\log(A) = \sum_{i=1}^n-\frac{1}{2}\left(\log(2\pi\sigma_i^2)-\frac{x_i^2}{\sigma_i^2}\right),\]
and this quantity is independant from the tree structure. We also know the explicit form of $\log(\psi_{kl})$ :
\[\log(\psi_{kl}(Y))=\frac{-1}{2}\left(\log\left(1-\frac{\sigma_{kl}^2}{\sigma_k^2 \sigma_{l}^2}\right)+\frac{(y_k^2\sigma_{l}^2+y_{l}^2\sigma_k^2-2\sigma_{kl}y_ky_{l})}
{\det(\Sigma_{kl})}-\left(\frac{y_k^2}{\sigma_k^2}+\frac{y_{l}^2}{\sigma_{l}^2}\right)\right)\]
\subsection{E step :}
\[ \mathds{P}(Y,T) = \mathds{P}(T)\times\mathds{P}(Y|T)\]
\begin{align*}
 \log(\mathds{P}(Y,T)) &= \sum_{(k,l)\in T} \log(\beta_{kl})  + \sum_{(k,l)\in T} \log(\psi_{kl}(Y))- \log (B)+\log(A)\\
 &=\sum_{k,l} \mathds{1}_{\{(k,l) \in T\}} \left(\log(\beta_{kl})  +  \log(\psi_{kl}(Y))\right)- \log (B)+\log(A)
\end{align*}
 Conditional expectation :
\[ \mathds{E}_\theta[\log(\mathds{P}(Y,T))|Y] \\
= \sum_{k,l}  \mathds{P}((k,l)\in T | Y) \times \left[ \log(\beta_{kl}) + \log(\psi_{kl}(Y)) \right]
 -\log(B)+\log(A)\]
 Computation of conditional probability : using Bayes, we specially consider the proportion of trees which contain an edge between the nodes $k$ and $l$.
 \begin{align*}
 \mathds{P}((k,l)\in T | Y)  &= \frac{\sum_{(k,l)\in T} \mathds{P}(T)\mathds{P}(Y|T)}{\sum_{T} \mathds{P}(T)\mathds{P}(Y|T)}\\
 &=1-\frac{\sum_{(k,l)\notin T} \prod \beta_{uv} \prod \psi_{uv}}{\sum_{T} \prod \beta_{uv} \prod \psi_{uv}}
 \end{align*}
 Defining $\Delta$ as the Laplacian $W_{\beta}$, the matrix  of weights $\beta_{kl}$, in can be shown that :

\begin{align*}
B &= \sum_{T \in \mathcal{T}} \prod_{k,l\in T} \beta_{kl}\\
 &=|\Delta^{\{u,v\}}(W_{\beta})|,
\end{align*}
where $\Delta^{\{u,v\}}$ is any minor of the Laplacien. We then get :

\[ \mathds{P}((k,l)\in T | Y) =1-\frac{|\Delta^{\{u,v\}}(W_{\beta}^{-kl}\bigodot\psi)|}{|\Delta^{\{u,v\}}(W_{\beta}\bigodot\psi)|}\]

The last quantity will be computed using the Krishner theorem, allowing for a great gain in computation time


\subsection{M step :\\}
Moving to the M step, the quantity $\tau_{kl} = \mathds{P}((k,l)\in T | Y)$ has been computed and is now considered as fixed.
We maximise the conditional expectation with respect to parameters $\beta_{kl}$.

\[\argmax_{\beta_{kl}} \left\{\sum_{k,l} \tau_{kl}\times \left[ \log(\beta_{kl}) + \log(\psi_{kl}(Y)) \right]
 -\log(B)+\log(A)\right\}\]
We use the following result :

\begin{equation}
 \label{res}
q^ * =  \argmax_{q_1,...q_n}  \sum_i q_i \log(p_i) \Rightarrow q^ * = (p_1,...p_n) \\
\end{equation}




\end{document}
