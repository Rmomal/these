\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,amsfonts,dsfont,bm} 
\usepackage[margin=2.5cm]{geometry}

%opening
\title{EM algorithm}
\author{}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\maketitle



\section{Context}

We have observed data $Y$ and unobserved data $Z$. The goal is to compute the likelihood of the data, $p_\theta (Y)$.
\[ \log(p_\theta (Y)) = \log (p_\theta (Y,Z)) - \log(p_\theta (Z|Y)).\]

The advantage of this is to link $p_\theta(Y)$ with $p_\theta (Y,Z)$ which is easier to compute in general. We now take the expectation,
conditioned on the data $Y$ :
\[ \log(p_\theta (Y) = \mathds{E}_\theta \left(\log(p_\theta (Y,Z))|Y \right) \underbrace{- \mathds{E}_\theta \left(\log(p_\theta (Y|Z))|Y \right)}_{\text{\normalsize{$\mathcal{H}(p_\theta (Y|Z))$}}} \]

\paragraph{E step :}
In this step, we must be very cautious on to what is varying. During this computation, data $Y$ is fixed, leading the entropy term to be fixed as well.
We compute this expectation with fixed parameters, only the hidden part is varying. So from now on we are interested in the first term, which is the conditional expectation of the complete log-likelihood. \\

\paragraph{M step :}
We consider that  $\theta$ is varying and we want to maximise the expectation with respect to these parameters.


\section{Example for Gaussian mixture models}
The data Y is an array of dimension $n\times d$, being for example $n$ samples of $d$ different species. Let
$Y_i$ be the $i^{th}$ row (i.e. sample) of Y. We then assume that data from the species follow a mixture of K multivariate Gaussians :
\[\forall k\in\{1,..K\}, f_k(Y_i) = \frac{1}{\sqrt{(2\pi)^d\det(\Sigma_k)}}\exp\left(-\frac{1}{2}(Y_i - \mu_k)^T\Sigma_k^{-1}(Y_i - \mu_k)\right)\]
With $d$ being the size of both $Y_i$ and $\mu_k$. The covariance matrix $\Sigma_k$ has size $d\times d$.\\

\paragraph{E step :}



\begin{align*}
\log(p_\theta(Y,Z)) &= \sum_{i,k} \mathds{1}_{\{Z_i = k\}} \times \log(\pi_k f_k(Y_i)|Y)\\
\mathds{E}_\theta (\log(p_\theta(Y,Z))|Y)&= \sum_{i,k} \mathds{E}_\theta \left( \mathds{1}_{\{Z_i = k\}} |Y_i\right)[\log(\pi_k) + \log(f_k(Y_i)) ]\\
\intertext{We can estimate the expectation with $\tau_{ik} = \frac{\pi_k f_k(Y_i)}{\sum_{l} \pi_l f_l(Y_i)}$ :}
&= \sum_{i,k}\tau_{ik}[\log(\pi_k) + \log(f_k(Y_i)) ] \\
&= \sum_{i,k}\tau_{ik}\left[\log(\pi_k)-\frac{1}{2}\log\left((2\pi)^d\det(\Sigma_k)\right) - \frac{1}{2}(Y_i - \mu_k)^T\Sigma_k^{-1}(Y_i - \mu_k)\right] \\
\end{align*}


\paragraph{M step :}
Maximising the last expression, we get after some algebraic manipulations :
\begin{itemize}
\item \large{$\hat{\mu}_k = \frac{\sum_i \tau_{ik} y_i}{\sum_i \tau_{ik}}$}\normalsize
\item \large{$\hat{\Sigma}_k = \frac{\sum_i \tau_{ik} (y_i-\mu_k)^T(y_i-\mu_k)}{\sum_i \tau_{ik}}$}\normalsize
\item \large{$\hat{\pi}_k = \frac{1}{n} \sum_i \tau_{ik}$}
\end{itemize}

\section{Example for mixtures of Gaussian Dependence Trees}
Let $T$ be a standard gaussian dependence tree : all means are null and all variances are equal to 1. We are considering a mixture of hidden trees, $k$ an $l$  are nodes of the trees (
i.e. variables or species).\\

\[ \mathds{P}(T) = \frac{1}{B}\prod_{k,l\in T} \beta_{kl} \text{ , with } B = \sum_T \prod_{k,l\in T} \beta_{kl} \]
\begin{align*}
\mathds{P}(Y|T) &=\mathds{P}(y_1|T)\prod_{i=2}^n \frac{\mathds{P}(y_i,y_{a_i}|T)}{\mathds{P}(y_{a_i}|T)}\\
&=\prod_{i=2}^n \mathds{P}(y_i|T)\prod_{i=1}^n \frac{\mathds{P}(y_i,y_{a_i}|T)}{\mathds{P}(y_i|T)\times \mathds{P}(y_{a_i}|T)}\\
&=\underbrace{\prod_{i=1}^n \mathds{P}(y_i|T)}_{\text{A}}\prod_{i=1}^n\prod_{k,l \in T}\psi_{kl}(Y_i)
\end{align*}

We know (cf. Chow gaussian document) that 
\[\log(A) = \sum_{i=1}^n-\frac{1}{2}\left(\log(2\pi\sigma_i^2)-\frac{y_i^2}{\sigma_i^2}\right),\]
and this quantity is independant from the tree structure. We also know the explicit form of $\log(\psi_{kl})$ :
\[\log(\psi_{kl}(Y_i))=\frac{-1}{2}\left(\log\left(1-\frac{\sigma_{kl}^2}{\sigma_k^2 \sigma_{l}^2}\right)+\frac{(y_k^2\sigma_{l}^2+y_{l}^2\sigma_k^2-2\sigma_{kl}y_ky_{l})}
{\det(\Sigma_{kl})}-\left(\frac{y_k^2}{\sigma_k^2}+\frac{y_{l}^2}{\sigma_{l}^2}\right)\right)\]

Remembering that we work with standard normal distributions, the last two expressions are greatly simplified. The correlation $\rho_{kl}$ 
between $y_k$ and $y_l$ is now their covariance too, and after some manipulations:

\[\log(A) = \sum_{i=1}^n-\frac{1}{2}\left(\log(2\pi)-y_i^2\right)\]
\[\log(\psi_{kl}(Y_i))=\log\left(\frac{1}{\sqrt{1-\rho_{kl}^2}}\right)+\frac{\rho_{kl}}{1-\rho_{kl}^2}\cdot y_{ik}y_{il} - 
\frac{\rho_{kl}^2}{1-\rho_{kl}^2}\cdot \frac{y_{ik}^2 + y_{il}^2}{2}\]

\subsection{E step :}
\[ \mathds{P}(Y,T) = \mathds{P}(T)\times\mathds{P}(Y|T)\]
\begin{align*}
 \log(\mathds{P}(Y,T)) &= \sum_{(k,l)\in T} \log(\beta_{kl})  + \sum_{(k,l)\in T} \log(\psi_{kl}(Y))- \log (B)+\log(A)\\
 &=\sum_{k,l} \mathds{1}_{\{(k,l) \in T\}} \left(\log(\beta_{kl})  +  \log(\psi_{kl}(Y))\right)- \log (B)+\log(A)
\end{align*}
 Conditional expectation :
\[ \mathds{E}_\theta[\log(\mathds{P}(Y,T))|Y] \\
= \sum_{k,l}  \mathds{P}((k,l)\in T | Y) \times \left[ \log(\beta_{kl}) + \log(\psi_{kl}(Y)) \right]
 -\log(B)+\log(A)\]
 Computation of conditional probability : using Bayes, we specially consider the proportion of trees which contain an edge between the nodes $k$ and $l$.
 \begin{align*}
 \mathds{P}((k,l)\in T | Y)  &= \frac{\sum_{(k,l)\in T} \mathds{P}(T)\mathds{P}(Y|T)}{\sum_{T} \mathds{P}(T)\mathds{P}(Y|T)}\\
 &=1-\frac{\sum_{(k,l)\notin T} \prod \beta_{uv} \prod \psi_{uv}}{\sum_{T} \prod \beta_{uv} \prod \psi_{uv}}
 \end{align*}
 We define the Laplacian matrix as the following symmetric matrix :
 \[\mathcal{Q}_{uv}(W_\beta)=\begin{cases}
               -\beta_{uv} & 1\leq u<v \leq n\\
               \sum_{w=1}^n \beta_{wv} & 1\leq u=v \leq n.
            \end{cases}\]
 Lets $\mathcal{Q}^*$ be the first $(n-1)$ rows and columns of $\mathcal{Q}$. The  Matrix Tree Theorem (MTT) of West \cite{west} says that
 for any adjacence matrix $A$ of a multigraph G, $|\mathcal{Q}^*(A)|$ is the number of spanning trees of G, where $|\cdot|$ is the determinant.
 Meila \textit{et al.} \cite{meila} demonstrate the generalization of the MTT (GMTT)for a real-valued matrix, so that we now  get :

\[ \mathds{P}((k,l)\in T | Y) =1-\frac{|\mathcal{Q}^*(W_{\beta}^{-kl}\bigodot\psi)|}{|\mathcal{Q}^*(W_{\beta}\bigodot\psi)|}\]

Where the notation $W_{\beta}^{-kl}$ means that the entry at the $k^{\text{th}}$ line and $l^{\text{th}}$ column has been set to zero
(concretely, we erased the edge between nodes $k$ and $l$). This last quantity will be computed using the Kirshner theorem, allowing for a great gain in computation time.


\subsection{M step :\\}
Moving to the M step, the quantity $\tau_{kl} = \mathds{P}((k,l)\in T | Y)$ has been computed and is now considered as fixed.
We maximise the conditional expectation with respect to parameters $\beta_{kl}$.


 \[\argmax_{\beta_{kl}} \left\{\sum_{k,l} \tau_{kl}\times \left[ \log(\beta_{kl}) + \log(\psi_{kl}(Y)) \right]
 -\log(B)+\log(A)\right\}\]



 We derive with respect to $\beta_{kl}$:
\begin{equation}
 \label{1} \frac{\partial\mathds{E}_\theta[\log(\mathds{P}(Y,T))|Y]}{\partial\beta_{kl}} =\frac{  \tau_{kl}}{\beta_{kl}} - \frac{1}{B}
\frac{\partial B}{\partial\beta_{kl}}
\end{equation}
 Meila \textit{et al.} give a formula for the derivative of $B$, using the GMTT. Lets define the $M(W_\beta)$ symmetric matrix with 0 diagonal such that :
 \[\begin{cases}
    M_{uv} = [\mathcal{Q}^{*-1}]_{uu} + [\mathcal{Q}^{*-1}]_{vv} -2[\mathcal{Q}^{*-1}]_{uv} & u,v < n\\
    M_{nv} =M_{vn} =[\mathcal{Q}^{*-1}]_{vv} & v<n\\
     M_{vv} =0.
   \end{cases}\]
Meila \textit{et al.} then demonstrate that 
\begin{align*}
 \frac{\partial B}{\partial\beta_{kl}} &= M_{kl} |\mathcal{Q}^*(W_\beta)|\\
 &=M_{kl} \times B
\end{align*}
The last equality comes from the GMTT : $B = |\mathcal{Q}^*(W_\beta)|$. Replacing in equation \ref{1} and setting the expression to 0 we get :
\[\boxed{\hat{\beta}_{kl} = \frac{ \tau_{kl}}{M_{kl}}}\]

\bibliographystyle{apalike}
\bibliography{bibi.bib}
\end{document}
