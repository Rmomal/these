\modif{\paragraph{A reminder on EM and VEM.} Expectation-Maximisation \citep[EM:][]{DLR77} has become the standard algorithm for the maximum likelihood inference of latent variable models. Denoting $\gammab$ the unknown parameter, $\Yb$ the observed variables and $\Hb$ the latent variables, the aim of EM is to maximise the {\sl observed} (log-)likelihood $\log p_\gammab(\Yb)$. 
In the model defined in Section \ref{sec:model}, the set of parameter to estimate is $\gammab = (\betab, \Sigmab, \thetab)$ and the latent variables are $\Hb = (\Zb, T)$.
Because the {\sl complete} (log-)likelihood $\log p_\gammab(\Yb, \Hb)$ is often much easier to handle, EM alternatively evaluates the conditional distribution of the latent variables $p_\gammab(\Hb \mid \Yb)$ (E step) and updates the parameter estimates by maximizing the conditional expectation of the complete log-likelihood (M step).}

\modif{Unfortunately, for many models, the conditional distribution $p_\gammab(\Hb |\Yb)$ is intractable. The variational EM (VEM) algorithm has been designed to deal with such cases. Briefly speaking, the E step (during which the intractable conditional distribution should be evaluated) is replaced with a VE step, during which an approximate distribution $\pt(\Hb) \simeq p_\gammab(\Hb \mid \Yb)$ is determined. Actually, the VEM algorithm maximizes a lower bound of the genuine log-likelihood, similar to this given in Eq.~\eqref{eq:LowerBound} \citep[see][for an introduction]{OrW10,BKM17}.}

\paragraph{Application to the Poisson log-normal model.} 
To estimate the fixed regression parameters gathered in $\thetab$, we resort to a surrogate model where the entries of the abundance matrix $\Yb$ still have the conditional distribution given in Eq.~\eqref{eq:pY.Z}, but where the distribution of the $\Zb_i$ is not constrained to be faithful to a specific graphical model. Namely, the latent vectors $\Zb_i$ are only supposed to be independent and identically distributed (iid) Gaussian with distribution $\Ncal(0, \Sigmab)$, without any restriction on $\Sigmab$. 

This surrogate model is actually a Poisson log-normal model as introduced by \cite{AiH89}, the parameters of which can be estimated using a variational approximation similar to this introduced in \cite{CMR18}. 
%Namely, instead of maximizing the log-likelihood $\log p(\Yb)$ with respect to the parameters $\thetab$ and $\Sigmab$ using a regular EM algorithm, we maximize the lower bound of it
\modif{More specifically, we maximize with respect to the parameters $\thetab$ and $\Sigmab$ the following lower bound of the log-likelihood $\log p(\Yb)$:}
\begin{linenomath} 
\begin{equation}\label{eq:LowerBound}
\mathcal{J}(\Yb; \thetab, \Sigmab, \pt) := \log p_{\thetab, \Sigmab}(\Yb) - KL\left(\pt(\Zb) || p_{\thetab, \Sigmab}(\Zb \mid \Yb)\right),
\end{equation}
\end{linenomath}
where $KL(q||p)$ stands for KÃ¼llback-Leibler divergence between distributions $q$ and $p$ and where the approximate distribution $\pt(\Zb)$ 
%(which approximates $\pt_{\thetab, \Sigmab}(\Zb \mid \Yb)$) 
is chosen to be Gaussian. This means that each conditional distribution $p(\Zb_i \mid \Yb_i)$ is approximated with a normal distribution $\Ncal(\mbt_i, \Sbt_i)$. As shown in \cite{CMR18}, $\mathcal{J}(\Yb, \thetab, \Sigmab, \pt)$ is bi-concave in $(\thetab, \Sigmab)$ and $\{(\mbt_i, \Sbt_i)_i\}$, so that gradient ascent can be used. The {\tt PLNmodels} R-package --available on CRAN-- provides an efficient implementation of it.

The entries of the $\mbt_i$ and $\Sbt_i$ provide us with approximations of the conditional expectation,  variance and covariance of the $Z_{ij}$ conditionally on the $\Yb$, which we use to get the estimates $\widehat{\sigma}_j^2$ and $\widehat{\rho}_{jk}$ given in Eq.~\eqref{eq:sigma.rho}. More specifically, we use $\Esp(Z_{ij} \mid \Yb_i) \simeq \mt_{ij}$, $\Esp(Z^2_{ij} \mid \Yb_i) \simeq \mt_{ij}^2 + \St_{i,jj}$ and $\Esp(Z_{ij} Z_{ik} \mid \Yb_i) \simeq \mt_{ij} \mt_{ik} + \St_{i, jk}$.