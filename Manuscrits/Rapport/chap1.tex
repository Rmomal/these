%network inference in ecology
 \section{Graphical Models}
 A graphical model is classically described as a probabilistic model which conditional dependence structure is given by a graph. This first section gives all necessary notions and definitions from scratch, and presents the general framework of graphical models, the special case of the multivariate Gaussian distribution (Gaussian Graphical Models), and the special case of spanning tree graphs. Except from the section on spanning trees, all that follows is from \citet{Lau96}. Definitions and properties are adapted to focus on undirected graphs, and continuous and positive densities.
 
 \subsection{General elements}
 A graph is defined as a pair $\G=(V,E)$ such that $V$ is a finite set of vertices, and the set of edges $E$ is a subset of $V\times V$ such that vertices are not linked to themselves and there are no multiple edges between two vertices. For any given pair of nodes $(k,\ell)$, we denote an edge by $k\sim\ell$ and an absence of edge by $k\nsim \ell$. In the literature, $V$ can be composed of both quantitative and qualitative variables. However here we  only use quantitative variables and so $V$ is called pure. The following definitions apply to the particular case of a pure vertex set $V$ and an undirected graph $\G$.

Let's first consider a subset $A$ of the vertex set $V$. $A$ is said to be \textit{complete} if all the nodes it contains are linked with each other. If $A$ is additionally of maximal size, it is then called a \textit{clique}. The \textit{subgraph} $\G_A$ defined by $A$  is obtained from $\G$ by keeping edges with both endpoints in $A$. Furthermore if $A$, $B$ and $S$ are disjoint subset of $V$, $S$ is said to \textit{separate} $A$ from $B$ if any path from $\G_A$ to $\G_B$ intersects with $\G_S$. The following notions of decomposable graphs and perfect sequences are central in the development of graphical models.
 
 \begin{definition}[proper decomposition]
 A triple $(A, B, C)$ of disjoint subsets of the pure vertex set $V$ of an undirected graph $\G$ form a decomposition of $\G$ if $V=A\cup B \cup C$, and if $C$ verifies:
 \begin{enumerate}[label=(\roman*)]
 \item $C$ is a complete subset of  $V$,
 \item $C$ separates $A$ from $B$.
 \end{enumerate}
 If $A$ and $B$ are both non-empty, the decomposition is proper.
 \end{definition}
 
 
 \begin{definition}[decomposable graph]
 An undirected graph is decomposable if it is complete, or if there exists a proper decomposition $(A, B, C)$ into decomposable subgraphs $\G_{A\cup B}$ and $\G_{B\cup C}$.
 \end{definition}
 This recursive definition proves to be very helpful in demonstrations of decomposable graphs properties, but not to prove that a graph is decomposable. The notion of triangulation will help.
 
 \begin{definition}[Triangulated graph]
 An undirected graph $\G$ is said to be triangulated if loops in $\G$ connect three nodes at most.
 \end{definition} 
 \begin{definition}[perfect sequence]
 Let $B_1,...,B_k$ be a sequence of subsets of the pure set of vertex $V$ of the undirected graph $\G$. Let $H_j=B_1\cup .. \cup B_j$, and $S_j = H_{j-1} \cap B_J$. The sequence is perfect if it satisfies the following conditions:
 \begin{enumerate}[label=(\roman*)]
 \item for all $i>1$ there is a $j<i$ such that $S_i \subseteq B_j$ (running intersection property),
 \item each $S_i$ is a complete subset
 \end{enumerate}
 $H_j$ are called the histories and $S_j$ the separators.
 \end{definition}
% \begin{definition}[perfect numbering]
% A perfect numbering of the vertices $V$ of $\G$ is a numbering $\alpha_1,...,\alpha_k$ such that the sequence $B_1,...,B_k$ with 
% $$B_j = cl(\alpha_j) \cap \{\alpha_1,...,\alpha_j\}, j\geq 1 $$ is a perfect sequence. This implies that each $B_j$ is a complete subset.
% \end{definition}
Note that the empty set can be included in the set of separators. An example where separators are complete but the sequence is not perfect is the following: consider the sequence $B=\big\{B_1=\{1,2\}, B_2=\{3,4\},B_3=\{2,3,5\}\big\}$. The separators are $S_2=\varnothing, S_3=\{2,3\}$. As $\{2,3\}$ is not a subset of any of the $B_i$, the running intersection property is violated and $B$ is not perfect.
 \begin{definition}[multiplicity of a separator]
 The multiplicity $\nu (S)$ of the separator $S$ is an index counting the number of times $S$ occurs in a perfect sequence.
 \end{definition}
 
\begin{figure}[H]
 \begin{center}
	\begin{tikzpicture}	
      \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw]
		\node[basic, label=left:1] (A1) at (-0.2*\edgeunit,-0.7*\edgeunit) {};
		\node[basic, label=left:2] (A2) at (-0.4,0) {};
		\node[basic, label=left:3] (A3) at (-0.2*\edgeunit,0.7*\edgeunit) {};
		\node[basic, label=above:4] (A4) at (1*\edgeunit,0) {};
		\node[basic, label=right:5] (A5) at (1.6*\edgeunit,1.2*\edgeunit) {};
		\node[basic, label=right:6] (A6) at (2.3*\edgeunit,0.7) {};
		\node[basic, label=right:7] (A7) at (2.4*\edgeunit,0*\edgeunit) {};
		\node[basic, label=right:8] (A8) at (1.5*\edgeunit,-0.8*\edgeunit) {};
		\path (A1) edge  (A2)
        (A2) edge [] (A4)
        (A4) edge [] (A1)
        (A2) edge [] (A3)
        (A4) edge [] (A3)
        (A4) edge [] (A5)
        (A4) edge [] (A6)
        (A5) edge [] (A6)
        (A7) edge [] (A6)
        (A4) edge [] (A7)
        (A4) edge [] (A8);
	\end{tikzpicture} 
 \caption{An undirected decomposable graph with 8 variables}
  \label{ex:graph}
    \end{center}
\end{figure}
 Let's consider the graph of Figure \ref{ex:graph}. A perfect sequence of its cliques is 
$C_1=\{1,2,4\}$, $C_2=\{2,3,4\}$, $C_3=\{4,5,6\}$, $C_4=\{4,6,7\}$, $C_5=\{4,8\}$. The corresponding separators are $S_2=\{2,4\}$, $S_3=S_5=\{4\}$, $S_4=\{4,6\},$ which gives the following multiplicities: $\nu(\{2,4\})=1$,  $\nu(\{4,6\})=1$, $\nu(\{4\})=2$.  


\begin{prop}
\label{decomp}
The following conditions are equivalent for an undirected graph $\G$:
\begin{enumerate}[label=(\roman*)]
\item $\G$ is decomposable.
\item The cliques of $\G$ can form a perfect sequence.
\item Loops in $\G$ connect three nodes at most.
\end{enumerate}
\end{prop}

 This property is the most useful, as it states that as soon as a graph do not present with cycles of size 4 or more, it is decomposable on its cliques. This decomposition is essential to the graphical model properties, as we shall see in Gaussian graphical models.
 
\subsection{Characterization of conditional independence}
The notion of conditional independence is central to understand graphical models. All that follows focuses on the case of positive and continuous densities. Let's consider random variables $X,Y$ and $Z$ with joint distribution $f$. The conditional independence of $X$ and $Y$ conditional on $Z$ is linked to the factorization of $f$. More precisely: 
$$X \independent Y \mid Z \iff f(x,y,z) = f(x,z) \, f(y,z)/f(z). $$
It is very useful to understand conditional independence in terms of information. That is, "$X$ is independent of $Y$, given $Z$" means that knowing $Z$, $Y$ will not bring any new information on $X$, and conversely. The Markov property below is the link between conditional independence of variables and its representation in an undirected graph. Let's consider a collection of random variables $(X_v)_{v\in V}$ taking values in probability spaces $(\mathcal{X}_v)_{v\in V}$, and $\mathcal{X}=\otimes_{v\in V} \mathcal{X}_v$. We further denote for any subset $A$ of $V$, $X_A=(X_v)_{v\in A}$, and $\G$ a graph with vertex set $V$. 

\begin{definition}[Markov property]
A probability measure P on $\mathcal{X}$ is \textit{global} Markov relative to the undirected graph $\G$ if for any triple $(A, B, S)$ of disjoint subset of $V$ such that:
 $$ \text{S separates A from B } \Rightarrow X_A\independent X_B \mid X_S$$
 If additionally $X_A\independent X_B \mid X_S \Rightarrow \text{S separates A from B}  $, then P is \textit{faithful} Markov and $\G$ perfectly describes the conditional dependence structure of P.
\end{definition}
\begin{definition}[Factorization]
A probability measure P on $\mathcal{X}$ factorizes according to $\G$ if it has density $f$ with respect to measure $\mu = \otimes_{v\in V} \mu_v$, where $f$ can be written as
$$f(x) = \prod_{c\in \mathcal{C} }\psi_c(x),$$
where $\mathcal{C}$ denotes the set of cliques of $\G$, and for any subset $C$ of the vertex set $V$, $\psi_C$ is a positive function of realizations of $X_C$ only.
\end{definition}

\begin{theorem}[Hammersley and Clifford]
For any undirected graph $\G$ and probability distribution P with positive and continuous density $f$ with respect to a product measure $\mu$, it holds that:
$$P\text{ factorizes according to } \G \iff P\text{ is global Markov relative to }\G $$
\end{theorem}

 
 
 
 
 \subsection{Gaussian Graphical Models}
 Gaussian graphical models describe the conditional dependency structure of a  multivariate Gaussian distribution. Let $X\sim \mathcal{N}(\mu, \Sigma)$ with precision matrix $\Omega=\Sigma^{-1}$ and density $f$. $X_k$ denote the $k^{th}$ variable and $X^i$ denotes the $i^{th}$ sample of $X$. An interesting property of the multivariate distribution is that it naturally factorizes on the non-null entries of its precision matrix:
 $$f(X) \propto \prod_{k,\ell, \omega_{kl}\neq 0} \exp(-X_{k}\omega_{k\ell}X_{\ell}/2)$$
 Therefore as stated by the Hammersley and Clifford Theorem, $f$ is global Markov and is associated with an undirected graph $\G$ which edges are determined by the non-null entries of $\Omega$.  Another useful property of the multivariate Gaussian distriubtion is that of the following:
 
 \begin{prop}If  $X\sim \mathcal{N}_V(\mu, \Omega^{-1})$, then it holds for any $k,\ell\in V$ with $k\neq \ell$ that
$$X_k \independent X_\ell \mid X_{V\setminus\{k,\ell\}} \iff \omega_{k\ell}=0 $$
 \end{prop}
 
Therefore a null entry of $\Omega$ implies a conditional independence in $X$. As it also implies no edge between the corresponding nodes in $\G$ thanks to the factorization of $f$, $f$ is faithful Markov and $\G$ perfectly represent the conditional dependency structure of $X$. In other words, in the framework of GGM we have the following equivalence:
 $$k\nsim \ell \iff  X_k \independent X_\ell \mid X_{V\setminus\{k,\ell\}} \iff \omega_{k\ell}=0$$. 
 
 If $X$ is associated with a decomposable graph $\G$, the Markov property reflects in the following form for the density. Considering a perfect sequence of the cliques of $\G$, we have that
 $$f(X)=\dfrac{\prod_{C\in \mathcal{C}} f(X_C)}{\prod_{S\in \mathcal{S}} f(X_S)^{\nu(S)}}.$$
 In what follows, for any  squared  matrix $A$ of dimension $V$ and $B$ a subset of $V$, $A_B$ will refer to the bloc $B$ of $A$: $A_{B}=(a_{ij})_{\{i,j\}\in B}$.   $[A_B]^V$ will then denote the matrix obtained by filling up with zero entries to obtain full dimension $V\times V$, so that:
$$([A_B]^q )_{ij}=\left\{ \begin{array}{rl}
a_{ij} & \text{if } \{i,j\}\in B\\
0 &  \text{if } \{i,j\}\in V_{\setminus B}
\end{array}\right.$$


Expression for $\Omega$ and its determinant can also be derived: 
 \begin{lemma} In a decomposable Gaussian graphical model with precision matrix $\Omega = \Sigma^{-1}$ of dimension $V$, a perfect sequence of the cliques $\mathcal{C}$ of $\G$ gives:
 \begin{equation*}
 \left\{
 \begin{array}{rl}
 \Omega &= \sum_{C\in \mathcal{C}} \big[(\Sigma_C)^{-1}\big]^V - \sum_{S\in \mathcal{S}} \nu(S)  \big[(\Sigma_S)^{-1}\big]^V \\\\
 |\Omega| &=\dfrac{\prod_{C\in \mathcal{C}} |\Sigma_C|^{-1}}{\prod_{S\in \mathcal{S}} |\Sigma_S|^{-1}}
 \end{array} \right.
\end{equation*}  
\end{lemma}

The GGM framework allows closed form expressions for the maximum likelihood estimators of the mean vector, the precision matrix, its determinant and the terms of the covariance matrix corresponding to edges in the graph. They all depend on the sum of squared deviances, defined as follows:
\begin{definition}
The sum of squared deviation matrix of a multivariate Gaussian $X$, also known as total sum of squares,  is given by:
$$ssd = \sum_{i=1}^n (X^i - \xbar{X})(X^i - \xbar{X})^\intercal = X^\intercal X - n  \xbar{X} \xbar{X}^\intercal$$
\end{definition}
\begin{theorem}[\citet{Lau96} Theorem 5.3]
In the Gaussian graphical model,  the maximum likelihood estimates of the unknown mean and covariance matrix exist if the ssd matrix is positive definite. If $n>|V|$ this happens with probability one. When they exist, the estimate of the mean is $\widehat{\mu} = \xbar{X}$, and the estimate of the unknown covariance matrix $\Sigma$ is determined as the unique solution of the system of equations
$$n\widehat{\sigma}_{jj} = ssd_{jj}, n\widehat{\sigma}_{k\ell} = ssd_{k\ell}, j,k,\ell \in V, k\sim\ell,$$
which also satisfies the model restriction $\omega_{k\ell}l =0 \iff k\nsim \ell $.
\end{theorem}

\begin{theorem}[\citet{Lau96} Proposition 5.9]
In a Gaussian graphical model with graph $\G$, the maximum likelihood estimate of the precision matrix exists with probability one if $n>max_{C\in\mathcal{C}}|C|$. It is then given as
$$\widehat{\Omega} = n \bigg\{\sum_{C\in\mathcal{C}} \big[(ssd_C)^{-1}\big]^V-\sum_{S\in\mathcal{S}} \nu(S) \big[(ssd_S)^{-1}\big]^V \bigg\},$$
where $\mathcal{C}$ is the set of cliques of $\G$ and $\mathcal{S}$ the set of separators with multiplicities $\nu$ in any perfect sequence. The determinant of the estimate can be calculated as
$$|\widehat{\Omega}| = n^{|V|} \dfrac{\prod_{C\in\mathcal{C}}|(ssd_C)^{-1}|}{\prod_{S\in\mathcal{S}} (|(ssd_S)^{-1}|)^{\nu(S)}}$$
\end{theorem}
 
 Following proposition 1.3, it is possible to carry a penalized estimation of $\Omega$ which selects the non-null entries.  Such methods (e.g. glasso) are very popular, as the maximum likelihood estimators require the precise knowledge of the graph structure, and is therefore rarely available. However relying on spanning tree structures greatly simplifies the expressions and make their use possible.
 
\subsection{Spanning trees}
Trees are graphs with no cycles, so all cliques are edges only. When a tree connects all the nodes, it is called a spanning tree. This particular type of graph is both the sparsest connected graph, and the most connected graph without loops. This feature makes it a decomposable graph by definition, following condition $(iii)$ of Proposition \ref{decomp}.  The following result is obtained directly.

\begin{prop}
If $T$ is a spanning tree, all its separators are nodes. For any node $k$ of $T$ we denote $d(k)$ its degree. Then its multiplicity in any perfect sequence is $\nu(k) = d(k)-1$.
\end{prop}
 
 
 The spanning tree is the first graph structure to have been studied. In the latter part of the $19^{th}$ century, Arthur Cayley established that the total number of spanning trees of a complete graph with $p$ nodes is $p^{p-2}$. Kirchhoff generalized this result in a theorem known as the "all-minor" or the "matrix-tree" theorem. It states that the total number of spanning trees in any graph can be computed in polynomial time as the determinant of any minor of its Laplacian matrix built from the graph adjacency matrix, that is where the weight matrix in definition \ref{laplacian} is binary. In this case, the diagonal of the graph Laplacian matrix is simply composed of the nodes degrees.
 
 \begin{definition}[Laplacian matrix]
 \label{laplacian}
 The Laplacian matrix $\Qbf$ of a symmetric matrix $\Wbf=[w_{jk} ]_{1\leq j,k\leq p}$ is as follows :

\[
 [\Qbf]_{jk}  =\begin{cases}
    -w_{jk}  & 1\leq j<k \leq p\\
    \sum_{u=1}^p w_{ju} & 1\leq j=k \leq p.
    \end{cases}
\]
 \end{definition}
 
 This theorem was extended for weighted graphs in the late 20th century. Hereafter we denote $\Abf^{uv}$ the squared matrix $\Abf$ deprived from its $u$th row and $v$th column, and remind that the $(u, v)$-minor of $\Abf$ is the determinant of this deprived matrix, namely $|\Abf^{uv}|$.

\begin{theorem}[Matrix Tree Theorem  \cite{matrixtree,MeilaJaak}] \label{thmm:MTT}
    For any symmetric weight matrix $\Wbf$ with all positive entries, the sum over all spanning trees of the product of the weights of their edges is equal to any minor of its Laplacian. That is, for any $1 \leq u, v \leq p$,
 
   \[
    W := \sum_{T\in\mathcal{T}} \prod_{(j, k)\in T} w_{jk} = |\Qbf^{uv}|.
    \]
   
\end{theorem}    

In the following, without loss of generality, we choose $\Qbf^{11}$. As an extension of this result, \cite{MeilaJaak} provide a close form expression for the derivative of $W$ with respect to each entry of $\Wbf$. 

\begin{lemma} [\cite{MeilaJaak}] \label{lemm:Meila}
    Define the entries of the symmetric matrix $\Mbf$ as
 
\[    
 [\Mbf]_{jk} =\begin{cases}
    \left[(\Qbf^{11})^{-1}\right]_{jj} + \left[(\Qbf^{11})^{-1}\right]_{kk} -2\left[(\Qbf^{11})^{-1}\right]_{jk} & 1< j<k \leq p\\
    \left[(\Qbf^{11})^{-1}\right]_{jj} & k=1, 1< j \leq p  \\
    0 &  j=k .
    \end{cases}
\]
 
it then holds that
 
$$
\partial_{w_{jk}} W = [\Mbf]_{jk}  \times W.
$$
\end{lemma}

Theorem \ref{thmm:MTT} allows to easily sum over all possible spanning trees and handle such sum-product forms in a computationally efficient way.  

\begin{definition}[decomposable tree distribution]
For any symmetric weight matrix $\Wbf$ with all positive entries, we denote $W$ the sum-product form defined in Theorem \ref{thmm:MTT} A decomposable  distribution for the tree $T$ is then defined as follows:
$$p_{\Wbf}(T) = \prod_{k,\ell \in T} w_{k\ell} / W $$
Therefore the probability of a tree is proportional to the product of its edges weights.
\end{definition}
Observe that a decomposable tree distribution is stable under a multiplicative transformation applied to its weight matrix. Once a probability is assigned to each tree of the spanning tree space, the probability of an edge in a tree can be defined as the sum of the probabilities of trees containing this edge. Namely for the tree $T$:
$$\mathds{P}\{kl\in T\} = \sum_{T\in \mathcal{T}} p_{\Wbf}(T).$$

\begin{lemma} [\cite{kirshner}] \label{lem:Kirshner}
    Let $p_{\Wbf}$ be a decomposable tree distribution with symmetric and positive weight matrix $\Wbf$. Taking the symmetric matrix $\Mbf$ as defined in Lemma  \ref{lemm:Meila}, the probability for an edge $kl$ to be in the tree $T$ writes:
 
$$\mathds{P}\{kl\in T\} =  w_{kl}\: \Mbf_{kl}$$
\end{lemma}
The Lemma above states that it is possible to compute all edges probabilities at the same time at the cost of the inversion of the weights matrix Laplacian.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Incomplete data inference}

 \subsection{Expectation-Maximization algorithm}
 
 \subsection{Variational version}
  
\section{Network inference from count data}
  inférence réseaux vs inférence des paramètres (on n'observe pas le réseau)
  
 \subsection{Linking count data to Gaussian data} 
\subsubsection{Transformations}
\begin{itemize}
\item copulas
\item clr : compositional data, loss of the variability
\end{itemize}

\subsubsection{Latent variables}
\begin{itemize}
\item JSDM : PLN
\end{itemize}
\subsection{Network inference from Gaussian data}
 \subsubsection{Penalized estimation}
 \subsubsection{Averaging on trees}