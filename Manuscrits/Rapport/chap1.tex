
\vspace{1cm}
This first chapter  details the technical background extracted from the literature which is used in the next chapters. It is designed to be read independently  of the rest of this work, and therefore repetitions with elements from the next chapters are unavoidable.  The chapter begins by covering all the required notions from graph theory, with a  focus on tree-shaped graphs. A second section  presents Gaussian graphical models properties and explains why it is a golden framework for network inference. After a reminder on the Expectation-Maximization algorithm as well as its variational interpretation, the last part is a state of the art of network inference from abundance data and presents methods stemming from both genomics and ecology. 


 \section{Graphical Models}
 A graphical model is classically described as a probabilistic model which conditional dependence structure is given by a graph. This first section gives the general framework of graphical models, and adapts definitions and properties from \citet{Lau96} to  undirected graphs involving only quantitative variables.  Then, the specific algebraic properties of  spanning tree graphs are presented.
 %%%%%%%%%%%%%%%%%%%%%%%%%%
 \subsection{General framework}
 A graph is defined as a pair $\G=(V,E)$ such that $V$ is a finite set of vertices, and the set of edges $E$ is a subset of $V\times V$. Here we consider $E$ such that vertices are not linked to themselves and there are no multiple edges between two vertices. For any given pair of nodes $(k,\ell)$, we denote by $k\sim\ell$ an edge and by $k\nsim \ell$ an absence of edge. In the literature, $V$ can be composed of both quantitative and qualitative variables. However here we  only use variables of one kind (quantitative) and so $V$ is called pure. The following definitions apply to the particular case of a pure vertex set $V$ and an undirected graph $\G$.\\

Let's first consider a subset $A$ of the vertex set $V$. $A$ is said to be \textit{complete} if all the nodes it contains are linked with each other. If $A$ is additionally of maximal size, it is then called a \textit{clique}. This definition makes the expression "maximal clique" a pleonasm. The \textit{subgraph} $\G_A$ defined by $A$  is obtained from $\G$ by keeping edges with both endpoints in $A$. Furthermore if $A$, $B$ and $S$ are disjoint subsets of $V$, $S$ is said to \textit{separate} $A$ from $B$ if any path from $\G_A$ to $\G_B$ intersects with $\G_S$. The following notions of decomposable graphs and perfect sequences are central in the definition of a graphical model.
 
 \begin{definition}[Proper decomposition]\label{decomp}
 A triple $(A, B, C)$ of disjoint subsets of the pure vertex set $V$ of an undirected graph $\G$ forms a decomposition of $\G$ if $V=A\cup B \cup C$, and if $C$ satisfies:
 \begin{enumerate}[label=(\roman*)]
 \item $C$ is a complete subset of  $V$,
 \item $C$ separates $A$ from $B$.
 \end{enumerate}
 If $A$ and $B$ are both non-empty, the decomposition is proper.
 \end{definition}
 \begin{figure}[H]
 \begin{center}
	\begin{tikzpicture}	
      \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw]
		\node[basic, label=above:1] (A1) at (-1*\edgeunit,1*\edgeunit) {};
		\node[basic, label=below:2] (A2) at (-1*\edgeunit,0) {};
		\node[basic, label=above:3] (A3) at (0,1*\edgeunit) {};
		\node[basic, label=below:4] (A4) at (0,0) {};
		\node[basic, label=above:5] (A5) at (1*\edgeunit,1*\edgeunit) {};
		\node[basic, label=below:6] (A6) at (1*\edgeunit,0) {};
		\path (A1) edge  (A3)
        (A3) edge [] (A5)
        (A3) edge [] (A4)
        (A2) edge [] (A4)
        (A4) edge [] (A6);
	\end{tikzpicture} 
 \caption{An undirected graph with 6 nodes.}
  \label{ex:graph0}
    \end{center}
\end{figure}
The graph in Figure \ref{ex:graph0} gives an example. Let's define the sets of nodes $A=\{1,2\}$, $B=\{5,6\}$ and $C=\{3,4\}$. As nodes $3$ and $4$ are linked, $C$ is a complete set. Moreover $C$ separates $A$ from $B$, and $A$ and $B$ are both non-empty, hence $(A, B, C)$  forms a proper decomposition. A decomposition is thus simply a separation of the set of nodes with complete subsets. A decomposable graph is then defined using Definition \ref{decomp} in a recursive manner:
 \begin{definition}[Decomposable graph]
 An undirected graph is decomposable if it is complete, or if there exists a proper decomposition $(A, B, C)$ into decomposable subgraphs $\G_{A\cup B}$ and $\G_{B\cup C}$.
 \end{definition}
 This definition proves to be very useful in demonstrations of decomposable graphs properties, but not to identify decomposable graphs in practice. The following notion of triangulation will help.
 
 \begin{definition}[Triangulated graph]
 An undirected graph $\G$ is said to be triangulated if loops in $\G$ connect three nodes at most.
 \end{definition} 
 In other words in a triangulated graph, the loop of maximal size will form a triangle. Triangulated graphs are also called chordal graphs, as it suffices to draw chords in all loops to triangulate a graph. Then, a \textit{sequence} is a numbered list of subsets of the node set. A sequence is called \textit{perfect} under the following conditions on the subsets and the numbering order:
 \begin{definition}[Perfect sequence]\label{def:seq}
 Let $B_1,...,B_k$ be a sequence of subsets of the pure set of vertex $V$ of the undirected graph $\G$. Let $H_j=B_1\cup .. \cup B_j$, and $S_j = H_{j-1} \cap B_j$. The sequence is perfect if it satisfies the following conditions:
 \begin{enumerate}[label=(\roman*)]
 \item for all $i>1$ there is a $j<i$ such that $S_i \subseteq B_j$ (running intersection property),
 \item each $S_i$ is a complete subset.
 \end{enumerate}
 $H_j$ are called the histories and $S_j$ the separators.
 \end{definition}
% \begin{definition}[perfect numbering]
% A perfect numbering of the vertices $V$ of $\G$ is a numbering $\alpha_1,...,\alpha_k$ such that the sequence $B_1,...,B_k$ with 
% $$B_j = cl(\alpha_j) \cap \{\alpha_1,...,\alpha_j\}, j\geq 1 $$ is a perfect sequence. This implies that each $B_j$ is a complete subset.
% \end{definition}
 An example where separators are complete but the sequence is not perfect is given in Figure \ref{ex:graph1}. A sequence is $B=\big\{B_1=\{1,2\}, B_2=\{3,4\},B_3=\{2,3,5\}\big\}$, with the separators $S_2=\varnothing, S_3=\{2,3\}$. As $\{2,3\}$ is not a subset of any of the $B_i$ with $i$ less than 3, the running intersection property is violated and $B$ is not perfect. Inverting $B_2$ and $B_3$ however yields a perfect sequence, which also shows that a simple way of designing a perfect sequence is to ensure that each new set $B_j$ includes its separator with the previous set $B_{j-1}$. 
%Note that in the definition the empty set can be included in the set of separators, but an easy way to find a perfect sequence is simply to avoid this situation. 
\begin{figure}[H]
 \begin{center}
	\begin{tikzpicture}	
      \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw]
		\node[basic, label=above:1] (A1) at (-1*\edgeunit,0*\edgeunit) {};
		\node[basic, label=below:2] (A2) at (0,0) {};
		\node[basic, label=below:3] (A3) at (1*\edgeunit,0*\edgeunit) {};
		\node[basic, label=above:4] (A4) at (2*\edgeunit,0) {};
		\node[basic, label=right:5] (A5) at (0.5*\edgeunit,0.8*\edgeunit) {};
		\path (A1) edge  (A2)
        (A2) edge [] (A3)
        (A3) edge [] (A4)
        (A2) edge [] (A5)
        (A5) edge [] (A3);
	\end{tikzpicture} 
 \caption{An undirected graph with 5 nodes.}
  \label{ex:graph1}
    \end{center}
\end{figure}
The notion of perfect sequence then allows to define the multiplicity of a separator:
 \begin{definition}[multiplicity of a separator]
 The multiplicity $\nu (S)$ of the separator $S$ is an index counting the number of times $S$ occurs in a perfect sequence.
 \end{definition}
 

The graph of Figure \ref{ex:graph2} gives an example. There a perfect sequence of its cliques is 
$C_1=\{1,2,4\}$, $C_2=\{2,3,4\}$, $C_3=\{4,5,6\}$, $C_4=\{4,6,7\}$, $C_5=\{4,8\}$. The corresponding separators are $S_2=\{2,4\}$, $S_3=S_5=\{4\}$, $S_4=\{4,6\},$ which gives the following multiplicities: $\nu(S_2)=1$,  $\nu(S_4)=1$, $\nu(S_3)=\nu(S_5)=2$.  

\begin{figure}[H]
 \begin{center}
	\begin{tikzpicture}	
      \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw]
		\node[basic, label=left:1] (A1) at (-0.2*\edgeunit,-0.7*\edgeunit) {};
		\node[basic, label=left:2] (A2) at (-0.4,0) {};
		\node[basic, label=left:3] (A3) at (-0.2*\edgeunit,0.7*\edgeunit) {};
		\node[basic, label=above:4] (A4) at (1*\edgeunit,0) {};
		\node[basic, label=right:5] (A5) at (1.6*\edgeunit,1.2*\edgeunit) {};
		\node[basic, label=right:6] (A6) at (2.3*\edgeunit,0.7) {};
		\node[basic, label=right:7] (A7) at (2.4*\edgeunit,0*\edgeunit) {};
		\node[basic, label=right:8] (A8) at (1.5*\edgeunit,-0.8*\edgeunit) {};
		\path (A1) edge  (A2)
        (A2) edge [] (A4)
        (A4) edge [] (A1)
        (A2) edge [] (A3)
        (A4) edge [] (A3)
        (A4) edge [] (A5)
        (A4) edge [] (A6)
        (A5) edge [] (A6)
        (A7) edge [] (A6)
        (A4) edge [] (A7)
        (A4) edge [] (A8);
	\end{tikzpicture} 
 \caption{An undirected decomposable graph with 8 nodes.}
  \label{ex:graph2}
    \end{center}
\end{figure}

\begin{prop}
\label{decomp}
The following conditions are equivalent for an undirected graph $\G$:
\begin{enumerate}[label=(\roman*)]
\item $\G$ is decomposable.
\item The cliques of $\G$ can form a perfect sequence.
\item $\G$ is triangulated.
\end{enumerate}
\end{prop}

 This property is the most useful in practice, as it states that as soon as a graph does not present with cycles of size 4 or more, it is decomposable on its cliques. This decomposition is essential to the graphical model properties, as we will see in Gaussian graphical models.
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Characterization of conditional independence}
The notion of conditional independence is central in the theory of graphical models. All that follows focuses on the case of variables associated with positive and continuous densities. Let's consider three random variables $X,Y$ and $Z$ with joint distribution $f$. The conditional independence of $X$ and $Y$ conditional on $Z$ is linked to the factorization of $f$. More precisely: 
$$X \independent Y \mid Z \iff f(x,y,z) = f(x,z) \, f(y,z)/f(z). $$
Conditional independence can be understood in terms of information. That is, "$X$ is independent of $Y$, given $Z$" means that knowing $Z$, $Y$ will not bring any new information on $X$, and conversely. The Markov property below is the link between conditional independence of variables and its representation in an undirected graph. Let's consider a collection of random variables $(X_v)_{v\in V}$ taking values in probability spaces $(\mathcal{X}_v)_{v\in V}$, and $\mathcal{X}=\otimes_{v\in V} \mathcal{X}_v$. We further denote for any subset $A$ of $V$, $X_A=(X_v)_{v\in A}$, and $\G$ a graph with vertex set $V$. 

\begin{definition}[Global Markov property]\label{def:markov}
A probability measure P on $\mathcal{X}$ is \textit{global} Markov relative to the undirected graph $\G$ if for any triple $(A, B, S)$ of disjoint subsets of $V$, it holds that:
 $$ \text{S separates A from B } \Rightarrow X_A\independent X_B \mid X_S.$$
 If additionally $X_A\independent X_B \mid X_S \Rightarrow \text{S separates A from B}  $, then P is \textit{faithful} Markov and $\G$ perfectly describes the conditional dependence structure of P.
\end{definition}
Therefore if a distribution is only global Markov, its related graph is possibly too dense: it can contain edges between variables which are actually conditionally independent from one another. 
\begin{definition}[Factorization]\label{def:fact}
A probability measure P on $\mathcal{X}$ factorizes according to $\G$ if it has density $f$ with respect to measure $\mu = \otimes_{v\in V} \mu_v$, where $f$ can be written as
$$f(x) = \prod_{c\in \mathcal{C} }\psi_c(x),$$
where $\mathcal{C}$ denotes the set of cliques of $\G$, and for any subset $C$ of the vertex set $V$, $\psi_C$ is a positive function of $X_C$ only.
\end{definition}
The following theorem links definitions \ref{def:markov} and \ref{def:fact} for positive and continuous distributions:

\begin{theorem}[Hammersley and Clifford] \label{thm:ham}
For any undirected graph $\G$ and probability distribution P with strictly positive and continuous density $f$ with respect to a product measure $\mu$, it holds that:
$$P\text{ factorizes according to } \G \iff P\text{ is global Markov relative to }\G. $$
\end{theorem}

This equivalence means in practice that writing a density in a product form on some combination of its variables helps to identify conditional independence relations.

%%%%%%%%%%%%%%%%%%%%%%%%%%
 \subsection{Spanning trees}\label{trees}
Trees are graphs with no cycles, meaning that all cliques are edges only. When a tree connects all the nodes, it is called a spanning tree. This particular type of graph is both the sparsest connected graph, and the most connected graph without loops. This feature makes it a decomposable graph by definition, following condition $(iii)$ of Proposition \ref{decomp}.  The following result is obtained directly.

\begin{prop}
If $T$ is a spanning tree, all of its separators are nodes. For any node $k$ of $T$, $d(k)$ denotes its degree. Then its multiplicity in any perfect sequence is $\nu(k) = d(k)-1$.
\end{prop}
 
 
 The study of the spanning tree structure began in the latter part of the $19^{th}$ century. Then, Arthur Cayley established that the total number of spanning trees of a complete graph with $p$ nodes is $p^{p-2}$. Kirchhoff generalized this result in a theorem known as the "all-minor" theorem, using the notion of graph Laplacian matrix.
  \begin{definition}[Laplacian matrix]
 \label{laplacian}
 The Laplacian matrix $\Qbf$ of a symmetric matrix $\Wbf=[w_{jk} ]_{1\leq j,k\leq p}$ is as follows :

\[
 [\Qbf]_{jk}  =\begin{cases}
    -w_{jk}  & 1\leq j<k \leq p\\
    \sum_{u=1}^p w_{ju} & 1\leq j=k \leq p.
    \end{cases}
\]
When $\Wbf$ is a graph adjacency matrix, $\Qbf$ is called the graph Laplacian matrix and its diagonal is composed of the degrees of each node.
 \end{definition}
 
 Hereafter $\Abf^{uv}$ denotes the squared matrix $\Abf$ deprived from its $u$th row and $v$th column, and remind that the $(u, v)$-minor of $\Abf$ is the determinant of this deprived matrix, namely $|\Abf^{uv}|$.
 
 \begin{theorem}[Kirchhoff's theorem]\label{th:kir}
 For any graph $\G$, let  $\Qb$ denote its graph Laplacian matrix. Then for any  pair of nodes $\{u,v\}$, the total number of spanning trees in $\G$ is equal to  $|\Qb^{uv}|$.
\end{theorem}  

 
The total number of spanning trees in any graph can thus be computed in polynomial time ($\mathcal{O}(p^3)$). For the sake of clarity, we introduce the notation $jk\in T$ which means that nodes $j$ and $k$ are linked in tree $T$. Theorem \ref{th:kir} was extended for weighted graphs in the late 20th century as follows, where $\mathcal{T}$ denotes the spanning tree space on $V=\{1,...,p\}$:

\begin{theorem}[Matrix Tree Theorem  \cite{matrixtree,MeilaJaak}] \label{thmm:MTT}
    For any symmetric weight matrix $\Wbf$ with all entries in $\mathds{R}^+$, the sum over all spanning trees of the product of the weights of their edges is equal to any minor of its Laplacian $\Qbf$. That is, for any $1 \leq u, v \leq p$,
 
   \[
    W := \sum_{T\in\mathcal{T}} \prod_{jk\in T} w_{jk} = |\Qbf^{uv}|.
    \]
   
\end{theorem}    

Consequently, the operation of summing over all spanning trees can be carried out in a computationally efficient way. \cite{MeilaJaak}  built on this result to provide a close form expression for the derivative of the sum-product $W$ with respect to each entry of the input weight matrix $\Wbf$.   Without loss of generality, we choose $\Qbf^{11}$.

\begin{lemma} [\cite{MeilaJaak}] \label{lemm:Meila}
    Define the entries of the symmetric matrix $\Mbf$ as
\[    
 [\Mbf]_{jk} =\begin{cases}
    \left[(\Qbf^{11})^{-1}\right]_{jj} + \left[(\Qbf^{11})^{-1}\right]_{kk} -2\left[(\Qbf^{11})^{-1}\right]_{jk} & 1< j<k \leq p\\
    \left[(\Qbf^{11})^{-1}\right]_{jj} & k=1, 1< j \leq p  \\
    0 &  j=k .
    \end{cases}
\] 
it then holds that
$$\partial_{w_{jk}} W = [\Mbf]_{jk}  \times W.$$
\end{lemma}

Theorem \ref{thmm:MTT} actually gives the solution to the computation of the sum on trees of a product function on the edges of a spanning tree, which can be used combined with  Lemma \ref{lemm:Meila} to perform network inference using average on trees, as we will see later on. 

%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%
 \section{Gaussian Graphical Models}
 Gaussian graphical models (GGM) describe the conditional dependency structure of a  multivariate Gaussian distribution. The different properties of the multivariate Gaussian distribution allow some specific interpretations and estimation results of its related graphical model,  making the GGM a sound and very convenient  framework to work with.
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%
 \subsection{Specific properties} \label{ggm:prop}
The multivariate Gaussian distribution possesses various properties which facilitate computations as well as interpretations. Two in particular are of interest in the graphical models setting. The first one concerns the natural writing of its density.
 \begin{prop}\label{pp:ggm1}
 Let $X\sim \mathcal{N}(\mu, \Sigmab)$ with precision matrix $\Omegab=\Sigmab^{-1}$ and $X_k$ denotes the $k^{th}$ column of $X$. Then the associated density $f$ factorizes as:
 $$f(X) \propto \prod_{k,\ell, \omega_{kl}\neq 0} \exp(-X_{k}\omega_{k\ell}X_{\ell}/2).$$
 \end{prop}
 
 \begin{prop}\label{pp:glob}
The density $f$ of a multivariate Gaussian distribution $\Ncal(\mu, \Omegab^{-1})$ is global Markov relative to an undirected graph $\G$, the edges of which are determined by the non-null entries of the precision matrix $\Omegab$.
 \end{prop}
 \begin{proof}
 This is a direct application of Theorem \ref{thm:ham} to the factorized multivariate Gaussian density reminded in Proposition \ref{pp:ggm1}.
\end{proof} 

As $f$ is global Markov, any two non-neighbors in $\G$ are conditionally independent in $f$.  The multivariate Gaussian distribution also presents with a facilitating property about conditional independence, reminded below.
 
 \begin{prop}\label{pp:ggm2}If  $X\sim \mathcal{N}_V(\mu, \Omegab^{-1})$, then it holds for any $k,\ell\in V$ with $k\neq \ell$ that
$$X_k \independent X_\ell \mid X_{V\setminus\{k,\ell\}} \iff \omega_{k\ell}=0. $$
 \end{prop}
  \begin{proof}
  This comes directly from the fact that the $2\times 2$  covariance matrix of variables $k$ and $l$ conditional on all others expresses in terms of the precision matrix : $$\Sigmab_{k,l\mid V\setminus\{k,l\}} = |\Omegab_{\{k,l\}}|^{-1} \left(\begin{array}{cc}
\omega_{kk} & -\omega_{kl}\\
-\omega_{kl} & \omega_{ll}  
  \end{array}\right).$$
  As conditional independence between Gaussian variables is equivalent to 0 terms in the conditional covariance matrix, the above expression shows that it is also equivalent with null precision terms.
  \end{proof}
  Note that Proposition \ref{pp:ggm2} is not a consequence of proposition \ref{pp:ggm1}.
\begin{prop}[Faithfulness property]\label{ggm:faith}
 Let $X\sim \mathcal{N}(\mu, \Omegab^{-1})$ with associated density $f$, $\G$ the graph which edges represent the non-null entries of $\Omega$. Then  it holds that:
 $$k\nsim \ell \iff  X_k \independent X_\ell \mid X_{V\setminus\{k,\ell\}}. $$
 and $f$ is faithful Markov relative to $\G$.
\end{prop} 
\begin{proof}
The right implication is the global Markov property of the multivariate Gaussian distribution reminded in Proposition \ref{pp:glob}.

The left implication comes for the combination of Propositions \ref{pp:ggm1} and \ref{pp:ggm2}. From Proposition \ref{pp:ggm2}:
$$X_k \independent X_\ell \mid X_{V\setminus\{k,\ell\}} \Rightarrow \omega_{k\ell}=0,$$
and a null precision term implies no edge between the corresponding nodes in $\G$ according to  Proposition \ref{pp:ggm1}.
\end{proof}
Proposition \ref{ggm:faith} is key for estimation in the GGM framework, as it states that finding the null precision entries gives all the graph of conditional dependencies. The Gaussian framework also offers specific conditioning results, and in particular the regression interpretation of $\Omegab$ entries.

\begin{prop}[Regression interpretation]\label{ggm:reg}
 Considering $X\sim \mathcal{N}(\mu, \Omegab^{-1})$, the linear regression of  $X_j$ on $X_{\setminus j}$ writes :
$$X_j = \sum_{k\neq j} \beta_{jk} X_k + \varepsilon_j,\qquad \text{where}\qquad\varepsilon_j \sim \Ncal (0, \omega_{jj}^{-1}), \qquad \beta_{jk} = -\frac{\omega_{jk}}{\omega_{jj}}.$$

\end{prop}
Hence $\omega_{jk}$ is, up to a scalar, the coefficient of $X_k$ in the multiple regression of $X_j$ on all other variables. This result is at the basis of GGM inference methods relying on regression \citep{MeB06}, which we will see later on. But first, let's turn to Lauritzen's  parameters estimation for GGM, made possible by the faithfulness property.

%%%%%%%%%%%%%%%%%%%%%%%%%%
 \subsection{Maximum likelihood estimation}\label{ggm:mle}
In this section we adopt the following notations: for any  squared  matrix $\Abf$ of dimension $p$ and $B$ a subset of $V=\{1,...,p\}$, we let $\Abf_B$ refer to the block $B$ of $\Abf$: $\Abf_{B}=(a_{ij})_{\{i,j\}\in B}$.   $[\Abf_B]^p$ then denotes the matrix obtained by filling up with zero entries to obtain full dimension $p\times p$, so that:
$$([\Abf_B]^p )_{ij}=\left\{ \begin{array}{rl}
a_{ij} & \text{if } \{i,j\}\in B,\\
0 &  \text{if } \{i,j\}\in V_{\setminus B}.
\end{array}\right.$$

Hereafter we consider  $X\sim\Ncal (\mu, \Omegab)$ with dimension $p$. If $X$ is associated with a decomposable graph $\G$, the Markov property \ref{def:markov} applies across the decomposition and reflects in a multiplicative property for the density, and additive property for the concentration matrix. More precisely, considering a perfect sequence of the cliques of $\G$ as in Definition \ref{def:seq}, we have that
 $$f(X)=\dfrac{\prod_{C\in \mathcal{C}} f(X_C)}{\prod_{S\in \mathcal{S}} f(X_S)^{\nu(S)}}.$$
This general expression of the density across decomposition of the graph is very helpful in likelihood computations, especially when some hypotheses are made on the structure of $\G$. The following lemma gives the expressions for $\Omegab$ and its determinant:
 \begin{lemma} In a decomposable Gaussian graphical model with precision matrix $\Omegab = \Sigmab^{-1}$ of dimension $p$:
 \begin{equation*}
 \left\{
 \begin{array}{rl}
 \Omegab &= \sum_{C\in \mathcal{C}} \big[(\Sigmab_C)^{-1}\big]^p - \sum_{S\in \mathcal{S}} \nu(S)  \big[(\Sigmab_S)^{-1}\big]^p \\\\
 |\Omegab| &=\dfrac{\prod_{C\in \mathcal{C}} |\Sigmab_C|^{-1}}{\prod_{S\in \mathcal{S}} |\Sigmab_S|^{-\nu(S)}}
 \end{array} \right.
\end{equation*}  
where $\mathcal{C}$ is the set of cliques of $\G$ and $\mathcal{S}$ the set of separators with multiplicities $\nu$ in any perfect sequence. 
\end{lemma}

The GGM framework provides exact results about maximum likelihood estimators (MLE) of the mean vector, the precision matrix, its determinant and the terms of the covariance matrix corresponding to edges in the graph. They all depend on the sufficient statistic that is the sum of squared deviations, defined as follows where $X^i$ denotes the $i^{th}$ sample of $X$:
\begin{definition}
Denoting $\xbar{X}$ the empirical mean of the multivariate Gaussian $X$, the sum of squared deviations matrix, also known as total sum of squares,  is given by:
$$SSD = \sum_{i=1}^n (X^i - \xbar{X})(X^i - \xbar{X})^\intercal = X^\intercal X - n  \xbar{X} \,\xbar{X}^\intercal.$$
\end{definition}
The following theorems \ref{mle:sig} and \ref{mle:ome} give the MLE estimators in the GGM framework using $SSD=(ssd_{ij})_{ij}$.
 
\begin{theorem}\label{mle:sig}
In the Gaussian graphical model,  the MLE of the unknown mean and covariance matrix exist with probability one if $n>p$. When they exist, the estimate of the mean is $\widehat{\mu} = \xbar{X}$, and the estimate of the unknown covariance matrix $\Sigmab$ is determined as the unique solution of the system of equations
\begin{equation*}
 \left\{
 \begin{array}{rl}
 n\widehat{\sigma}_{jj} = ssd_{jj}&, j\in V\\
 n\widehat{\sigma}_{k\ell} = ssd_{k\ell}&, k\sim\ell, (k,\ell) \in V
 \end{array} \right.
\end{equation*}  
which also satisfies the model restriction $\omega_{k\ell} =0 \iff k\nsim \ell $.
\end{theorem}

\begin{theorem}\label{mle:ome}
In a Gaussian graphical model with graph $\G$, the MLE of the precision matrix exists with probability one if $n>max_{C\in\mathcal{C}}|C|$. It is then given as
$$\widehat{\Omegab} = n \bigg\{\sum_{C\in\mathcal{C}} \big[(SSD_C)^{-1}\big]^p-\sum_{S\in\mathcal{S}} \nu(S) \big[(SSD_S)^{-1}\big]^p\bigg\},$$
where $\mathcal{C}$ is the set of cliques of $\G$ and $\mathcal{S}$ the set of separators with multiplicities $\nu$ in any perfect sequence. The determinant of the estimate can be calculated as
$$|\widehat{\Omegab}| = n^p \dfrac{\prod_{C\in\mathcal{C}}|(SSD_C)^{-1}|}{\prod_{S\in\mathcal{S}} (|(SSD_S)^{-1}|)^{\nu(S)}}.$$
\end{theorem}
 
The stronger condition $n>p$ of Theorem \ref{mle:sig} is only sufficient, whereas the condition $n>max_{C\in\mathcal{C}}|C|$ of Theorem \ref{mle:ome} is necessary for the existence of all estimators, as it ensures the positive definiteness of all $SSD_C$ (otherwise some might not have full rank).  

 All of the above maximum likelihood estimators require the precise knowledge of the graph structure and is therefore rarely available. However relying on simple structures (e.g. spanning trees) greatly simplifies the expressions and make their use possible.

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Network inference from Gaussian data}
\label{infGGM}
  Network inference here refers to the inference of the conditional dependence structure of multiple variables jointly observed in a series of sites. The interactions between the species are unknown, therefore this is not to be confused with the art of inferring a network parameters (e.g. sign and strength of the interactions) from observations of repeated interactions.
  
In the special framework of GGM, conditional dependence relations are perfectly represented by the non-nul entries of the precision matrix $\Omegab$, as specified in Proposition \ref{ggm:faith}. Therefore the general approach to network inference in GGM is then to perform a sparse estimation of $\Omegab$.

 \subsubsection{Penalized estimation}
 In a standard linear regression model with data matrix $\Yb$ and covariates matrix $\Xb$, penalized estimation can be used to select predictors. The most popular penalized method is the Lasso \citep{lasso} which applies the $\ell_1$ penalty and estimates the $\beta$ coefficients as the solution of:
 $$\argmin_{\beta \in \mathds{R}^p} \big\{ ||\Yb - \Xb\beta ||^2_2 + \lambda ||\beta||_1\big\}, \qquad ||\beta||_1=\sum_{i=1}^p |\beta_i|,$$
 where $\lambda$ is a penalty called the regularization parameter.  The form of the $\ell_1$ penalty forces some coefficients to exactly equal zero. Taking advantage of the regression interpretation in GGM reminded in Proposition \ref{ggm:reg}, a method for inferring non-nul elements of the precision matrix is to run separate penalized regressions of each node against all others and possible covariates, as in \citet{MeB06}. \\
 
 A joint estimation of these regression models is performed by the widely used Graphical Lasso (glasso) \citep{FHT08}. The glasso introduces sparsity in the precision matrix by imposing Lasso penalties on its entries. It is an iterative procedure which aims a sparse MLE of the penalized precision matrix $\Omegab$.  The log-likelihood of a GGM writes:
 $$L(\Omegab) = \log |\Omegab| +\tr{\Yb^\intercal \Yb \Omegab}+cst .$$
 The precision matrix is then estimated as the matrix which maximizes the penalized log-likelihood:
 $$\argmax_{\Omegab\geq 0} \big\{L(\Omegab) - \lambda ||\Omegab||_1\big\}, \qquad ||\Omegab||_1 = \sum_{j\neq k} |\omega_{jk}|.$$
The glasso  selects the network edges by forcing some of them to zero in the precision matrix. The greater the penalty $\lambda$, the less edges are included in the network and therefore choosing $\lambda$ is critical for the glasso. The selection of $\lambda$ can be performed using classical tools such as cross-validation, Akaike information criterion (AIC),  Bayesian information criterion (BIC) or its extended version.  Another popular approach is to perform a stability selection with StARS \citep{stars}, which fosters the network robustness under random sampling of the input data.

 %builds on the regression interpretation of the partial correlation

 
 \subsubsection{Averaging on trees}\label{tree:aver}

%chow-liu dans le cas gaussien
  Another inference strategy is to assume a sparse structure. The sparsest connected graphs are spanning trees. If a multivariate variable $\Ybf_i$ of dimension $p$  is faithful to a spanning tree $T $, its likelihood writes as a factorization on the nodes and the edges of $T$:
  $$p_T(\Ybf_i) = \prod_{j=1}^p p(Y_{ij}) \prod_{jk\in T} \frac{p(Y_{ij},Y_{ik})}{p(Y_{ij})p(Y_{ik})}.$$

The quotient in the product on the edges is close to the mutual information between two variables, which is the Küllback-Leibler divergence between the bivariate density and the product of their marginal densities. It can be seen as a measure of the independence between a pair of variables.   The Chow-Liu algorithm \citep{ChowLiu} finds the tree structure which maximizes the above likelihood from observations. The maximal spanning tree is thus the tree with the maximal bivariate dependencies on its edges. Note that the Chow-Liu algorithm is not restricted to the Gaussian case, even if it is the most general one.\\
   %To do so it maximizes the above distribution by defining the edges weights as the mutual information between the variables, which is the Küllback-Leibler divergence between the bivariate density and the product of the two marginal densities. 
 
  %mélange d'arbres : distribution d'arbre et proba d'arêtes
  %The graphical model is assumed to be a tree $T^*$ and itself treated as a latent parameter of the model.

However the data conditional dependency structure is unlikely to be a tree in general. Assuming the dependency tree $T$ to be random provides with a more flexible approach.  Considering any distribution on the space of spanning trees, we now define the tree averaging model (also called tree aggregation or mixture of trees) which is a sum on a set of trees weighted by their respective probability to be the random tree $T$. This is classically  formulated as follows:

%A way to explore the spanning tree space is to consider a mixture model on spanning trees, which amounts to assume the dependency tree $T$ to be random. 

% take advantage of all the spanning tree properties of section \ref{trees}. While performing network inference using tree averaging,  the underlying graphical model $T^*$ is treated as a latent parameter of the model.
%is assumed to be random as opposed to the Chow-Liu and penalized framework previously presented.  
 
 \begin{definition}[Tree averaging \citep{MixtTrees}]
 Considering a collection of $m$ spanning trees $T_k$, $k\in\{1,...,m\}$ and a choice variable $\zb$ taking values in $\{1,...,m\}$, the distribution of an observed variable $\Ybf$ following a  mixture of trees  is defined as:
$$ p(\Ybf) = \sum_{k=1}^m p(\zb=k) p(T_k).$$
Conditionally on $\zb$, the distribution of $\Ybf$ is a tree.
 \end{definition}

Averaging on trees then means to treat the underlying tree $T$ as a latent parameter of the model. In the context of GGM  the use of tree averages for network inference relies on a Gaussian tree mixture, which is a mixture model of GGM that are faithful to trees.
% The latent Gaussian parameters $\Zbf$ follow a Gaussian mixture on the space of spanning trees $\mathcal{T}$, where each Gaussian component is faithful to a spanning tree.
\begin{definition}[Gaussian tree mixture]\label{treemixt}
The distribution of a variable $\Ybf$ is a Gaussian tree mixture on the space of $m$ spanning trees if it writes as follows:
$$p(\Ybf) = \sum_{k=1}^m  p(T_k) p(\Ybf\mid T_k),\qquad  \Ybf\mid T_k\sim\Ncal (0, \Omegab_{T_k}^{-1}), \;\; \forall k\in\{1,...,m\}.$$
Conditional on $T_k$, $\Ybf$ is faithful relative to this tree and follows a multivariate Gaussian with the corresponding precision matrix $\Omegab_{T_k}$.
\end{definition}
 
Instead of a subset $m$, considering an average on all possible spanning trees %allows for an exhaustive exploration of the space of spanning trees  $\mathcal{T}$. An interesting 
%considering all spanning trees at the same time is the possibility to
presents with the advantage of  resorting to edges probabilities. Indeed the probability for an edge to be in the tree $T$ can be defined as the sum of the probabilities of all trees containing this edge:
$$\mathds{P}\{kl\in T\} = \sum_{\substack{T\in \mathcal{T}\\ T \ni kl}} p_{\Wbf}(T).$$
The final output of a strategy using an average on trees for network inference is not the latent tree presenting with the highest probability, but the matrix filled with the edges probabilities obtained by summing on all trees. Therefore the output is a weighted network which has no reasons to be shaped as a tree itself.\\

%with decomposable distribution 
We now detail a useful distribution on the space of trees: the decomposable distribution, which assigns a strictly positive weight to each edge  \citep{MixtTrees,MeilaJaak}.
\begin{definition}[Decomposable tree distribution] \label{decompdistrib}
For any symmetric weight matrix $\Wbf$ with all positive entries, a decomposable  distribution for the tree $T$ is defined as follows:
$$p_{\Wbf}(T) = \prod_{kl \in T} w_{kl} / W ,$$
where  $W$ is a normalizing constant with sum-product form as in Theorem \ref{thmm:MTT}.
\end{definition}
Therefore  under the decomposable distribution, the probability of a tree is proportional to the product of its edges weights. Such distribution is stable under a multiplicative transformation applied to its weight matrix, which is particularly useful during the implementation. The Chow-Liu algorithm actually maximizes the decomposable distribution with the mutual information as the edges weights.\\

%algebraic advantage
Defining a tree averaging  with the decomposable tree distributions $p_{\Wbf}(T)$ presents several advantages. First, it introduces the quantity $\sum_{T\in\mathcal{T}}\prod_{k\ell\in T} w_{kl}$, which is a sum-product form easily handled thanks to Theorem \ref{thmm:MTT} and Lemma \ref{lemm:Meila}. Then, the following algebraic result from \cite{kirshner} states that under a decomposable distribution it is possible to compute all edges probabilities at the same time, and this at the cost of the inversion of the  Laplacian matrix of the edges weights.

\begin{lemma} [\cite{kirshner}] \label{lem:Kirshner}
    Let $p_{\Wbf}$ be a decomposable tree distribution with symmetric and positive weight matrix $\Wbf$. Taking the symmetric matrix $\Mbf$ as defined in Lemma  \ref{lemm:Meila}, the probability for an edge $kl$ to be in the tree $T$ writes:
$$\mathds{P}\{kl\in T\} =  w_{kl}\: \Mbf_{kl}.$$
\end{lemma}

Unlike the previous penalized approach which assumes a fixed graph, tree-averaging methods consider the graph as a latent and random tree. Coupled with decomposable tree distributions, they offer an efficient and exhaustive exploration of the space of spanning tree graphs. Tree averaging has recently been used in the context of GGM inference in the Bayesian setting \citep{SRS19} and for the inference of unobserved data \citep{RAR19}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inference of incomplete data models}
Incomplete data can refer either to unobserved variables due to experimental constraints, or latent variables in the model. The Expectation-Maximization (EM) algorithm \citep{DLR77} is the classic approach to compute the maximum likelihood estimate in presence of hidden variables. In this section we present the general principles of the EM algorithm, as well as its variational version. We let $\Ybf$ denote the observed incomplete data, $\Zbf$ the hidden variables, $p$ their joint distribution with parameter vector $\thetab$.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%
 \subsection{Expectation-Maximization algorithm}
 The EM algorithm is an iterative procedure which aims at maximizing the log-likelihood $\log p_\theta(\Ybf)$ of the observed data $Y$. It is based on a decomposition of the incomplete data likelihood:
 $$\log p_{\thetab} (\Ybf) = \Esp_\thetab [\log p_{\thetab} (\Ybf,\Zbf)\mid\Ybf] - \Esp_\thetab [\log p_{\thetab} (\Zbf\mid\Ybf) \mid \Ybf]. $$
This relation links the observed likelihood and the complete likelihood $\log p_{\thetab} (\Ybf,\Zbf)$. The second term is actually the entropy of the latent variables $\Zbf$ given the observed data.
The iteration $t+1$ of the EM algorithm then consists of the two following steps:
 \begin{description}
 \item [E step:] Given $\thetab^t$, compute $\Esp_{\thetab^t}[\log p_\thetab (\Ybf, \Zbf)\mid \Ybf]$ as a function of $\theta$,
 \item [M step:] Update $\thetab$ as $\thetab^{t+1} = \argmax_\thetab \{ \Esp_{\thetab^t}[\log p_\thetab (\Ybf, \Zbf)\mid \Ybf]\}$.
 \end{description} 
There is no guarantee as for the convergence of the estimate of $\thetab$ towards a global maximum of $\log p_\theta(\Ybf)$. However an important property of the EM, obtained as a consequence of the Jensen's inequality,  is that the log-likelihood of the observed data $\log p_{\thetab} (\Ybf)$ increases with the iterations of the EM algorithm \citep{DLR77}: $\log p_{\thetab^{t+1}} (\Ybf) > \log p_{\thetab^t} (\Ybf)$.\\
 
 Another formulation of the EM assumes a distribution $q$ for the hidden variables \citep{NH98} and defines a  lower bound $\bound$ as:
\begin{align*}
\bound(\thetabf, q) &= \log p_\thetab (\Ybf) - KL(q(\Zbf) || p_\thetab(\Zbf\mid\Ybf))\\
&=\Esp_q [\log p_\theta (\Ybf,\Zbf)] - \Esp_q[\log q(\Zbf)].
\end{align*}
 where $KL(q(\Zbf)||p(\Zbf)) = \Esp_q[\log q(\Zbf) - \log p(\Zbf)]$ is the Küllback-Leibler divergence. In this formulation, the link between the maximization of $\bound$ and that of the likelihood is made clear. Iteration $t+1$ of the EM can then be written as a double maximization:
  \begin{description}
 \item [E step:]  $q^{t+1}=\argmax_q \{ \bound(\thetabf^t, q^t)\} = \argmin_q \{ KL(q(\Zbf) || p_\thetab(\Zbf\mid\Ybf))\},$
 \item [M step:] $\thetab^{t+1} = \argmax_\thetab \{ \bound(\thetabf^t, q^{t+1})\} = \argmax_\theta \{ \Esp_q [\log p_\theta (\Ybf,\Zbf)]\}.$
 \end{description} 

The EM algorithm has become a classic approach to perform inference of models involving hidden or latent variables. In particular it is widely used to infer mixture models, for example mixture of trees \cite{MixtTrees}. It corresponds to the case where no constraint is imposed to the $q$ distribution: the solution of the E step is thus $q(\Zbf)=p(\Zbf\mid\Ybf)$, and the Küllback-Leibler divergence is canceled.  Unfortunately the conditional density $p_\thetab(\Zbf\mid\Ybf)$ is not always available, in which case one might resort to a variational computation.
 

%%%%%%%%%%%%%%%%%%%%%%%%%% 
 \subsection{Variational estimation}
 Variational inference is a method for approximating conditional distributions \citep{JGZ99,WaJ08,BKM17}. It is widely used in Bayesian settings to approximate posterior densities, that is looking for $q(\theta)\approx p(\theta\mid \Ybf)$, as an alternative for Monte-Carlo Markov Chain (MCMC) sampling. Another use of the variational inference is to approximate the conditional distribution of a latent variable, thus looking for $q(\Zbf) \approx p(\Zbf\mid \Ybf)$. These are two examples of the same general approximation problem.
 
  When the conditional density of latent variables given the observed data cannot be computed, the variational version of the EM reduces the search space of the approximate distribution $q$  to a set $Q$ in the E step, and chooses a divergence $D$ to define the lower bound. Doing so results in a variational approximation of $p_\thetab(\Zbf\mid\Ybf)$. Therefore, in a  Variation EM (VEM) algorithm, the E step is replaced by a variational-E (VE) step, which computes an approximate conditional distribution as the solution of the following optimization problem:
 $$q^{t+1}=\argmin_{q\in Q} \big\{ D\big(q^t(\cdot) \mid\mid p_{\theta^t}(\cdot\mid\Ybf)\big)\big\}.$$
 
 In other words the idea behind variational estimation is to choose a divergence $D$ and a family of densities $Q$, and then find the member of this family which is the closest to the target distribution $p_\thetab(\Zbf\mid\Ybf)$ as measured by the divergence. Their exists a variety of divergence measures (see \citet{M05} for an overview), and the Küllback-Leibler divergence as presented in the previous section is generally preferred for maximum-likelihood estimations. The complexity of the optimization is determined by the choice of the variational family. The set $Q$ can be chosen as a parametric family, for example the set of Gaussian distributions: $Q=\{q(\Zbf)=\Ncal(\Zbf; m,S)\}$. Another widely used choice for $Q$ is the set of product-form or factorized distributions, where the latent variables are mutually independent: for $K$ hidden variables $Q=\{q(\Zbf)=\prod_{k=1}^K q_k(Z_k)\}$.  These two sets can be combined into the set of factorized Gaussian distributions. \\ 
 
The optimization problem of the VE step is most commonly solved using coordinate ascent. With a factorized approximate distribution $q(\Zbf)$, coordinate ascent consists in iteratively optimize each hidden factor $q_k(Z_k)$ of the product while keeping the other fixed. A result from \citet{beal} directly gives the optimal solution for each variable of a factorized distribution and is given in Proposition \ref{beal} below. It was originally formulated in the Bayesian setting, and as such this result applies to hidden variables as well as Bayesian random parameters. Hidden variables in proposition \ref{beal} can refer to either latent factors, unobserved variables, or Bayesian parameters.

 \begin{prop}[\citet{beal}]\label{beal}
In a variational EM using a factorized approximate distribution and the Küllback-Leibler divergence with observed data $\Ybf$ and $K$ hidden variables $\Zbf=\{z_1,...,z_K\}$, the solution of the VE step optimization problem is the following. At step $t+1$ and for any $k$ in $\{1,...,p\}$ the optimal variational marginal distribution $q_k$ is proportional to the exponential of the expected log of the complete joint density:
$$ q_k^{(t+1)}(z_k)  \propto \exp \left\{ \Esp_{q_{\setminus k}^t} \left[ \log p_{\thetabf^{t+1}}(\Ybf, \Zbf) \right] \right\}.$$

\end{prop}

Proposition \ref{beal} shows that choosing $Q$ as the set of factorized distributions leads to a so-called "mean-field" approximation, allowing a clearer presentation and easier use of variational inference algorithms with factorized approximations. Variational inference has been widely used in very diverse fields, among which computational biology and genetics \citep{CS12,RSP14}. Its study in network stochastic block model analysis yielded some specific theoretical results \citep{CDP12,BCC13}, however establishment of general theoretical guarantees of variational inference is an active research area, as underlined in \citet{BKM17}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Network inference from count data}
 Inferring a network  from multivariate counts requires to model the joint distribution of discrete variables. Discrete distributions are not particularly practical to work with and their exist few options of multivariate formulations. A solution is then to use Gaussian random latent parameters and rely on a mixed generalized multivariate model, known as Joint Species distribution Model in ecology \citep{WBO15}. This model then allows to resort to the GGM framework for network inference. The last part of this section summarizes several modeling strategies for network inference, as well as the one we adopted.
%  As greatly summarized in the review by   \citet{inouye}, there exist three main model classes to do so, which each use either marginal Poisson, mixture of Poisson or conditional Poisson distributions. This section gathers the two first model classes in the framework of joint modeling. While focusing on abundance data, most of what follows could be easily transcribed for presence/absence data.
  
 
  \subsection{Modeling multivariate count data}
  A convenient distribution for modeling discrete data is the Poisson, which possesses some interesting features as it is part of the exponential family. As explained in \citet{inouye}, the first idea for extending the Poisson distribution to the multidimensional framework is it to build a multidimensional Poisson from univariate Poisson distributions. The multivariate Poisson of dimension $p$ is formulated as a collection of variables that are the sums of $p$ univariate Poisson distributions. This construction  takes advantage of the subsequent summation of the Poisson distribution parameters \citep{T54}. It writes easily and presents the advantage of preserving the marginal Poisson distributions, but the set of possible correlations between variables is restricted. The extension of this formulation allows  for a full covariance structure modeling, and can be inferred using an EM algorithm (see \citet{K03}). However  a significant modeling restriction is that only positive dependencies can be modeled, which is generally too strong an assumption. Moreover in higher dimensions the computational cost dramatically increases, and the writing becomes intractable.  \\

Fortunately, there exist other more general ways of jointly modeling discrete data,  the majority of which transposes the problem in the Gaussian framework, where they can take advantage of the easy handling of the multivariate Gaussian distribution and its practical properties. These methodologies can be gathered in the framework of multivariate mixed models, which are a general class of statistical parametric models for counts of multiple variables. More specifically, they are an extension of the generalized linear model and thus readily handle covariates and offsets. Additionally multivariate mixed models capture the correlation between the variables, and resort to Gaussian random effects and a link function to do so. Hereafter we consider the input abundance data matrix $\Yb$, and denote by the index $i$ the rows (samples) and the index $j$ the columns (species) of $\Yb$. A multivariate random effect $\Zbf_i$ is associated with each data sample $\Yb_i$.  Denoting $\xb_i$ the vector of covariates with regression coefficients $\thetab_j$, $o_{ij}$ a possible offset  and $g(\cdot)$ a link function, the mean abundance $m_{ij}$ can be specified in a general manner as follows:
$$g(m_{ij}) = o_{ij}+\xb_i^\intercal  \thetab_j + Z_{ij}, \qquad \Zb_i \sim \Ncal(0, \Sigmab).$$

The art of multivariate mixed modeling then resides in the specification of $g$ and the random effects. We here focus on counts, but binary data (e.g. presence/absence) could also be handle through mixed modeling. This model is known as a Joint Species Distribution Model in ecology,  where the data at hand represents the sampled measures on a set of species at different sites.  The following section presents some of the most used settings  for multivariate mixed modeling of counts in ecology and microbiology. 

\subsection{Shift to Gaussian universe} 
\subsubsection{Transformations}
The first idea when it comes to transposing counts into the Gaussian framework is to transform them. Doing so allows to analyze the data without resorting to discrete modeling. A common transformation of counts is to apply the log function, with a  small constant $c$ added to counts to avoid zeros (pseudo-counts). In this case the mean of $\log(Y_{ij}+c)$ is then assumed to follow a multivariate Gaussian distribution. 

In genomics, high throughput sequencing yields  samples $\Yb_i$  of observed counts of $p$ species or taxa that are constrained by the library size $\kappa_i$ such that $\sum_j Y_{ij}=\kappa_i$ \citep{GMP17}. Thus the available observations are not absolute counts but relative counts, and each sample can be viewed as a compositional vector \citep{A82} sampled from the simplex
$$\mathcal{S}_i^p = \bigg\{ \boldsymbol{u}=(u_1,...,u_p)\in \mathds{R}^p\bigg\rvert\sum_{i=1}^p u_i = \kappa_i\bigg\}.$$
There exist several transformations for compositional data to change the $\kappa_i$-sum constraint, the most popular being the centered log-ratio transformation  which writes:
$$clr(\Yb)=\left(\log \frac{Y_1}{m(\Yb)}, ..., \log \frac{Y_p}{m(\Yb)}\right), $$
where $m(\Ybf) = \left(\prod_{j=1}^p Y_j \right)^{1/p}$ is the geometric mean. A clr-transformed vector is thus constrained to a zero sum.  In the framework of joint modeling, the clr function can be used as a link function to account for covariates and offsets. Recent works on network inference use the clr transformation  \citep{kurtz}, or the logistic normal distribution \citep{AS80} to model relative counts \citep{gcoda}.

%The transposed compositional representation is to consider  genes counts across all samples and to represent the relative gene expression per site, namely for gene $j$ and sample $i$: $\frac{Y_{ij}}{\sum_i Y_{ij}}$. Inputs are then the expression profiles of each gene \citep{RMR18}, onto which transformations  can be applied. 

\subsubsection{Copulas}
  
Copulas are  joint  distributions which map the marginal  cumulative distribution functions of a multivariate random vector. Their use with continuous distributions originates from the Sklar's theorem which states that a joint continuous distribution can be decomposed into a copula and marginal distributions. Conversely the pairing of a copula and some marginal distributions yields a valid joint distribution. As copulas fully describe the dependence structure, these models allow to fully separate the modeling of marginal distributions from the modeling of dependencies. In their original form, they only apply to continuous marginals, and their extension to discrete data has been controversial in particular about identifiability, and computationally challenging \citep{F17}. However recent developments made the use of  Gaussian copula coupled with discrete marginal distributions possible \citep{PCJ12,PHW18}, opening the way to the application of copulas in the analysis of multivariate count data \citep{AVP19}. 

In this context, a first step estimates each $p$ marginal univariate discrete distributions parameters while accounting for covariates and possible offsets. Then the distribution of the response Gaussian copula is computed as a $p-$dimensional integrand (see \citet{PHW18}). \citet{PWT19} showed that  Gaussian copulas are a relevant and promising approach to the problem of network inference from abundance data, even if the computation cost remains substantial as it requires importance sampling as well as Monte Carlo Expectation Maximization algorithms. One way of taking advantage of the copula theory without having to actually estimate the joint distribution is to use copulas as a data transformation. The nonparanormal transformation of counts is a semiparametric Gaussian copula \citep{LLW09}. It is estimated in a computational efficient manner by marginally transforming the variables using ranks and univariate Gaussian quantiles. This transformation however is sensitive to ex-aequo and 0 counts. \cite{CWL18} resorts to the nonparanormal approach in an attempt to estimate network parameters varying across a gradient of covariates. 

\subsubsection{Latent variables}

Latent variables enable the modeling of multivariate discrete data using a hierarchical setting where random discrete variables are modeled conditionally to random latent variables.  Two specifications of  latent variables stand out in community ecology \citep{WBO15}:  the Multivariate Generalized Linear Mixed Model (GLMM) \citep{OHS10, PTM14}, and the Latent Variable Model (LVM)  \citep{OAP16, OTN17}. The difference between these models lies in the dimension of their respective random effects: there are as many latent variables as there are species in the GLMM, whereas in the LVM their number is a parameter of the model. 


In more details, a GLMM models the latent variables and abundances as follows:
 \begin{align*}
 Y_{ij}\mid \Zb_i &\sim F(m_{ij}, \phi_j)\\
 \Zb_i &\sim \Ncal(0, \Sigmab),
 \end{align*} 
 where $F$ is a distribution with mean $m_{ij}$ and dispersion parameter $\phi_j$. There are as many latent variables as observed ones. In the case of LVM the random effect associated with the mean abundance $m_{ij}$ is a linear combination of a set of $K<p$  latent variables, and is generally specified as:
 $$g(m_{ij}) = o_{ij}+\xb_i^\intercal  \thetab_j +\sum_{k=1}^K Z_{ik} \lambda_{kj},$$
 where the matrix $\Lambdab$ gathers the factors loadings $\lambdab_j$ in columns. Abundances are then assumed to follow
 \begin{align*}
 Y_{ij}\mid \Zb_i &\sim F(m_{ij}, \phi_j)\\
\Zb_i&\sim \Ncal(0, \boldsymbol{I}).
 \end{align*}
The covariance matrix of the latent layer of random effects  is then of rank $K$ and can be computed from the factor loadings as $\Sigmab=\Lambdab^\intercal \Lambdab$. The LVM is appreciated for its small number of parameters to estimate \citep{OTN17}.\\

The Poisson log-normal distribution (PLN, \citet{AiH89}) is a GLMM where $F$ is the Poisson distribution and $g$ the log function.  This model can also be seen as an infinite Poisson mixture as presented in \citet{inouye}, with a log-normal distribution for the parameters. The PLN model presents the advantage of having  closed form moments:
 \begin{align*}
 \Esp[\Yb_i] &= e^{\mu_i + \frac{1}{2} \sigma_{ii}} = \alpha_i\\
 \Var(\Yb_i) &=  \alpha_i+ \alpha_i^2(e^{\sigma_{ii}} -1)\\
 \Cov (\Yb_i, \yb_j) &=  \alpha_i\, \alpha_j (e^{\sigma_{ij}} -1).
 \end{align*}
An interesting property of the multivariate Poisson log-normal law is the conservation of the correlation signs between the observed and latent layers: $\text{sign}(\text{Cor}(Y_{ij}, Y_{ik})) = \text{sign}(\text{Cor}(Z_{ij}, Z_{ik}))$, including the null correlation. Moreover if $\sigma_{jk}$ is null, then it can be shown that the bivariate distribution $p(Y_{ij},Y_{ik})$ factorizes under the product of its marginals, yielding the marginal independence of the two variables \citep{AiH89}. This model can be  estimated using variational inference in \citet{CMR18}. \citet{MInt} and \citet{CMR19} use the PLN distribution in the context of network inference.
 
 \subsection{Network inference}
 
As stated in the previous section, most methodologies to infer networks from count data first model count data in a way to transpose the problem in the Gaussian setting. There they take advantage of the GGM framework detailed in section \ref{ggm:prop} to perform network inference. The majority of them then use the penalized estimation using either the LASSO or the glasso as detailed in section \ref{infGGM}. Table \ref{tab:infmeth} below summarizes the modeling of counts and inference strategy adopted by the methods of interest in this work, which have all been previously mentioned. Note that we focused on methods using the framework of Graphical Models, and included LITree and saturnin even if they do not model count data for they have been a source of inspiration for this work.\\

\begin{table}[H]
\begin{tabular}{l|l|ccc|cc|}
\cline{3-7}
\multicolumn{1}{l}{} &\multicolumn{1}{l|}{}& \multicolumn{3}{c|}{ Gaussian shift} & \multicolumn{2}{c|}{Network inference}  \\ \cline{2-7} 
&\multicolumn{1}{l|}{Ref.}& Trans.  & Copulas &LV & Penalized & Trees  \\ \hline
\multicolumn{1}{|l|}{SpiecEasi} &\citet{kurtz}& x &  &  & x &   \\ \hline
\multicolumn{1}{|l|}{MInt} & \citet{MInt}&  &  & x & x &    \\ \hline
\multicolumn{1}{|l|}{gCoda} & \citet{gcoda}& x &  &  & x &   \\ \hline
\multicolumn{1}{|l|}{MRFcov} &\citet{CWL18}&  &  x&  & x &   \\ \hline
\multicolumn{1}{|l|}{ecoCopula} &\citet{PWT19}&  &  x&  & x &    \\ \hline
\multicolumn{1}{|l|}{PLNnetwork} &\citet{CMR19}&  &  &x  & x &    \\ \hline
\multicolumn{1}{|l|}{saturnin} &\citet{SRS19}& \multicolumn{1}{l}{\cellcolor[HTML]{d8d8d8}} & \multicolumn{1}{l}{\cellcolor[HTML]{d8d8d8}} & \multicolumn{1}{l|}{\cellcolor[HTML]{d8d8d8}} & \multicolumn{1}{l}{} & \multicolumn{1}{c|}{x}  \\ \hline
\multicolumn{1}{|l|}{LITree} &\citet{RAR19}& \multicolumn{1}{l}{\cellcolor[HTML]{d8d8d8}} & \multicolumn{1}{l}{\cellcolor[HTML]{d8d8d8}} & \multicolumn{1}{l|}{\cellcolor[HTML]{d8d8d8}} & \multicolumn{1}{l}{} & \multicolumn{1}{c|}{x}  \\ \hline
\end{tabular}
\caption{Network inference methods of interest: their strategy to model counts (transformation, copulas or latent variables), and choice for network inference (penalized estimation or tree average).}
\label{tab:infmeth}
\end{table} 

\subsubsection*{Inference in the observed layer $\Ybf$}
We presented methods relying on a Gaussian layer for the network inference.  Another mathematical framework for graphs with discrete data are the Poisson Graphical Models, which are  graphical models of the observed layer $\Ybf$ directly, and not the $\Zbf$. These models use properties of the exponential family to derive graphical models with node-conditional Poisson distributions. However this particular class of graphical models entails a major drawback: only negative dependencies relationships can be modeled. As this is a too unrealistic assumption when it comes to species interactions we did not discuss this solution here, although see \citet{YRA13} and \citet{IRD16} for valuable extensions of this model. In Chapter 4 we propose a model for network inference in the layer of  $\Ybf$, using tree averaging.
 
  \subsection{Proposed methodology}
 The network inference method which is developed in the following chapters of this work models count data $\Ybf$ using the aforementioned PLN distribution, including offsets and covariates:
 $$ Y_{ij}\mid \Zbf_i\sim \Pcal (\exp (o_{ij}+\xb_i^\intercal \thetab_j + Z_{ij})), \;\;\; (Y_{ij} \independent) \mid \Zbf_i .$$
 
The  graph underlying the latent layer of Gaussian parameters $\Zbf$ is assumed to be a random tree, so that the conditional distribution is faithful to $T$: 
 $$\Zbf_i\mid T  \sim \Ncal (0, \Omegab_T), \;\;\;  \{\Zbf_i\}_i \text{ iid}.$$
The tree $T$ is assumed to be distributed following a  decomposable distribution  as defined in Definition \ref{decompdistrib}:
 $$ T\sim \prod_{kl\in T} \beta_{kl} / B, \;\;\;  B= \sum_{T\in\mathcal{T}} \prod_{kl \in T} \beta_{kl}.$$
 

The approach then relies on tree averaging for the network inference as detailed in section \ref{tree:aver}. Namely, the latent Gaussian parameters $\Zbf$  follow a Gaussian tree mixture on the whole space of spanning trees $\mathcal{T}$, where each Gaussian component is faithful to a spanning tree as is specified in Definition \ref{treemixt}. This writes $$\Zb_i \sim \sum_{T\in \mathcal{T}} p(T) \Ncal (\Zbf_i\mid T; 0, \Omegab_T).$$

Chapter 2 details the inference of this model. Chapter 3 considers  the case of additional dimensions in the latent layer $\Zbf$, that is when $\Zbf_i$ is actually of dimension $p+r$, where $r>0$ is a number of unobserved actors.
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  