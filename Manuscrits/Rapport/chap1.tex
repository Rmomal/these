

This first chapter  details the technical background extracted from the literature which is used in the latter chapters. It begins by covering all the required notions of graphical models, with a technical focus on Gaussian graphical models and spanning tree structures. Then, a state of the art both of how to infer networks from abundance data both in genomics and in ecology is drawn. The last part details the Expectation-Maximization algorithm and in particular its variational interpretation. This chapter is meant to be independent from the rest of this work, and to this aim some properties are repeated later on.


 \section{Graphical Models}
 A graphical model is classically described as a probabilistic model which conditional dependence structure is given by a graph. This first section gives the general framework of graphical models, definitions and properties are adapted from \citet{Lau96}.  Then the special case of  spanning tree graphs is presented.
 %%%%%%%%%%%%%%%%%%%%%%%%%%
 \subsection{General framework}
 A graph is defined as a pair $\G=(V,E)$ such that $V$ is a finite set of vertices, and the set of edges $E$ is a subset of $V\times V$ such that vertices are not linked to themselves and there are no multiple edges between two vertices. For any given pair of nodes $(k,\ell)$, we denote an edge by $k\sim\ell$ and an absence of edge by $k\nsim \ell$. In the literature, $V$ can be composed of both quantitative and qualitative variables. However here we  only use variables of one kind (quantitative) and so $V$ is called pure. The following definitions apply to the particular case of a pure vertex set $V$ and an undirected graph $\G$.

Let's first consider a subset $A$ of the vertex set $V$. $A$ is said to be \textit{complete} if all the nodes it contains are linked with each other. If $A$ is additionally of maximal size, it is then called a \textit{clique}. This definition makes the expression "maximal clique" a pleonasm. The \textit{subgraph} $\G_A$ defined by $A$  is obtained from $\G$ by keeping edges with both endpoints in $A$. Furthermore if $A$, $B$ and $S$ are disjoint subsets of $V$, $S$ is said to \textit{separate} $A$ from $B$ if any path from $\G_A$ to $\G_B$ intersects with $\G_S$. The following notions of decomposable graphs and perfect sequences are central in the definition of a graphical model.
 
 \begin{definition}[Proper decomposition]
 A triple $(A, B, C)$ of disjoint subsets of the pure vertex set $V$ of an undirected graph $\G$ form a decomposition of $\G$ if $V=A\cup B \cup C$, and if $C$ satisfies:
 \begin{enumerate}[label=(\roman*)]
 \item $C$ is a complete subset of  $V$,
 \item $C$ separates $A$ from $B$.
 \end{enumerate}
 If $A$ and $B$ are both non-empty, the decomposition is proper.
 \end{definition}
 
 
 \begin{definition}[Decomposable graph]
 An undirected graph is decomposable if it is complete, or if there exists a proper decomposition $(A, B, C)$ into decomposable subgraphs $\G_{A\cup B}$ and $\G_{B\cup C}$.
 \end{definition}
 This recursive definition proves to be very helpful in demonstrations of decomposable graphs properties, but not to prove that a graph is decomposable. The notion of triangulation will help.
 
 \begin{definition}[Triangulated graph]
 An undirected graph $\G$ is said to be triangulated if loops in $\G$ connect three nodes at most.
 \end{definition} 
 In other words in a triangulated graph, the loop of maximal size will form a triangle.
 \begin{definition}[Perfect sequence]\label{def:seq}
 Let $B_1,...,B_k$ be a sequence of subsets of the pure set of vertex $V$ of the undirected graph $\G$. Let $H_j=B_1\cup .. \cup B_j$, and $S_j = H_{j-1} \cap B_j$. The sequence is perfect if it satisfies the following conditions:
 \begin{enumerate}[label=(\roman*)]
 \item for all $i>1$ there is a $j<i$ such that $S_i \subseteq B_j$ (running intersection property),
 \item each $S_i$ is a complete subset.
 \end{enumerate}
 $H_j$ are called the histories and $S_j$ the separators.
 \end{definition}
% \begin{definition}[perfect numbering]
% A perfect numbering of the vertices $V$ of $\G$ is a numbering $\alpha_1,...,\alpha_k$ such that the sequence $B_1,...,B_k$ with 
% $$B_j = cl(\alpha_j) \cap \{\alpha_1,...,\alpha_j\}, j\geq 1 $$ is a perfect sequence. This implies that each $B_j$ is a complete subset.
% \end{definition}
Note that the empty set can be included in the set of separators. An example where separators are complete but the sequence is not perfect is the following.
\begin{figure}[H]
 \begin{center}
	\begin{tikzpicture}	
      \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw]
		\node[basic, label=above:1] (A1) at (-1*\edgeunit,0*\edgeunit) {};
		\node[basic, label=below:2] (A2) at (0,0) {};
		\node[basic, label=below:3] (A3) at (1*\edgeunit,0*\edgeunit) {};
		\node[basic, label=above:4] (A4) at (2*\edgeunit,0) {};
		\node[basic, label=right:5] (A5) at (0.5*\edgeunit,0.8*\edgeunit) {};
		\path (A1) edge  (A2)
        (A2) edge [] (A3)
        (A3) edge [] (A4)
        (A2) edge [] (A5)
        (A5) edge [] (A3);
	\end{tikzpicture} 
 \caption{An undirected graph with 5 nodes.}
  \label{ex:graph1}
    \end{center}
\end{figure}
Consider the graph in Figure \ref{ex:graph1}. A sequence is $B=\big\{B_1=\{1,2\}, B_2=\{3,4\},B_3=\{2,3,5\}\big\}$, with the separators $S_2=\varnothing, S_3=\{2,3\}$. As $\{2,3\}$ is not a subset of any of the $B_i$ with $i$ less than 3, the running intersection property is violated and $B$ is not perfect.
 \begin{definition}[multiplicity of a separator]
 The multiplicity $\nu (S)$ of the separator $S$ is an index counting the number of times $S$ occurs in a perfect sequence.
 \end{definition}
 
\begin{figure}[H]
 \begin{center}
	\begin{tikzpicture}	
      \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw]
		\node[basic, label=left:1] (A1) at (-0.2*\edgeunit,-0.7*\edgeunit) {};
		\node[basic, label=left:2] (A2) at (-0.4,0) {};
		\node[basic, label=left:3] (A3) at (-0.2*\edgeunit,0.7*\edgeunit) {};
		\node[basic, label=above:4] (A4) at (1*\edgeunit,0) {};
		\node[basic, label=right:5] (A5) at (1.6*\edgeunit,1.2*\edgeunit) {};
		\node[basic, label=right:6] (A6) at (2.3*\edgeunit,0.7) {};
		\node[basic, label=right:7] (A7) at (2.4*\edgeunit,0*\edgeunit) {};
		\node[basic, label=right:8] (A8) at (1.5*\edgeunit,-0.8*\edgeunit) {};
		\path (A1) edge  (A2)
        (A2) edge [] (A4)
        (A4) edge [] (A1)
        (A2) edge [] (A3)
        (A4) edge [] (A3)
        (A4) edge [] (A5)
        (A4) edge [] (A6)
        (A5) edge [] (A6)
        (A7) edge [] (A6)
        (A4) edge [] (A7)
        (A4) edge [] (A8);
	\end{tikzpicture} 
 \caption{An undirected decomposable graph with 8 variables}
  \label{ex:graph}
    \end{center}
\end{figure}
 Let's consider the graph of Figure \ref{ex:graph}. A perfect sequence of its cliques is 
$C_1=\{1,2,4\}$, $C_2=\{2,3,4\}$, $C_3=\{4,5,6\}$, $C_4=\{4,6,7\}$, $C_5=\{4,8\}$. The corresponding separators are $S_2=\{2,4\}$, $S_3=S_5=\{4\}$, $S_4=\{4,6\},$ which gives the following multiplicities: $\nu(\{2,4\})=1$,  $\nu(\{4,6\})=1$, $\nu(\{4\})=2$.  


\begin{prop}
\label{decomp}
The following conditions are equivalent for an undirected graph $\G$:
\begin{enumerate}[label=(\roman*)]
\item $\G$ is decomposable.
\item The cliques of $\G$ can form a perfect sequence.
\item $\G$ is triangulated.
\end{enumerate}
\end{prop}

 This property is the most useful, as it states that as soon as a graph does not present with cycles of size 4 or more, it is decomposable on its cliques. This decomposition is essential to the graphical model properties, as we shall see in Gaussian graphical models.
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Characterization of conditional independence}
The notion of conditional independence is central in the theory of graphical models. All that follows focuses on the case of variables associated with positive and continuous densities. Let's consider three random variables $X,Y$ and $Z$ with joint distribution $f$. The conditional independence of $X$ and $Y$ conditional on $Z$ is linked to the factorization of $f$. More precisely: 
$$X \independent Y \mid Z \iff f(x,y,z) = f(x,z) \, f(y,z)/f(z). $$
Conditional independence can be understood in terms of information. That is, "$X$ is independent of $Y$, given $Z$" means that knowing $Z$, $Y$ will not bring any new information on $X$, and conversely. The Markov property below is the link between conditional independence of variables and its representation in an undirected graph. Let's consider a collection of random variables $(X_v)_{v\in V}$ taking values in probability spaces $(\mathcal{X}_v)_{v\in V}$, and $\mathcal{X}=\otimes_{v\in V} \mathcal{X}_v$. We further denote for any subset $A$ of $V$, $X_A=(X_v)_{v\in A}$, and $\G$ a graph with vertex set $V$. 

\begin{definition}[Markov property]\label{def:markov}
A probability measure P on $\mathcal{X}$ is \textit{global} Markov relative to the undirected graph $\G$ if for any triple $(A, B, S)$ of disjoint subset of $V$ such that:
 $$ \text{S separates A from B } \Rightarrow X_A\independent X_B \mid X_S$$
 If additionally $X_A\independent X_B \mid X_S \Rightarrow \text{S separates A from B}  $, then P is \textit{faithful} Markov and $\G$ perfectly describes the conditional dependence structure of P.
\end{definition}
Therefore if a distribution is only global Markov, its related graph is possibly too dense: it can contain edges between variables which are actually conditionally independent from one another. 
\begin{definition}[Factorization]\label{def:fact}
A probability measure P on $\mathcal{X}$ factorizes according to $\G$ if it has density $f$ with respect to measure $\mu = \otimes_{v\in V} \mu_v$, where $f$ can be written as
$$f(x) = \prod_{c\in \mathcal{C} }\psi_c(x),$$
where $\mathcal{C}$ denotes the set of cliques of $\G$, and for any subset $C$ of the vertex set $V$, $\psi_C$ is a positive function of $X_C$ only.
\end{definition}
The following theorem links definitions \ref{def:markov} and \ref{def:fact} for positive and continuous distributions:

\begin{theorem}[Hammersley and Clifford] \label{thm:ham}
For any undirected graph $\G$ and probability distribution P with positive and continuous density $f$ with respect to a product measure $\mu$, it holds that:
$$P\text{ factorizes according to } \G \iff P\text{ is global Markov relative to }\G $$
\end{theorem}

This equivalence means in practice that writing a density in a product form on some combinations of its variable helps identify conditional independence relations.

%%%%%%%%%%%%%%%%%%%%%%%%%%
 \subsection{Spanning trees}
Trees are graphs with no cycles, meaning that all cliques are edges only. When a tree connects all the nodes, it is called a spanning tree. This particular type of graph is both the sparsest connected graph, and the most connected graph without loops. This feature makes it a decomposable graph by definition, following condition $(iii)$ of Proposition \ref{decomp}.  The following result is obtained directly.

\begin{prop}
If $T$ is a spanning tree, all of its separators are nodes. For any node $k$ of $T$ we denote $d(k)$ its degree. Then its multiplicity in any perfect sequence is $\nu(k) = d(k)-1$.
\end{prop}
 
 
 The study of the spanning tree structure began in the latter part of the $19^{th}$ century. Then, Arthur Cayley established that the total number of spanning trees of a complete graph with $p$ nodes is $p^{p-2}$. Kirchhoff generalized this result in a theorem known as the "all-minor" theorem, using the notion of graph Laplacian matrix.
  \begin{definition}[Laplacian matrix]
 \label{laplacian}
 The Laplacian matrix $\Qbf$ of a symmetric matrix $\Wbf=[w_{jk} ]_{1\leq j,k\leq p}$ is as follows :

\[
 [\Qbf]_{jk}  =\begin{cases}
    -w_{jk}  & 1\leq j<k \leq p\\
    \sum_{u=1}^p w_{ju} & 1\leq j=k \leq p.
    \end{cases}
\]
When $\Wbf$ is a graph adjacency matrix, $\Qbf$ is called the graph Laplacian matrix and its diagonal is composed of the degrees of each node.
 \end{definition}
 
 Hereafter we denote $\Abf^{uv}$ the squared matrix $\Abf$ deprived from its $u$th row and $v$th column, and remind that the $(u, v)$-minor of $\Abf$ is the determinant of this deprived matrix, namely $|\Abf^{uv}|$.
 
 \begin{theorem}[Kirchhoff's theorem]\label{th:kir}
 For any graph $\G$, let  $\Qb_\G$ denote its graph Laplacian matrix. Then for any  pair of nodes $\{u,v\}$, the total number of spanning trees in $\G$ is equal to  $|\Qb_\G^{uv}|$.
\end{theorem}  

 
The total number of spanning trees in any graph can thus be computed in polynomial time. Theorem \ref{th:kir} was extended for weighted graphs in the late 20th century as follows, where $\mathcal{T}$ denotes the spanning tree space on $V=\{1,...,p\}$:

\begin{theorem}[Matrix Tree Theorem  \cite{matrixtree,MeilaJaak}] \label{thmm:MTT}
    For any symmetric weight matrix $\Wbf$ with all entries in $\mathds{R}^+$, the sum over all spanning trees of the product of the weights of their edges is equal to any minor of its Laplacian $\Qbf$. That is, for any $1 \leq u, v \leq p$,
 
   \[
    W := \sum_{T\in\mathcal{T}} \prod_{(j, k)\in T} w_{jk} = |\Qbf^{uv}|.
    \]
   
\end{theorem}    

Consequently, the operation of summing over all spanning trees can be carried out in a computationally efficient way.  Theorem \ref{thmm:MTT} actually gives the solution to the computation of the sum on trees of a product function on the edges of a spanning tree, which can be judiciously used. \cite{MeilaJaak} then build on this result to provide a close form expression for the derivative of the sum-product $W$ with respect to each entry of the input weight matrix $\Wbf$.   Without loss of generality, we choose $\Qbf^{11}$.

\begin{lemma} [\cite{MeilaJaak}] \label{lemm:Meila}
    Define the entries of the symmetric matrix $\Mbf$ as
\[    
 [\Mbf]_{jk} =\begin{cases}
    \left[(\Qbf^{11})^{-1}\right]_{jj} + \left[(\Qbf^{11})^{-1}\right]_{kk} -2\left[(\Qbf^{11})^{-1}\right]_{jk} & 1< j<k \leq p\\
    \left[(\Qbf^{11})^{-1}\right]_{jj} & k=1, 1< j \leq p  \\
    0 &  j=k .
    \end{cases}
\] 
it then holds that
$$\partial_{w_{jk}} W = [\Mbf]_{jk}  \times W.$$
\end{lemma}

As we will see later on, it is possible to carry out network inference using average on trees, in which case  Theorem \ref{thmm:MTT} and Lemma \ref{lemm:Meila} are particularly useful.

%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%
 \section{Gaussian Graphical Models}
 Gaussian graphical models describe the conditional dependency structure of a  multivariate Gaussian distribution. An interesting property of the multivariate distribution is that it naturally factorizes on the non-null entries of its precision matrix: 
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%
 \subsection{Specific properties}
 \begin{prop}\label{pp:ggm1}
 Let $X\sim \mathcal{N}(\mu, \Sigma)$ with precision matrix $\Omega=\Sigma^{-1}$ and $X_k$ denote the $k^{th}$ column of $X$. Then the associated density $f$ factorizes as:
 $$f(X) \propto \prod_{k,\ell, \omega_{kl}\neq 0} \exp(-X_{k}\omega_{k\ell}X_{\ell}/2)$$
 \end{prop}
 Therefore as stated by Theorem \ref{thm:ham}, $f$ is global Markov relative to an undirected graph $\G$, the edges of which are determined by the non-null entries of its precision matrix $\Omega$.  Another useful property of the multivariate Gaussian distribution is:
 
 \begin{prop}\label{pp:ggm2}If  $X\sim \mathcal{N}_V(\mu, \Omega^{-1})$, then it holds for any $k,\ell\in V$ with $k\neq \ell$ that
$$X_k \independent X_\ell \mid X_{V\setminus\{k,\ell\}} \iff \omega_{k\ell}=0 $$
 \end{prop}
  Proposition \ref{pp:ggm2} is not a consequence of proposition \ref{pp:ggm1}, and comes directly from the fact that the $2\times 2$  covariance matrix of variables $k$ and $l$ conditional on all others expresses in terms of the precision matrix : $$\Sigma_{k,l\mid V\setminus\{k,l\}} = |\Omega_{\{k,l\}}|^{-1} \left(\begin{array}{cc}
\omega_{kk} & -\omega_{kl}\\
-\omega_{kl} & \omega_{ll}  
  \end{array}\right).$$
Combining the above propositions \ref{pp:ggm1} and \ref{pp:ggm2}, we deduce that a conditional independence implies a null entry in $\Omega$, which in turn implies no edge between the corresponding nodes in $\G$. This proves the following result.

\begin{prop}[Faithfulness property]\label{ggm:faith}
 Let $X\sim \mathcal{N}(\mu, \Omega^{-1})$ with associated density $f$, $\G$ the graph which edges represent the non-nul entries of $\Omega$. Then $f$ is faithful Markov relative to $\G$,  and it holds that:
 $$k\nsim \ell \iff  X_k \independent X_\ell \mid X_{V\setminus\{k,\ell\}} \iff \omega_{k\ell}=0.$$
\end{prop} 


 Now that the GGM framework is in place, we turn to its parameters estimation made possible by the faithfulness property.

%%%%%%%%%%%%%%%%%%%%%%%%%%
 \subsection{Parameters estimation}
First some notations: for any  squared  matrix $A$ of dimension $V$ and $B$ a subset of $V$, we let $A_B$ refer to the bloc $B$ of $A$: $A_{B}=(a_{ij})_{\{i,j\}\in B}$.   $[A_B]^V$ then denotes the matrix obtained by filling up with zero entries to obtain full dimension $V\times V$, so that:
$$([A_B]^q )_{ij}=\left\{ \begin{array}{rl}
a_{ij} & \text{if } \{i,j\}\in B\\
0 &  \text{if } \{i,j\}\in V_{\setminus B}
\end{array}\right.$$

  If $X$ is associated with a decomposable graph $\G$, the Markov property \ref{def:markov} decomposes accordingly and reflects in a multiplicative property for the density, and additive property for the concentration matrix. Considering a perfect sequence of the cliques of $\G$ as in Definition \ref{def:seq}, we have that
 $$f(X)=\dfrac{\prod_{C\in \mathcal{C}} f(X_C)}{\prod_{S\in \mathcal{S}} f(X_S)^{\nu(S)}}.$$
This general expression of the density across decomposition of the graph is very helpful in likelihood computations, especially when some hypotheses are made on the structure of $\G$. The following lemma gives the expressions for $\Omega$ and its determinant:
 \begin{lemma} In a decomposable Gaussian graphical model with precision matrix $\Omega = \Sigma^{-1}$ of dimension $V$, a perfect sequence of the cliques $\mathcal{C}$ of $\G$ gives:
 \begin{equation*}
 \left\{
 \begin{array}{rl}
 \Omega &= \sum_{C\in \mathcal{C}} \big[(\Sigma_C)^{-1}\big]^V - \sum_{S\in \mathcal{S}} \nu(S)  \big[(\Sigma_S)^{-1}\big]^V \\\\
 |\Omega| &=\dfrac{\prod_{C\in \mathcal{C}} |\Sigma_C|^{-1}}{\prod_{S\in \mathcal{S}} |\Sigma_S|^{-\nu(S)}}
 \end{array} \right.
\end{equation*}  
\end{lemma}

The GGM framework allows exact results for the estimation of maximum likelihood estimators of the mean vector, the precision matrix, its determinant and the terms of the covariance matrix corresponding to edges in the graph. They all depend on the sum of squared deviations, defined as follows where $X^i$ denotes the $i^{th}$ sample of $X$:
\begin{definition}
The sum of squared deviations matrix of a multivariate Gaussian $X$, also known as total sum of squares,  is given by:
$$SSD = \sum_{i=1}^n (X^i - \xbar{X})(X^i - \xbar{X})^\intercal = X^\intercal X - n  \xbar{X} \xbar{X}^\intercal$$
\end{definition}
Hereafter we let $ssd_{ij}$ denote then $ij$ entry of the $SSD$ matrix.
\begin{theorem}[\citet{Lau96} Theorem 5.3]
In the Gaussian graphical model,  the maximum likelihood estimates of the unknown mean and covariance matrix exist if the $SSD$ matrix is positive definite. If $n>|V|$ this happens with probability one. When they exist, the estimate of the mean is $\widehat{\mu} = \xbar{X}$, and the estimate of the unknown covariance matrix $\Sigma$ is determined as the unique solution of the system of equations
$$n\widehat{\sigma}_{jj} = ssd_{jj}, n\widehat{\sigma}_{k\ell} = ssd_{k\ell}, j,k,\ell \in V, k\sim\ell,$$
which also satisfies the model restriction $\omega_{k\ell} =0 \iff k\nsim \ell $.
\end{theorem}

\begin{theorem}[\citet{Lau96} Proposition 5.9]
In a Gaussian graphical model with graph $\G$, the maximum likelihood estimate of the precision matrix exists with probability one if $n>max_{C\in\mathcal{C}}|C|$. It is then given as
$$\widehat{\Omega} = n \bigg\{\sum_{C\in\mathcal{C}} \big[(SSD_C)^{-1}\big]^V-\sum_{S\in\mathcal{S}} \nu(S) \big[(SSD_S)^{-1}\big]^V \bigg\},$$
where $\mathcal{C}$ is the set of cliques of $\G$ and $\mathcal{S}$ the set of separators with multiplicities $\nu$ in any perfect sequence. The determinant of the estimate can be calculated as
$$|\widehat{\Omega}| = n^{|V|} \dfrac{\prod_{C\in\mathcal{C}}|(SSD_C)^{-1}|}{\prod_{S\in\mathcal{S}} (|(SSD_S)^{-1}|)^{\nu(S)}}$$
\end{theorem}
 
The necessary condition $n>max_{C\in\mathcal{C}}|C|$  ensures the positive definiteness of all $SSD_C$ (otherwise some might not have full rank). All of the above maximum likelihood estimators require the precise knowledge of the graph structure and is therefore rarely available. However relying on spanning tree structures greatly simplifies the expressions and make their use possible.

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Network inference from Gaussian data}
  Network inference here refers to the inference of the conditional dependence structure of multiple variables jointly observed in a series of sites. The interactions between the species are unknown, therefore this is not to be confused with the art of inferring a network parameters (e.g. sign and strength of the interactions) from observations of repeated interactions.
  
In the special framework of GGM, conditional dependence relations are perfectly represented by the non-nul entries of the precision matrix $\Omegab$, as specified in Proposition \ref{ggm:faith}. The general approach to network inference in GGM is then to perform a sparse estimation of $\Omegab$.

 \subsubsection{Penalized estimation}
 In a standard linear regression model, penalized estimation is used to select predictors. The most popular penalized method is the Lasso \citep{lasso} which apply the $\ell_1$ penalty and estimates the $\beta$ coefficients as the solution of:
 $$\argmin_{\beta \in \mathds{R}^p} \big\{ ||\Yb - \Xb\beta ||^2_2 + \lambda ||\beta||_1\big\}, \qquad ||\beta||_=\sum_{i=1}^p |\beta_i|,$$
 where $\lambda$ is a penalty parameter. The form of the penalty forces some coefficients to be exactly zero. Penalized estimation in GGM is performed by the Graphical Lasso (glasso) \citep{glasso}, which introduces sparsity in the precision matrix by imposing Lasso penalties on its entries. The log-likelihood of a GGM writes:
 $$L_\lambda(\Omegab) = \log |\Omegab| +\tr{\Yb^\intercal \Yb \Omegab}+cst .$$
 The precision matrix is then estimated as the matrix which maximizes the penalized log-likelihood:
 $$\argmax_{\Omegab\geq 0} \big\{L_\lambda(\Omegab) - \lambda ||\Omegab||_1\big\}, \qquad ||\Omegab||_1 = \sum_{j\neq k} |\omega_{jk}|.$$
The glasso gives a sparse penalized maximum likelihood estimator of $\Omegab$, and select the network edges by forcing some of them to zero in the precision matrix. The greater the penalty $\lambda$, the less edges are included in the network and therefore choosing $\lambda$ is critical for the glasso. The selection of $\lambda$ can be performed using classical tools such as cross-validation, Akaike information criterion (AIC),  Bayesian information criterion (BIC) or its extended version.  Another popular approach is to perform a stability selection with StARS \citep{stars}, which aims the network robustness under random sampling.

 %builds on the regression interpretation of the partial correlation

 
 \subsubsection{Averaging on trees}
  Another way to foster sparsity in the graph is to assume a sparse structure. Spanning trees are the sparsest connected graphs, and it is possible to computes the tree which maximizes the likelihood from observations using  the Chow-Liu algorithm \citep{ChowLiu}. However the data conditional dependency structure is unlikely to be a tree in general, and tree averages provide the opportunity to take advantage of all the spanning trees properties while being flexible on the data structure. \\

 The idea behind using an average on spanning trees to perform network inference  is that the underlying graphical model is supposed to be random and is itself treated as a latent parameter of the model. This graph is also supposed to be a tree and to follow a distribution which is decomposable on its edges as in the definition below.
\begin{definition}[decomposable tree distribution]
For any symmetric weight matrix $\Wbf$ with all positive entries, we denote $W$ the sum-product form defined in Theorem \ref{thmm:MTT} A decomposable  distribution for the tree $T$ is then defined as follows:
$$p_{\Wbf}(T) = \prod_{k,\ell \in T} w_{k\ell} / W ,$$
so that the probability of a tree is proportional to the product of its edges weights.
\end{definition}

An interesting property of a decomposable tree distribution is that it is stable under a multiplicative transformation applied to its weight matrix. This is particularly useful during the implementation.

Once a probability is assigned to each tree of the spanning tree space, the probability of an edge in a tree can be defined as the sum of the probabilities of trees containing this edge. Namely for the tree $T$:
$$\mathds{P}\{kl\in T\} = \sum_{T\in \mathcal{T}} p_{\Wbf}(T).$$
The following lemma  states that it is possible to compute all edges probabilities at the same time, and this at the cost of the inversion of the  Laplacian matrix of the edges weights.

\begin{lemma} [\cite{kirshner}] \label{lem:Kirshner}
    Let $p_{\Wbf}$ be a decomposable tree distribution with symmetric and positive weight matrix $\Wbf$. Taking the symmetric matrix $\Mbf$ as defined in Lemma  \ref{lemm:Meila}, the probability for an edge $kl$ to be in the tree $T$ writes:
 
$$\mathds{P}\{kl\in T\} =  w_{kl}\: \Mbf_{kl}$$
\end{lemma}

The final output of a strategy using an average on trees is not the latent tree presenting with the highest probability, but the matrix filled with the edges probabilities obtained by summing on all trees. Therefore the output is a weighted network which has no reasons to be shaped as a tree itself.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inference of incomplete data models}
Incomplete data can refer either to unobserved variables due to experimental constraints, or latent variables in the model. The Expectation-Maximization (EM) algorithm \citep{DLR77} is the classic approach to compute the maximum likelihood estimate in presence of hidden variables. In this section we present the general principles of the EM algorithm, as well as its variational version. We let $\Ybf$ denote the observed incomplete data, $\Zbf$ the hidden variables, $p$ their joint distribution with parameter vector $\thetab$.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%
 \subsection{Expectation-Maximization algorithm}
 The EM algorithm is an iterative procedure which maximizes the expected value on  the complete log-likelihood $\log p_\thetab (\Ybf, \Zbf)$ conditional on the observed data $Y$. The iteration $t+1$ consists in two steps:
 \begin{description}
 \item [E step:] estimate $\Esp_{\thetab^t}[\log p_\thetab (\Ybf, \Zbf)\mid \Ybf]$
 \item [M step:] $\thetab^{t+1} = \argmax_\thetab \{ \Esp_{\thetab^t}[\log p_\thetab (\Ybf, \Zbf)\mid \Ybf]\}$
 \end{description} 
 The  log-likelihood $\log p_{thetab} (\Ybf)$ increases with the iterations of the EM algorithm \citep{DLR77}. \\
 
 
 Another formulation of the EM assumes a distribution $q$ for the hidden variables \citep{NH98} and defines a  lower bound $\bound$ as:
 $$\bound(\thetabf, q) = \log p_\thetab (\Ybf) - KL(q(\Zbf) || p_\thetab(\Zbf\mid\Ybf)),$$
 where $KL(p||q) = -\Esp_q[\log q - log p]$ is the Küllback-Leibler divergence. In this formulation, the link between the maximization of $\bound$ and that of the likelihood is made clear. The iteration $t+1$ of the EM can then be written as a double maximization :
  \begin{description}
 \item [E step:]  $q^{t+1}=\argmax_q \{ \bound(\thetabf^t, q^t)\}$
 \item [M step:] $\thetab^{t+1} = \argmax_\thetab \{ \bound(\thetabf^t, q^{t+1})\}$
 \end{description} 
 
In the E step, the search space for $q^{t+1}$ is the whole set of distributions on the definition space of the hidden variables. The solution is the distribution which cancels the Küllback-Leibler divergence, and that is $p_\thetab(\Zbf\mid\Ybf)$. Unfortunately the latter is not always available, in which case one might resort to a variational computation.
 

%%%%%%%%%%%%%%%%%%%%%%%%%% 
 \subsection{Variational version}
 When $p_\thetab(\Zbf\mid\Ybf)$ cannot be computed, the variational version of the EM reduces the search space of the $q$ distribution to a set $Q$, which is chosen to ease the computations of the M step. Doing so results in a variational approximation of $p_\thetab(\Zbf\mid\Ybf)$. Therefore in a  Variation EM (VEM) algorithm, the E step is replaced by a VE step, which is  
 $$q^{t+1}=\argmax_{q\in Q} \{ \bound(\thetabf^t, q^t)\}.$$
 
 A widely used choice for $Q$ are product-form distributions, such that for $K$ hidden variables $Q=\{q(\Zbf)=\prod_{k=1}^K q_k(z_k)\}$. This set of distribution results in a so-called a mean-field approximation. 
 
In the variational bayesian setting, \citet{beal} derive update equations  for the hidden variables as well as the parameters. Random parameters are a specificity of the bayesian framework, and in fact they can be viewed as other hidden variables. Hidden entities in proposition \ref{beal} below can thus be latent factors, unobserved variables, or bayesian parameters.

 \begin{prop}[\citet{beal}]\label{beal}
 Let's consider a variational EM with observed data $\Ybf$ and $K$ hidden variables $\Zbf=\{z_1,...,z_K\}$. If the approximation is mean-field, then at step $t+1$ and for any $k$ in $\{1,...,p\}$ the update of the variational marginal distribution $q_k$ in the VE step is:
$$ q_k^{(t+1)}(z_k)  \propto \exp \left\{ \Esp_{q_{\setminus k}^t} \left[ \log p_{\thetabf^{t+1}}(\Ybf, \Zbf) \right] \right\}.$$
\end{prop}


 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Network inference from count data}
  Inferring a network directly from count data would require to jointly model abundances with a multivariate discrete distribution. Unfortunately such distribution is not available, and the majority of techniques transposes the problem in the Gaussian framework, enabling to take advantage of the multivariate Gaussian distribution easy-handling and convenient properties.
  
  \subsection{Joint species distribution models}
Joint species distribution models (JSDM) are a general class of statistical parametric models for multiple discrete variables, for example the abundances of multiple species. JSDMs are an extension of the generalized linear model and thus readily handle covariates and offsets. Additionally they capture the correlation between the variables, and resorts to Gaussian random effects and a link function to do so. Hereafter we consider the input abundance data $\Yb$, and denote by the index $i$ the rows (samples) and the index $j$ the columns (species) of $\Yb$. A multivariate random effect $\zb_i$ is introduced to each data sample $\yb_i$.  Denoting $\xb_i$ the vector of covariates with regression coefficients $\thetab_j$, $o_{ij}$ a possible offset  and $g(\cdot)$ a link function, the mean abundance $m_{ij}$  in a JSDM can be specified in a general manner as follows:
$$g(m_{ij}) = o_{ij}+\xb_i^\intercal  \thetab_j + z_{ij}, \qquad \zb_i \sim \Ncal(0, \Sigmab).$$

The art of JSDMs then resides in the specification of $g$ and the random effects. The following section presents some of the most used settings in ecology and genomics.

\subsection{Shift to Gaussian universe} 
\subsubsection{Transformations}
The first idea when it comes to transposing counts into the Gaussian framework is to transform them. Doing so allows to analyze the data without resorting to discrete modeling. A common transformation of counts is to apply the log function, with a  small constant $c$ added to counts to avoid zeros (pseudo-counts). In this case the mean abundance is simply modeled as $\log(m_{ij}+c)$ and assumed to follow a multivariate Gaussian distribution. 

In genomics, high throughput sequencing yields  samples $\yb_i$  of observed counts of $p$ species or taxa that are constrained by the library size $\kappa_i$, a technical parameter of the experiment, such that $\sum_j y_{ij}=\kappa_i$ \citep{GMP17}. Thus the available observations are not absolute counts but relative counts, and each sample can be viewed as a compositional vector \citep{A82} sampled from the simplex
$$\mathcal{S}_i^p = \bigg\{ \boldsymbol{u}=(u_1,...,u_p)\in \mathds{R}^p\bigg\rvert\sum_{i=1}^p u_i = \kappa_i\bigg\}.$$
The transposed compositional representation is to consider  genes counts across all samples and to represent the relative gene expression per site, namely for gene $j$ and sample $i$: $\frac{y_{ij}}{\sum_i y_{ij}}$. Inputs are then the expression profiles of each gene \citep{RMR18}, onto which transformations  can be applied. 
There exists several transformations for compositional data to change the $\kappa_i$-sum constraint, the most popular being the centered log-ratio transformation  which writes:
$$clr(\yb)=\left(\log \frac{y_1}{m(\yb)}, ..., \log \frac{y_p}{m(\yb)}\right), $$
where $m(\cdot)$ is the geometric mean. A clr-transformed vector is constrained to a zero sum. In the framework of joint modeling, the clr function can be used as a link function to account for covariates and offsets. Recent works on network inference use the clr transformation  \citep{kurtz}, or the logistic normal distribution \citep{AS80} to model relative counts \citep{gcoda}.


\subsubsection{Copulas}
Copulas are  joint  distributions which map the marginal  cumulative distribution functions of a multivariate random vector.  They are used to describe the dependence between random variables, and in particular for Gaussian copulas the correlation matrix can be estimated. In their original form, they only apply to continuous marginals, and their extension to discrete data has been controversial \citep{F17}. However recent developments made the use of  Gaussian copula coupled with discrete marginal distributions possible \citep{PCJ12,PHW18}, opening the way to the application of copulas in the analysis of multivariate count data \citep{AVP19}. In this context, a first step estimates each $p$ marginal univariate discrete distributions parameters while accounting for covariates and possible offsets. Then the distribution of the response Gaussian copula is computed as a $p-$dimensional integrand (see \citet{PHW18}). \citet{PWT19} proved that  Gaussian copulas are a relevant and promising approach to the problem of network inference from abundance data, even if the computation cost remains substantial as it requires importance sampling as well as Monte Carlo expectation maximization algorithms.

\subsubsection{Latent variables}

Latent variables enable the modeling of multivariate discrete data using a hierarchical setting where random discrete variables are modeled conditionally to random latent variables.  Two specifications of  latent variables stand out in community ecology \citep{WBO15}:  the Multivariate Generalized Linear Mixed Model (GLMM) \citep{OHS10, PTM14}, and the Latent Variable Model (LVM)  \citep{OAP16, OTN17}. The difference between these models lies in the dimension of their respective random effects: there are as many latent variables as there are species in the GLMM, whereas in the LVM their number is a parameter of the model. 

In more details, in a LVM the random effect associated to the mean abundance $m_{ij}$ is a linear combination of a set of latent variables, and is generally specified as:
 $$g(m_{ij}) = o_{ij}+\xb_i^\intercal  \thetab_j +\sum_{k=1}^K z_{ik} \lambda_{kj},$$
 where $K$ is the number of latent factors included in the model, and the matrix $\Lambdab$ gathers the factors loadings $\lambdab_j$ in columns. Abundances are then assumed to follow
 \begin{align*}
 y_{ij}\mid \zb_i &\sim F(m_{ij}, \phi_j)\\
\zb_i&\sim \Ncal(0, \boldsymbol{I}),
 \end{align*}
and the covariance matrix of the latent layer of random effects can be computed from the factor loadings as $\Sigmab=\Lambdab^\intercal \Lambdab$.

In the case of a GLMM, $K=p$ and latent variables and abundances are  modeled as:
 \begin{align*}
 y_{ij}\mid \zb_i &\sim F(m_{ij}, \phi_j)\\
 \zb_i &\sim \Ncal(0, \Sigmab),
 \end{align*} 
 where $F$ is a distribution with mean $m_{ij}$ and dispersion parameter $\phi_j$. The Poisson log-normal distribution (PLN, \citet{AiH89}), estimated in \citet{CMR18} and used in \citet{MRA20}, is a GLMM where $F$ is the Poisson distribution and $g$ the exponential function.  
 
 \subsection{Network inference}
 Network inference from count data generally moves the problem in the Gaussian framework  in order to  take advantage of all the aforementioned results on Gaussian graphical models. \\
 
 
\begin{table}[H]
\begin{tabular}{l|ccc|cc|cc|}
\cline{2-8}
 & \multicolumn{3}{c|}{ Gaussian shift} & \multicolumn{2}{c|}{Network inference} & \multicolumn{2}{c|}{Estimation} \\ \cline{2-8} 
 & Trans. & LV & Copulas & glasso & Trees & MCMC & EM \\ \hline
\multicolumn{1}{|l|}{gCoda} & x &  &  & x &  &  &  \\ \hline
\multicolumn{1}{|l|}{ecoCopula} &  &  & x & x &  & x &  \\ \hline
\multicolumn{1}{|l|}{(V)EMtree} &  & x &  &  & x &  & x \\ \hline
\multicolumn{1}{|l|}{MInt} &  & x &  & x &  &  &  \\ \hline
\multicolumn{1}{|l|}{MRFcov} &  &  &  & x &  &  &  \\ \hline
\multicolumn{1}{|l|}{SpiecEasi} & x &  &  & x &  &  &  \\ \hline
\end{tabular}
\end{table}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  