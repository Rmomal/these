%network inference in ecology
 \section{Graphical Models}
 A graphical model is a probabilistic model which conditional dependence structure is given by a graph.
 \subsection{Definitions}
 A graph is defined as a pair $\G=(V,E)$ such that $V$ is a finite set of vertices, and the set of edges $E$ is a subset of $V\times V$ such that vertices are not linked to themselves and there are no multiple edges between two vertices. In the literature, $V$ can be composed of both quantitative and qualitative variables. However here we  only use quantitative variables and so $V$ is called pure. The following definitions are adapted from \citet{Lau96}.
 \begin{definition}[Subgraphs]
 Let $A$ be a subset of the vertex set $V$. Then the subgraph $\G_A$ is obtained from $\G$ by keeping edges with both endpoints in $A$.
 \end{definition}
 
 \begin{definition}[Cliques and complete subsets]
 A subset of a graph is complete if all the vertices it contains are linked. A complete subset of maximal size is called a clique.
 \end{definition}
  \begin{definition}[separation]
  Let A,B and S be disjoint subset of V. S is said to separate A from B if any path from A to B intersects S.
  \end{definition}
 \begin{definition}[proper decomposition]
 A triple $(A, B, C)$ of disjoint subsets of the pure vertex set $V$ of an undirected graph $\G$ form a decomposition of $\G$ if $V=A\cup B \cup C$, and if $C$ verifies:
 \begin{enumerate}[label=(\roman*)]
 \item $C$ is a complete subset of  $V$,
 \item $C$ separates $A$ from $B$.
 \end{enumerate}
 If $A$ and $B$ are both non-empty, the decomposition is proper.
 \end{definition}
 
 \begin{definition}[decomposable graph]
 An undirected graph is decomposable if it is complete, or if there exists a proper decomposition $(A, B, C)$ into decomposable subgraphs $\G_{A\cup B}$ and $\G_{B\cup C}$.
 \end{definition}
 This recursive definition thus states that a decomposable graph can be successively decomposed into its cliques.
 
 \begin{definition}[perfect sequence]
 Let $B_1,...,B_k$ be a sequence of subsets of the pure set of vertex $V$ of the undirected graph $\G$. Let $H_j=B_1\cup .. \cup B_j$, and $S_j = H_{j-1} \cap B_J$. The sequence is perfect if it satisfies the following conditions:
 \begin{enumerate}[label=(\roman*)]
 \item for all $i>1$ there is a $j<i$ such that $S_i \subseteq B_j$ (running intersection property),
 \item each $S_i$ is a complete subset
 \end{enumerate}
 $H_j$ are called the histories and $S_j$ the separators.
 \end{definition}
% \begin{definition}[perfect numbering]
% A perfect numbering of the vertices $V$ of $\G$ is a numbering $\alpha_1,...,\alpha_k$ such that the sequence $B_1,...,B_k$ with 
% $$B_j = cl(\alpha_j) \cap \{\alpha_1,...,\alpha_j\}, j\geq 1 $$ is a perfect sequence. This implies that each $B_j$ is a complete subset.
% \end{definition}
Note that the empty set can be included in the set of separators. An example where separators are complete but the sequence is not perfect is the following: consider the sequence $B=\big\{B_1=\{1,2\}, B_2=\{3,4\},B_3=\{2,3,5\}\big\}$. The separators are $S_2=\varnothing, S_3=\{2,3\}$. As $\{2,3\}$ is not a subset of any of the $B_i$, the running intersection property is violated and $B$ is not perfect.
 \begin{definition}[multiplicity of a separator]
 The multiplicity $\nu (S)$ of the separator $S$ is an index counting the number of times $S$ occurs in a perfect sequence.
 \end{definition}
 
\begin{figure}[H]
 \begin{center}
	\begin{tikzpicture}	
      \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw]
		\node[basic, label=left:1] (A1) at (-0.2*\edgeunit,-0.7*\edgeunit) {};
		\node[basic, label=left:2] (A2) at (0.1,0) {};
		\node[basic, label=left:3] (A3) at (-0.2*\edgeunit,0.7*\edgeunit) {};
		\node[basic, label=above:4] (A4) at (1*\edgeunit,0) {};
		\node[basic, label=right:5] (A5) at (2.2*\edgeunit,0.7*\edgeunit) {};
		\node[basic, label=right:6] (A6) at (1.9*\edgeunit,0) {};
		\node[basic, label=right:7] (A7) at (2.2*\edgeunit,-0.7*\edgeunit) {};
		\node[basic, label=right:8] (A8) at (1*\edgeunit,-1.2*\edgeunit) {};
		\path (A1) edge  (A2)
        (A2) edge [] (A4)
        (A4) edge [] (A1)
        (A2) edge [] (A3)
        (A4) edge [] (A3)
        (A4) edge [] (A5)
        (A4) edge [] (A6)
        (A5) edge [] (A6)
        (A7) edge [] (A6)
        (A4) edge [] (A7)
        (A4) edge [] (A8);
	\end{tikzpicture} 
 \caption{An undirected decomposable graph with 8 variables}
  \label{ex:graph}
    \end{center}
\end{figure}
 Let's consider the graph of Figure \ref{ex:graph}. A perfect sequence of its cliques is 
$C_1=\{1,2,4\}, C_2=\{2,3,4\}, C_3=\{4,5,6\}, C_4=\{4,6,7\}, C_5=\{4,8\}. $
The corresponding separators are $S_2=\{2,4\}, S_3=S_5=\{4\}, S_4=\{4,6\},$
which gives the following multiplicities: $\nu(\{2,4\})=1,  \nu(\{4,6\})=1, \nu(\{4\})=2$.  

\begin{prop}
An undirected graph $\G$ is decomposable if and only if the set of its cliques can form a perfect sequence
\end{prop}

This characterization of a decomposable graph is used to get the factorization property.
 
\subsection{Characterization of conditional independence}
The notion of conditional independence is central to understand graphical models. Let's consider random variables $X,Y$ and $Z$ with joint distribution $f$. The conditional independence of $X$ and $Y$ conditional on $Z$ is linked to the factorization of $f$. More precisely: 
$$X \independent Y \mid Z \iff f(x,y,z) = f(x,z) \, f(y,z)/f(z). $$
It is very useful to understand conditional independence in terms of information. That is, $X \independent Y \mid Z$ means that knowing $Z$, $Y$ will not bring any new information on $X$, and conversely.

The Markov property below is the link between conditional independence of variables and its representation in an undirected graph. Let's consider a collection of random variables $(X_v)_{v\in V}$ taking values in probability spaces $(\mathcal{X}_v)_{v\in V}$, and $\mathcal{X}=\otimes_{v\in V} \mathcal{X}_v$. We further denote for any subset $A$ of $V$, $X_A=(X_v)_{v\in A}$, and $\G$ a graph with vertex set $V$. 

\begin{definition}[Markov property]
A probability measure P on $\mathcal{X}$ is \textit{global} Markov relative to the undirected graph $\G$ if for any triple $(A, B, S)$ of disjoint subset of $V$ such that:
 $$ \text{S separates A from B } \Rightarrow X_A\independent X_B \mid X_S$$
 If additionally $X_A\independent X_B \mid X_S \Rightarrow \text{S separates A from B}  $, then P is \textit{faithful} Markov and $\G$ perfectly describes the conditional dependence structure of P.
\end{definition}
\begin{definition}[Factorization]
A probability measure P on $\mathcal{X}$ factorizes according to $\G$ if it has density $f$ with respect to measure $\mu = \otimes_{v\in V} \mu_v$, where $f$ can be written as
$$f(x) = \prod_{c\in \mathcal{C} }\psi_c(x),$$
where $\mathcal{C}$ denotes the set of cliques of $\G$, and far any subset $a$ of the vertex set $V$, $\psi_a$ is a positive function of realizations of $X_a$ only.
\end{definition}

\begin{prop}[Hammersley and Clifford]
For any undirected graph $\G$ and probability distribution P with positive and continuous density $f$ with respect to a product measure $\mu$, it holds that:
$$P\text{ factorizes according to } \G \iff P\text{ is global Markov relative to }\G $$
\end{prop}

 
 
 
 
 \subsection{Gaussian Graphical Models}
  \begin{itemize}
 \item magical factorization on non-nul terms of precision matrix
 \item utilisation des définitions pour les expressions de la densité et precision K et det K
 \item ML estimators
 \end{itemize}

  \subsection{Spanning trees}
  \begin{itemize}
  \item definition and properties (multiplicity)
  \item specific writing of ML estimators ? or in article
  \item algebra 
  \end{itemize}
  
 We define the Laplacian matrix $\Qbf$ of a symmetric matrix $\Wbf=[w_{jk} ]_{1\leq j,k\leq p}$ as follows :

\[
 [\Qbf]_{jk}  =\begin{cases}
    -w_{jk}  & 1\leq j<k \leq p\\
    \sum_{u=1}^p w_{ju} & 1\leq j=k \leq p.
    \end{cases}
\]
 
We further denote $\Wbf^{uv}$ the matrix $\Wbf$ deprived from its $u$th row and $v$th column and we remind that the $(u, v)$-minor of $\Wbf$ is the determinant of this deprived matrix, that is $|\Wbf^{uv}|$.

\begin{theorem}[Matrix Tree Theorem  \cite{matrixtree,MeilaJaak}] \label{thm:MTT}
    For any symmetric weight matrix W with all positive entries, the sum over all spanning trees of the product of the weights of their edges is equal to any minor of its Laplacian. That is, for any $1 \leq u, v \leq p$,
 
   \[
    W := \sum_{T\in\mathcal{T}} \prod_{(j, k)\in T} w_{jk} = |\Qbf^{uv}|.
    \]
   
\end{theorem}    

In the following, without loss of generality, we will choose $\Qbf^{11}$. As an extension of this result, \cite{MeilaJaak} provide a close form expression for the derivative of $W$ with respect to each entry of $\Wbf$. 

\begin{lemma} [\cite{MeilaJaak}] \label{lem:Meila}
    Define the entries of the symmetric matrix $\Mbf$ as
 
\[    
 [\Mbf]_{jk} =\begin{cases}
    \left[(\Qbf^{11})^{-1}\right]_{jj} + \left[(\Qbf^{11})^{-1}\right]_{kk} -2\left[(\Qbf^{11})^{-1}\right]_{jk} & 1< j<k \leq p\\
    \left[(\Qbf^{11})^{-1}\right]_{jj} & k=1, 1< j \leq p  \\
    0 &  j=k .
    \end{cases}
\]
 
it holds that
 
$$
\partial_{w_{jk}} W = [\Mbf]_{jk}  \times W.
$$
\end{lemma}
\begin{lemma} [\cite{kirshner}] \label{lem:Kirshner}
    Let $p_W$ be a distribution on the space of spanning trees, such that $p_W(T)=\prod_{kl\in T} w_{kl} / W$, where $W$ is defined as in Theorem \ref{thm:MTT}. Taking the symmetric matrix $\Mbf$ as defined in Lemma  \ref{lem:Meila}, the probability for an edge $kl$ to be in the tree $T$ writes:
 
$$\mathds{P}\{kl\in T\} = \sum_{T\in \mathcal{T}} p_W(T)= w_{kl}\: \Mbf_{kl}$$
\end{lemma}

  \section{Incomplete data inference}
  here different type of data that are incomplete: counts, gaussian latent variables, trees (discrete )
  partie sur modèles à variables latentes ?

 \subsection{Expectation-Maximization algorithm}
 
 \subsection{Variational version}
  
\section{Statistical modeling for network inference}
  inférence réseaux vs inférence des paramètres (on n'observe pas le réseau)
 \subsection{Modeling count data}
PLN
 JSDM ?
\subsection{State of the art}
 