\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, mathtools}
\usepackage{amsfonts, dsfont}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{empheq}
\usepackage{bm}
\usepackage[round, sort]{natbib}
\usepackage{tikz}
\usepackage{mathtools}  
%\mathtoolsset{showonlyrefs}  
\usetikzlibrary{shapes,backgrounds,arrows,automata,snakes,shadows,positioning, mindmap}

%%%%% Commands
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newcommand{\argmin}{\arg\!\min}
\newcommand{\argmax}{\arg\!\max}
\newcommand*\widefbox[1]{\fbox{\hspace{3em}#1\hspace{3em}}}
\newcommand*\lesswidefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\newcommand{\entr}{\mathcal{H}}
\newcommand{\betabf}{\boldsymbol{\beta}}
\newcommand{\thetabf}{\boldsymbol{\theta}}
\newcommand{\mubf}{\boldsymbol{\mu}}
\newcommand{\Omegabf}{\boldsymbol{\Omega}}
\newcommand{\Sigmabf}{\boldsymbol{\Sigma}}
\newcommand{\zerobf}{\boldsymbol{0}}
\newcommand{\Xbf}{\boldsymbol{X}}
\newcommand{\xbf}{\boldsymbol{x}}
\newcommand{\Ybf}{\boldsymbol{Y}}
\newcommand{\Zbf}{\boldsymbol{Z}}
\newcommand{\Wbf}{\boldsymbol{W}}
\newcommand{\Qbf}{\boldsymbol{Q}}
\newcommand{\Mbf}{\boldsymbol{M}}
\newcommand{\Ubf}{\boldsymbol{U}}
\newcommand{\Sbf}{\boldsymbol{S}}
\newcommand{\mbf}{\boldsymbol{m}}
\newcommand\Ncal{\mathcal{N}}
\newcommand\Pcal{\mathcal{P}}
\newcommand\Tcal{\mathcal{T}} 
\newcommand{\Esp}{\mathds{E}}
\newcommand{\bound}{\mathcal{J}}
\newcommand{\had}{\boldsymbol{\cdot}}

 %%%%%%% TIKZ
\newcommand{\edgeunit}{1.5}
\newcommand{\nodesize}{1em}
\providecommand\given{} % is redefined in \Set
\newcommand\SetSymbol[1][]{\nonscript\:#1\vert\nonscript\:}
%\usepackage[mathcal]{eucal}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\tikzset{%
    observed/.style={%
    scale=0.9,rectangle,draw=white,transform shape,fill=white,font=\Large}
}
\tikzset{%
    basic/.style={%
    scale=0.9,circle,draw=black,transform shape,fill=white,font=\small}
}
\tikzstyle{basic}=[draw, circle, minimum width=0.4*\nodesize, inner sep=0, color=black, fill=black]
\tikzstyle{clique}=[draw, rectangle, minimum width=2*\nodesize, minimum height=2*\nodesize, inner sep=0, color=black]

\setlength\parindent{0pt}

\title{Reconstruction of a missing actor from count data using network inference}

\begin{document}

\maketitle
\vspace{3cm}
\tableofcontents
\newpage
\section{Model}

$$\left\{\begin{array}{rl}
T & \sim\prod_{jk \in T} \beta_{jk}/B \\\\
\Ubf |T& \sim\mathcal{N}(0,R_T)\\\\
\Ybf|\Ubf&\sim\mathcal{P}( \exp( \Xbf\theta^\intercal + \Ubf\sigma^\intercal) )
\end{array} \right.$$

\paragraph{Dimensions:}
The model is build on matrices $\Ybf$ and $\Zbf$ with the following dimensions:\\
$\Ybf$: $n\times p$\\
$\Ubf_O$: $n\times p$\\
$\Ubf_H$: $n\times r$


\paragraph{Underlying dependencies:} This diagram is the graphical model behind the model, it describes the variables direct denpendencies and how data is simulated. Here, a tree $T$ is first drawn, which controls the dependency structure of parameters $\Ubf = (\Ubf_O,\Ubf_H)$. Count data $\Ybf$ is drawn from a distribution depending only on observed parameters $\Ubf_O$.
\begin{center}
	\begin{tikzpicture}	
      \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw]
		\node[observed] (A1) at (0.625*\edgeunit, 2*\edgeunit) {$T$};
		\node[observed] (A2) at (0*\edgeunit, 1*\edgeunit) {$\Ubf_O$};
		\node[observed] (A3) at (1.25*\edgeunit, 1*\edgeunit) {$\Ubf_H$};
		\node[observed] (A4) at (0*\edgeunit, 0*\edgeunit) {$\Ybf$};
		\path (A1) edge [] (A2)
        (A1) edge [] (A3)
        (A2) edge [] (A3)
        (A2) edge [] (A4);
	\end{tikzpicture}
\end{center}

\subsection{Distributions}
\label{distrib}
 
The complete vector of underlying parameters $\Ubf$ follows a mixture of normalized Gaussian distributions on the space of all existing spanning trees.
\begin{align*}
(\Ubf_O,\Ubf_H) &\sim \sum_{T \in \mathcal{T}} p(T) \mathcal{N}(0,R_T) \\
\end{align*}

 
Conditionally on the dependency structure $T$, $\Ubf$ is a centered Gaussian variable, with covariance matrix $R_T$ defined by block. As latent vectors are normalized, $R_T$ is a correlation matrix, and we define $\Upsilon$ its inverse. We obtain the following distributions:
\begin{align*}
(\Ubf_O,\Ubf_H)|T & \sim\mathcal{N}(0,R_T)\\
\Ubf_O|T & \sim\mathcal{N}(0,\Upsilon_{Tm}^{-1})
\end{align*}
 

Where  $ \Upsilon_{Tm} $ is the Schur's complement of $R_{TO}$: $\Upsilon_{Tm}=R_{TO}^{-1} =  \Upsilon_{TO} - \Upsilon_{TOH}\Upsilon_{TH}^{-1}\Upsilon_{THO}$. 

\subsection{Assumptions}
\paragraph{Assumption A:} The missing latent variables are not connected with each other. The $H$ bloc of $\Upsilon_T$ should thus be diagonal.

\paragraph{Assumption B:} To reduce the number of parameters, we assume that precision matrices which are faithful to different trees  only differ in the position of their zero entries, meaning that any trees sharing  an edge will also share the same value in their precision matrix. Let's define $\Upsilon$ such as $$\forall T_a \in \{T \in\mathcal{T} / kl \in T \}, \Upsilon_{T_a}[k,l] =  \Upsilon [k,l].$$

 Let's also define $I_T$ the binary matrix with diagonal one indicating the  edges presence in tree $T$ : $I_T=(\mathds{1}_{kl \in T} + \mathds{1}_{k=l})_{kl}$. Then we can write :
$$\forall T\in \mathcal{T}, \;\;\; \Upsilon_T = I_T \had \Upsilon $$
where '$\had$' denotes the Hadamard product. We obtain the following for $\Upsilon_T$:
\[
 \Upsilon_T=
  \left( {\begin{array}{cc}
 (I_T \had \Upsilon)_O &  (I_T \had \Upsilon)_{OH}\\\\
 (I_T \had \Upsilon)_{HO} &   \Upsilon_H 
  \end{array} } \right) \]\\
 
 \section{Variational EM inference}
\subsection{Approximation of the likelihood}
\paragraph{Joint likelihood:}
The graphical model gives the following writing of the joint likelihood:
\begin{align*}
p(\Ybf,\Ubf,T)& = p(T) \: p(\Ubf|T) \: p(\Ybf|\Ubf) \\
&= p(T)\: p(\Ubf_O,\Ubf_H|T) \: p(\Ybf|\Ubf_O,\Ubf_H) \\
&= p(T) \: p(\Ubf_O|T) \: p(\Ubf_H | \Ubf_O,T)  \: p(\Ybf|\Ubf_O)
\end{align*} 


\paragraph{Variational approximation:}

Here $p(T,\Ubf \mid \Ybf)$ is untractable. We adopt a variational approximation aiming to maximise a lower bound of the log-likelihood of observed data $\log p(\Ybf)$, namely  
\begin{align*}
    \mathcal{J}(\Ybf; g,h)
    & = \log p(\Ybf) - KL\left[q(T,\Ubf) \mid\mid p(T,\Ubf \mid \Ybf)\right]
\end{align*}
Where KL is the Küllback-Leibler divergence. The approximation we adopt relies on factorizing $q$ in the product of two distributions $h$ and $g$ relating respectively to $\Ubf$  and $T$: 
$$p(T,\Ubf \mid\Ybf) \approx  q(\Ubf,T) = g(T)\:h(\Ubf).$$

 A consequence of this hypothesis is the contributions of   $\Esp_h[\log p(\Ubf\mid T)]$ and $\log p(T)$ --~only terms involving $T$ in the lower bound~-- writing as sums on the edges. As distribution   $g$ minimizes the KL divergence, it has to factorizes on the edges as well and hence:
$ g(T) = \left(\prod_{kl} \widetilde{\beta}_{kl} \right) / \widetilde{B}$. \\

Moreover, it follows from the independence of the samples that $h$ is a product law: $ h(\Ubf) = \prod_i h_i(\Ubf_i)$. Our variational approximation consists in assuming that each of its components is a multivariate Gaussian: $h(\Ubf_i) = \Ncal(\Ubf_i; \mbf_i, \Sbf_i)$, where $\Sbf_i$ is diagonal, meaning that $\Ubf_H$ and $\Ubf_O$ are independent under distribution $h$.  In the following we use the  $n\times (p+r)$ matrices $M$ and $S$ which compile respectively all vectors of means $\mbf_i$ and diagonals of variances $\Sbf_i$.


\paragraph{Lower bound:}
The lower bound of our model writes:
\begin{align*}
\mathcal{J}(\Ybf; g,h)&=\log p(\Ybf) - KL\left[g(T) h(\Ubf) \middle\vert\middle\vert\ p(T,\Ubf | \Ybf)\right]\\
&= \log p(\Ybf) - \Esp_{gh}[\log( g(T) h(\Ubf)) - \log p(\Ubf,T\mid \Ybf) ]\\
&= \log p(\Ybf) - \Esp_{gh}[\log g(T) + \log h(\Ubf) ] + \Esp_{gh}[\log p(\Ybf,\Ubf,T)] - \Esp_{gh}[\log p(\Ybf)]\\
&= \Esp_{gh} [\log p(\Ybf,\Ubf,T)] + \entr[g(T)] + \entr[h(\Zbf)]
\end{align*}

\begin{equation}
\label{firstJ}
 \boxed{\mathcal{J}(\Ybf; g,h) = \Esp_{gh} \Big[\log \big(p_\beta(T)\:p_{\Upsilon}(\Ubf\mid T)\:p_{\theta, \sigma}(\Ybf\mid \Ubf)\big)\Big] + \entr[g_{\widetilde{\beta}}(T)] + \entr[h_{M,S}(\Ubf)]}
\end{equation}
Thus the model's parameters are $\Phi = (\theta,\sigma,M,S,\widetilde{\beta}, \beta,  \Upsilon)$.

\subsection{PLNmodels: optimizing in $\theta$, $\sigma$, and $h(\Ubf_O)$:}
\citet{CMR18} compute the lower bound of a VEM for the original PLN model, that is where no missing actor is assume and the independence structure of the latent layer of parameters is not supposed to be faithful to a tree. We let $\mathcal{J}_{PLN}$ denote this lower bound, which writes:
$$\mathcal{J}_{PLN} = \Esp_h[\log p(\Ubf_O)] + \Esp_h[\log p(\Ybf|\Ubf_O)] + \entr[h(\Ubf_O)].$$ $\mathcal{J}_{PLN}$ is implemented  in the R package \texttt{PLNmodels}, and can be used in our computation as well.

\subsubsection{Rewriting  $\mathcal{J}(\Ybf; g,h)$}
Let's first separate observed and unobserved latent variables:
\begin{align*}
\mathcal{J}(\Ybf; g,h)&= \Esp_{gh}\Big[\log \big(p(T)\:  p(\Ubf_H| \Ubf_O,T)\: p(\Ubf_O|T)\:p(\Ybf|\Ubf_O)\big)\Big] + \entr[g(T)] +\entr[h(\Ubf_H,\Ubf_O)]
\end{align*}

 $\Ubf_O$ and $\Ubf_H$ being independent under the $h$ distribution, we have that $\entr[h(\Ubf_H,\Ubf_O)] =\entr[h(\Ubf_O)] +\entr[h(\Ubf_H)]$. Now using the hypothesis of independence of $g$ and $h$, we obtain quantities with separate dependencies:
\begin{align}
\mathcal{J}(\Ybf; g,h)&=  \Esp_{gh}[\log p(\Ubf_O | T)] +\Esp_h[\log p(\Ybf|\Ubf_O)]+\entr[h(\Ubf_O)]  \label{PLNlike}\\
& \;\; + \Esp_{gh}[\log p(\Ubf_H | \Ubf_O,T) ]+\Esp_g[\log p(T)] +\entr[g(T)]+\entr[h(\Ubf_H)] \label{new}
\end{align}

Which we write as below, where each term of the right hand-side corresponds to each line above:
$$\mathcal{J}(\Ybf; g,h)= \Esp_g[\mathcal{J}_{\Ubf_O}(\Ybf;h)] + \mathcal{J}_{\Ubf_H|\Ubf_O}(\Ybf;h,g)]$$
The idea is to take advantage of the maximization of  $\mathcal{J}_{PLN}$, which appears as follows in $\Esp_g[\mathcal{J}_{\Ubf_O}(\Ybf;h)] $:
\begin{align*}
\mathcal{J}_{\Ubf_O}(\Ybf;h) &= \Esp_h[\log p(\Ubf_O|T)] + \Esp_h[\log p(\Ybf|\Ubf_O)]+\entr[h(\Ubf_O)]\\
&= \mathcal{J}_{PLN} + \big(\Esp_h[\log p(\Ubf_O|T)-\log p(\Ubf_O) ]\big)\\
\Esp_g[\mathcal{J}_{\Ubf_O}(\Ybf;h)] &= \mathcal{J}_{PLN} + \Big(\Esp_h\big[\Esp_g[\log p(\Ubf_O|T)]-\log p(\Ubf_O) \big]\Big)
\end{align*}
Noticing that $\log p(\Ubf_O) = \log (\Esp_g[ \log p(\Ubf_O|T)])$, the last term above is negative according to Jensen's inequality.

 \subsubsection{Maximization of $\mathcal{J}(\Ybf; g,h)$}

$$\argmax_{\theta}\Big\{\Esp_g[\mathcal{J}_{\Ubf_O}(\Ybf;h)]\Big\} = \argmax_{\theta}\Big\{\mathcal{J}_{PLN}\Big\} $$
$$ max_{h(\Ubf_O)}\Big\{\Esp_g[\mathcal{J}_{\Ubf_O}(\Ybf;h)]\Big\} \leq  max_{h(\Ubf_O)}\Big\{\mathcal{J}_{PLN}\Big\} $$

\subsection{VE step: optimizing in $g$ and $h(\Zbf_H)$}
The VE step minimizes the Küllback-Leibler divergence between the approximated and the aimed distributions :

$$  \argmin_{g,h(\Ubf_H)} KL\left(g(T)h(\Ubf) \mid\mid p(\Ubf,T|\Ybf)\right)$$

\begin{align*}
KL\left(g(T)h(\Ubf) \mid\mid  p(\Ubf,T|\Ybf)\right) &= \Esp_{gh}\left[\log g(T)h(\Ubf) - \log p(\Ubf,T\mid \Ybf) \right]\\
&= \Esp_{gh}\left[\log g(T)h(\Ubf) - \log p(\Ybf,\Ubf,T) + \log p(\Ybf)  \right]\\
&= \Esp_{gh}\left[\log g(T)+ \log h(\Ubf) - \log (p(T)p(\Ubf\mid T) p(\Ybf\mid \Ubf_O)) + \log p(\Ybf)  \right]\\
&= -\Esp_{gh}[\log p(\Ubf \mid T) ] - \Esp_g[\log p(T) - \log g(T)] - \entr[h(\Ubf)]\\
& \;\;\;\; -\Esp_h[\log p(\Ybf \mid \Ubf_O)] +\log p(\Ybf) 
\end{align*}
Distributions $p(\Ybf)$ and $p(\Ybf\mid \Ubf_O)$  do not depend on distributions $g$ and $h(\Ubf_H)$. Moreover, as $\Ubf_O$ and $\Ubf_H$ are independant under the $h$ distribution, $\entr[h(\Ubf)] = \entr[h(\Ubf_O)]+ \entr[h(\Ubf_H)]$ and  $\entr[h(\Ubf_O)]$ is already optimized. We obtain:
 
\begin{empheq}[box=\widefbox]{align*}
\argmin_{g,h(\Ubf_H)} KL  &=\argmin_{g,h(\Ubf_H)} \Big\{ -\Esp_{gh}[\log p(\Ubf \mid T) ] - \Esp_g[\log p(T) - \log g(T)] -\entr[h(\Ubf_H)] \Big\}
\end{empheq}
 
 During the minimization, we will use the optimized distribution $h(\Ubf_O)^* = \prod_i^n \mathcal{N}(\widetilde{\mbf}_{Oi}, \widetilde{S}_{Oi})$, its parameters being gathered in $n\times p$ matrices $\widetilde{M}_O$ and $\widetilde{S}_O$.
 
 \subsubsection{Details of  $\Esp_{gh}[\log p(\Ubf \mid T) ]$} 
 $$\log p(\Ubf \mid T) = -\frac{n}{2} \log |R_T| - \frac{1}{2} Tr(\Upsilon_T \times \Ubf^\intercal \Ubf) $$
 Taking the expectation, and by independence of $g$ and $h$:
 $$\Esp_{gh} [\log p(\Ubf \mid T)] = \frac{n}{2} \Esp_{gh} [\log | \Upsilon_T|] - \frac{1}{2} Tr(\Esp_g[\Upsilon_T] \Esp_h[\boldsymbol{U}^\intercal \boldsymbol{U}])$$
 
 We let $SSD$ denote the sum of squared deviance. From the definition of the $h$ distribution: $$ \Esp_h[\Ubf^\intercal \Ubf] = M^\intercal M + S = SSD$$
  $SSD$ is a sufficient statistic of the distribution, and  as latent vectors have variance 1, $SSD/n$ has diagonal 1.
 
\paragraph{Expectation of the log-determinant of $\Upsilon_T$\\}
Thanks to $\Upsilon_T$ being tree-structured, its determinant factorizes on the edges of $T$ as follows:
\begin{align*}
|\Upsilon_T| &=\frac{\prod_k |R_{Tkk}|^{deg(k)-1}}{\prod_{kl \in T} |R_{Tkl}|} =  \Big(\prod_{kl \in T} (1-\rho_{Tkl}^2)\Big)^{-1},
 \end{align*}

 This gives $$ \Esp_g[\log|\Upsilon_T|]=-\frac{1}{2}\sum_{kl} P_{kl}\log(1-\rho_{Tkl}^2).$$
\paragraph{Trace of expectations\\}
Following assumption B, we have that:
  \[ \Esp_g[\Upsilon_T]_{kl}  =  \Esp_g[I_T\had\Upsilon]_{kl} \left\{ 
\begin{array}{cc}
(P \had \Upsilon)_{kl} & k \neq l\\
\upsilon_{kk} & k = l
\end{array}
\right.
\]
Therefore we obtain 
\begin{align*}
Tr(\Esp_g[\Upsilon_T] SSD) &= \sum_{kl} P_{kl} \upsilon_{kl} ssd_{kl} + \sum_k \upsilon_{kk} ssd_{kk}\\
&=n( \sum_{kl} P_{kl} \upsilon_{kl} \:ssd_{kl}/n + \sum_k \upsilon_{kk} )
\end{align*}

Finally, 
\begin{empheq}[box=\lesswidefbox]{align*}
\Esp_{gh} [\log p(\Ubf \mid T)] =- \frac{n}{2}\Big(\frac{1}{2}\sum _{\substack{kl\\ k \neq l}} P_{kl} \log (1-\rho_{Tkl}^2)+ \sum_{\substack{kl\\ k \neq l}} P_{kl} \upsilon_{kl} \frac{ssd_{kl}}{n} + \sum_{k} \upsilon_{kk} \Big)
 \end{empheq}
\subsubsection{Details of $\Esp_g[\log g(T) - \log p(T)]$}
\begin{align*}
\log g(T) - \log p(T) &= \left(  \sum_{j<k} \mathds{1}\{jk \in T\} \log \widetilde{\beta}_{jk} - \log \widetilde{B}\right) - \left(  \sum_{j<k} \mathds{1}\{jk \in T\} \log {\beta}_{jk} - \log {B}\right)\\
&=\frac{1}{2}\sum_{jk} \mathds{1}\{jk \in T\} \log \frac{\widetilde{\beta}_{jk}}{{\beta}_{jk}} - \log \frac{\widetilde{B}}{B}
\end{align*}
$$\boxed{
\Esp_g[\log g(T) - \log p(T)] = \frac{1}{2}\sum_{jk}P_{jk} \left(\log \frac{\widetilde{\beta}_{jk}}{{\beta}_{jk}}\right) - \log \frac{\widetilde{B}}{B} }$$

 \subsubsection{Details of $\entr[h(\Ubf_H)]$}
 
 The entropy of the Gaussian multivariate vector $\Ubf_H$ mainly depends on its variance matrix $S_H$. By definition of $S_H$ we have:
 \begin{align*}
 \log |S_H| &= \log(\prod_{k=p+1}^{p+r} \sum_{i=1}^n S_{ik})
\end{align*}
which gives the following entropy:
\begin{align*}
\entr[h(\Ubf_H)]&= \frac{1}{2} \log |S_H| + \frac{nr}{2}(1+\log(2\pi))\\
 &=\frac{1}{2}\left[ \sum_{k=p+1}^{p+r} \log\left(\sum_{i=1}^n S_{ik}\right)+ nr(1+\log 2\pi )\right]
\end{align*}

\subsubsection{Final quantity to optimize:}
\begin{align*}
\argmin_{g,h(\Ubf_H)} KL  &=\argmin_{g,h(\Ubf_H)}  \Big\{-\Esp_{gh}[\log p(\Ubf \mid T) ] + \Esp_g[\log g(T) - \log p(T)-\entr[h(\Ubf_H)]\Big\}\\
&= \argmin_{g,h(\Ubf_H)}  \bigg\{ \frac{n}{2}\Big(\frac{1}{2}\sum _{\substack{kl\\ k \neq l}} P_{kl} \log (1-\rho^2) +  \sum_{\substack{kl\\ k \neq l}} P_{kl} \upsilon_{kl} \frac{ssd_{kl}}{n} +  \sum_{k} \upsilon_{kk} \Big)\\
& \;\;\;\; + \frac{1}{2}\sum_{\substack{kl\\ k\neq l}}P_{kl} \left(\log \frac{\widetilde{\beta}_{kl}}{{\beta}_{kl}}\right) - \log \frac{\widetilde{B}}{B} -\frac{1}{2}\sum_{k\in H} \log\left(\sum_i \widetilde{S}_{ik}\right) \bigg\}
\end{align*}
 
 
 \subsubsection{Update formulas for $\widetilde{\beta}$ and $M_H$ }

\paragraph{Edges weights under the $g$ distribution \\}
Let's isolate the terms referring to the edges in the expression to be minimized:
\begin{align*}
  \sum_{\substack{kl\\ k \neq l}} \frac{P_{kl}}{2}\Big(\log \widetilde{\beta}_{kl} - \log \beta_{kl} + \frac{n}{2} \log (1-\rho_{Tkl}^2) + \upsilon_{kl}\: ssd_{kl} \Big)   
\end{align*}

 
Equating the term $kl$ of the above sum to zero, we obtain:
 $$\log \widetilde{\beta}_{kl} = \log \beta_{kl} - \frac{n}{2} \log  (1-\rho_{Tkl}^2) - \upsilon_{kl}\:ssd_{kl} $$
 
Which gives the following update formula for $\widetilde{\beta}_{kl}$:
 
  $$\boxed{\displaystyle \widetilde{\beta}_{kl} = \beta_{kl} \: (1-\rho_{Tkl}^2)^{-n/2} \exp( -\upsilon_{kl}\: ssd_{kl} ) }$$


\paragraph{Means of the hidden part of the latent Gaussian vector under the $h$ distribution \\}
Means $M_H$ only appear in the $H$, $OH$ and $HO$ blocs of $SSD$. The derivative of KL with respect to mean $M_{ik}$ for the $i^\text{th}$ sample of species $k\in H$ writes:

\begin{align*}
\frac{\partial KL}{\partial M_{ik}} &= \frac{\partial}{\partial M_{ik}}\Big(2\sum_{\substack{k \neq l\\
(k, l) \in H\times O}} P_{kl} \upsilon_{kl} (M^\intercal M)_{kl} + \sum_{k\in H} \upsilon_{kk} (M^\intercal M)_{kk}\Big)\\
&=   \sum_{l \in O } M_{il} P_{kl}\upsilon_{kl} + M_{ik} \upsilon_{kk}
\end{align*}
\begin{align*}
\frac{\partial KL}{\partial M_{ik}}  = 0\iff M_{ik} &= -\frac{1}{\upsilon_{kk}} \sum_{l\in O} M_{il} (P\had\Upsilon)_{lk}\\
&=-\frac{M_{iO} (P\had\Upsilon)[O,H]}{\upsilon_{kk}}
\end{align*}

As $\Upsilon_H$ is diagonal, this  matricially comes down to
$$\boxed{ M_H = -M_O(P\had\Upsilon)_{OH} \Upsilon_H^{-1}}$$


\paragraph{Variances of the hidden part of the latent Gaussian vector under the $h$ distribution \\}
$S-H$ appears in the entropy of $h(\Zbf_H)$ and in the diagonal of $\Sigma$. 
\begin{align*}
\frac{\partial KL}{\partial S_{ik}} &= \frac{1}{2}  \frac{\partial KL}{\partial S_{ik}}  \Big(\sum_k \upsilon_{kk}(\sum_j S_{j k}) - \sum_k \log (\sum_jS_{j k})\Big)\\
&=\upsilon_{kk} - \frac{1}{S_{ik}}
\end{align*}
We obtain for $k\in H$ and all $i$:
$$\boxed{S_{ik} = 1/\upsilon_{kk}}$$

\subsubsection{Computing edge probabilities under $g$}
During computations, we need the edge probability under the $g$ distribution. This can be computed as the sum of probabilities of trees containing this edge:

\begin{align*}
\mathds{P}_g(kl \in T)  &= \sum_{\substack{T  \in \mathcal{T} \\ kl \in T }} g(T) =\frac{1}{\widetilde{B}} \sum_{\substack{T  \in \mathcal{T} \\ kl \in T }} \prod_{uv \in T} \widetilde{\beta}_{uv}
\end{align*}
where $\displaystyle \widetilde{B}= \sum_{T \in \mathcal{T} }\prod_{uv \in T}  \widetilde{\beta}_{uv}$.\\

 This is again a sum-product form, but on a special set of trees. This can be efficiently computed using a formula from \citet{kirshner} (Theorem 3), stated in the appendix.
 
 
 \subsection{M step: optimizing in $(\beta, \Upsilon)$}
 The M step aims at maximizing the lower bound in the parameters referring to original distributions for $T$ and $\Ubf|T$, therefore $\beta$ and $\Upsilon$. From Eq.(\ref{firstJ}) we obtain that: 
$$ \argmax_{\beta, \Upsilon} \mathcal{J}(\Ybf ; g,h) =\argmax_{\beta, \Upsilon} \left\{ \Esp_{gh} [\log (p_\beta(T)p_{\Upsilon_T}(\Ubf\mid T) ]\right\} $$

As distributions of VE step mimic original ones, computations are very similar. We get:
\begin{align*}
\log (p_\beta(T)p_{\Upsilon_T}(\Ubf\mid T))  &= \frac{1}{2}\sum_{kl} \mathds{1}\{kl \in T\} \log \beta_{kl} - \log B + \frac{n}{2}\log |\Upsilon_T| - \frac{1}{2}Tr(\Upsilon_T \Ubf^\intercal \Ubf)\\
\Esp_{gh} [\log (p_\beta(T)p_{\Upsilon_T}(\Ubf\mid T) ] &= \frac{1}{2}\sum_{kl} P_{kl} \log\beta_{kl} +\frac{n}{2} \Esp_g[\log |\Upsilon_T|] -\frac{1}{2} Tr(\Esp_g [\Upsilon_T] \; SSD)- \log B
\end{align*}

Reminding that $\Esp_g[\log |\Upsilon_{T}|]= \frac{-1}{2}\sum _{kl} P_{kl}  \log \left(1-\rho_{Tkl}^2\right)$,  we obtain:
\begin{align*}
\Esp_{gh} [\log (p_\beta(T)p_{\Upsilon_T}(\Ubf\mid T) ] &=\frac{1}{2}\sum_{kl} P_{kl} \log  \beta_{kl} - \log B - \frac{n}{4} \sum_{kl} P_{kl} \log\left(1-\rho_{Tkl}^2\right)  - \frac{1}{2}Tr\big( (P \had \Upsilon)\: SSD\big) 
\end{align*}
 
 \subsubsection{Update formula for $\beta$}
 The derivative of $ \mathcal{J}(\Ybf ; g,h)$ with respect to weight $\beta_{kl}$ of edge $kl$ is:
 
 \begin{align*}
\frac{\partial}{\partial_{\beta_{kl}}} \mathcal{J}(\Ybf ; g,h) &=  \frac{\partial}{\partial_{\beta_{kl}}} \Esp_{gh} [\log (p_\beta(T)p_{\Upsilon_T}(\Ubf\mid T) ] \\
&= \frac{P_{kl}}{\beta_{kl}} - \frac{\partial_{\beta_{kl}} B }{B} \\
\end{align*}

The derivative of the normalization constant with respect to $\beta_{kl}$ is the derivative of an output from the Matrix Tree theorem with respect to one of the weight's matrix entry. This quantity is given by \citet{Meila} (Lemma 2). Defining matrix $Q(\betabf)$ as showed in the appendix, then $\partial_{\beta_{kl}} B = Q(\betabf)_{kl}$ and:
$$\frac{\partial}{\partial_{\beta_{kl}}} \mathcal{J}(\Ybf ; g,h) 
=0 \iff  \boxed{\widehat{\beta}_{kl} = \frac{P_{kl}}{ Q(\betabf)_{kl}} }$$

 \subsubsection{Update formula for $\Upsilon$ and $R$}
 \paragraph{Notations\\}
In what follows, for any $q\times q$  matrix $A$, $A_{kl}$ will refer to the bloc $kl$ of $A$: $A_{kl}=(a_{ij})_{\{i,j\}\in\{k,l\}}$.   $[A_{kl}]^q$ will then denote the matrix obtained by filling up with zero entries to obtain full dimension $q\times q$, so that:
$$([A_{kl}]^q )_{ij}=\left\{ \begin{array}{rl}
a_{ij} & \text{if } \{i,j\}\in\{k,l\}\\
0 &  \text{if } \{i,j\}\in\{1,..., q\}_{\setminus kl}
\end{array}\right.$$

 
\paragraph{Justification\\}
Let $\hat{\Upsilon}^*$ and $\hat{\Upsilon}_T^*$ be estimators of $\Upsilon$ and $\Upsilon_T$ respectively, such that:
$$\hat{\Upsilon}^*=\argmax_\Upsilon \Big\{\Esp_g[\Esp_h[\log p_\Upsilon(\Ubf|T)]]\Big\} \text{ , and\hspace{0.5cm}}\hat{\Upsilon}_T^*=\argmax_{\Upsilon_T} \Big\{\Esp_h[\log p_{\Upsilon_T}(\Ubf|T)]\Big\}.$$
Then by definition of $\hat{\Upsilon}^*$:
$$\Esp_g[\Esp_h[\log p_{\hat{\Upsilon}^*}(\Ubf|T)]] \geq \Esp_g[\Esp_h[\log p_{\hat{\Upsilon}_T^*}(\Ubf|T)]] $$
Therefore the use of $\hat{\Upsilon}_T^*$ is sub-optimal. However, for a any other estimator  $\hat{\Upsilon}_T$ of $\Upsilon_T$, we have by definition of $\hat{\Upsilon}_T^*$ that
$$\Esp_h[\log p_{\hat{\Upsilon}_T}(\Ubf|T)] \leq \Esp_h[\log p_{\hat{\Upsilon}_T^*}(\Ubf|T)] $$
Then by increase of the expectation we obtain
$$\Esp_g[\Esp_h[\log p_{\hat{\Upsilon}_T}(\Ubf|T)]] \leq \Esp_g[\Esp_h[\log p_{\hat{\Upsilon}_T^*}(\Ubf|T)]] $$
which guarantees the increase of the lower bound. Therefore, maximizing only inside the expectation in $g$ and using $\hat{\Upsilon}_T^*$ falls within the framework of GEMs.

\paragraph{Computation\\}
 We use the maximum likelihood estimates $\hat{\rho} $and $\hat{\Upsilon}_T$ established in \citet{Lau96}, which depends only on the $SSD$ matrix and the underlying dependency structure.
  The correlation correpsonding to an existing edge in the structure $T$ is estimated using
  $$ \forall kl\in T,\boxed{\hat{\rho}_{Tkl} = ssd_{kl}/n}.$$
  
   The estimator  $\hat{\Upsilon}_T$  writes as the following in our context:
\begin{align*}
\widehat{\Upsilon}_T &= n  \sum_{kl\in T}   [(SSD_{kl})^{-1}]^{p+r} - n\sum_k (deg(k)-1)[(SSD_{kk})^{-1}]^{p+r}\\
&=n \sum_{kl\in T}  [(SSD_{kl})^{-1} - (SSD_{kk})^{-1} -  (SSD_{ll})^{-1} ]^{p+r} + n\sum_k[(SSD_{kk})^{-1}]^{p+r}
\end{align*}
As SSD has diagonal n, the expression simplifies. Denoting $I_d$ the identity matrix of dimension $d$ we obtain:
$$\widehat{\Upsilon}_T =n\sum_{kl\in T} [(SSD_{kl})^{-1} -\frac{1}{n} I_2]^{p+r}+ I_{p+r}$$

We can detail each bloc matrices as follows:
\[
n\times [(SSD_{kl})^{-1} - \frac{1}{n}I_2] = \frac{1}{1-(ssd_{kl}/n)^2}
\left(\begin{array}{cc}
		(ssd_{kl}/n)^2   & -ssd_{kl}/n\\
		-ssd_{kl}/n& (ssd_{kl}/n)^2 
		\end{array}\right)
\]

To derive an estimate for $\Upsilon$, we consider the expectation in $g$:

$$\Esp_g[\widehat{\Upsilon}_T]= n\sum_{kl} P_{kl} \had [(SSD_{kl})^{-1} - I_2]^{p+r} + I_{p+r}$$
 The expectation above defines a matrix which off-diagonal term $kl$ and diagonal term $kk$ write as follows:
$$(\Esp_g[\widehat{\Upsilon}_T])_{kl} = P_{kl} \left(\frac{-ssd_{kl}/n}{1-(ssd_{kl}/n)^2} \right)\text{\hspace{0.5cm},\hspace{1cm}}(\Esp_g[\widehat{\Upsilon}_T])_{kk} = \sum_l P_{kl} \frac{(ssd_{kl}/n)^2}{1-(ssd_{kl}/n)^2} +1 $$
 
 We now define the estimator $\widehat{\Upsilon}$, such that the relationship $\Esp_g[\widehat{\Upsilon}_T]=P\had \widehat{\Upsilon}$ remains. The above yields:
 \[ \boxed{\widehat{\Upsilon}_{kl}=\left\{\begin{array}{cl}
  \dfrac{-ssd_{kl}/n}{1-(ssd_{kl}/n)^2} & ,k\neq l\\\\
 \sum_l P_{kl} \dfrac{(ssd_{kl}/n)^2}{1-(ssd_{kl}/n)^2} +1 & ,k=l
 \end{array}\right.
 }\]
All diagonal terms are positive. Out of curiosity we could rewrite term $kk$ to make the estimate for the degree of node $k$ appear. With $d\widehat{e}g(k)=\sum_l P_{kl}$ we also have:
$$\widehat{\Upsilon}_{kk} =\sum_l \frac{P_{kl} }{1-(ssd_{kl}/n)^2} -(d\widehat{e}g(k)-1).$$
 
%%%%%%%%%%%%%% 
\newpage
\bibliographystyle{apsrev} %tested plainnat
\bibliography{bibi}
 \appendix
 \section{Algebraic Tools}
 For any matrix $\Wbf$, we denote its entry in row $u$ and column $v$ by $[\Wbf]_{uv}$. We define the Laplacian matrix $\Qbf$ of a symmetric matrix $\Wbf=[w_{jk} ]_{1\leq j,k\leq p}$ as follows :
 
\[
 [\Qbf]_{jk}  =\begin{cases}
    -w_{jk}  & 1\leq j<k \leq p\\
    \sum_{u=1}^p w_{ju} & 1\leq j=k \leq p.
    \end{cases}
\]
 
We further denote $\Wbf^{uv}$ the matrix $\Wbf$ deprived from its $u$th row and $v$th column and we remind that the $(u, v)$-minor of $\Wbf$ is the determinant of this deprived matrix, that is $|\Wbf^{uv}|$.

\begin{theorem}[Matrix Tree Theorem  \cite{matrixtree,Meila}] \label{thm:MTT}
    For any symmetric weight matrix W, the sum over all spanning trees of the product of the weights of their edges is equal to any minor of its Laplacian. That is, for any $1 \leq u, v \leq p$,
 
   \[
    W := \sum_{T\in\mathcal{T}} \prod_{(j, k)\in T} w_{jk} = |\Qbf^{uv}|.
    \]
   
\end{theorem}    

In the following, without loss of generality, we will choose $\Qbf^{pp}$. As an extension of this result, \cite{Meila} provide a close form expression for the derivative of $W$ with respect to each entry of $\Wbf$. 

\begin{lemma} [\cite{Meila}] \label{lem:Meila}
    Define the entries of the symmetric matrix $\Mbf$ as
 
\[    
 [\Mbf]_{jk} =\begin{cases}
    \left[(\Qbf^{pp})^{-1}\right]_{jj} + \left[(\Qbf^{pp})^{-1}\right]_{kk} -2\left[(\Qbf^{pp})^{-1}\right]_{jk} & 1\leq j<k < p\\
    \left[(\Qbf^{pp})^{-1}\right]_{jj} & k=p, 1\leq j \leq p  \\
    0 & 1\leq j=k \leq p.
    \end{cases}
\]
 
it holds that
 
$$
\partial_{w_{jk}} W = [\Mbf]_{jk}  \times W.
$$
\end{lemma}
\begin{lemma} [\cite{kirshner}] \label{lem:Kirshner}
    Let $p_W$ be a distribution on the space of spanning trees, such that $p_W(T)=\prod_{kl\in T} w_{kl} / W$, where $W$ is defined as in Theorem \ref{thm:MTT}. Taking the symmetric matrix $\Mbf$ as defined in Lemma  \ref{lem:Meila}, the probability for an edge $kl$ to be in the tree $T$ writes:
 
$$\mathds{P}\{kl\in T\} = \sum_{T\in \mathcal{T}} p_W(T)= w_{kl}\: \Mbf_{kl}$$
\end{lemma}
 \section{Divers}
 
\subsection{Dérivation du log-déterminant}
On peut dériver par rapport à $\Omega$, en supposant la première ligne :
\begin{align}
    \partial_{\Omega}\left(\Esp_{gh}\left[\log p(\Zbf | T)\right] \right)&=  \Esp_{gh}\left[\partial_{\Omega}( \log p(\Zbf | T) )\right] \\
    &=\Esp_{gh}\left[\frac{n}{2}\partial_{\Omega}( \log |\Omega_T|) -\frac{1}{2} \partial_\Omega (Tr(\Omega_T \; \Zbf^\intercal \Zbf ))\right]
\end{align}

\paragraph{\underline{$\partial_{\Omega}( \log |\Omega_T|)$} :}
 $$\log |I_T \cdot \Omega| = \log \big(\sum_{k=1}^q (I_T\cdot \Omega)_{ik} \mathsf{Com}_{ik}\big)$$
Le terme $\omega_{ik}$ n'apparaît pas dans le cofacteur $\mathsf{Com}_{ik}$, ce qui donne :
\begin{align*}
    \partial_{\omega_{ij}} \log |I_T\cdot \Omega| &= (I_T)_{ij} \frac{\mathsf{Com}_{ij}}{|I_T\cdot \Omega |}
\end{align*}
Or on a $(I_T\cdot \Omega)^{-1} = \frac{1}{|I_T \cdot \Omega|}\times \mathsf{Com}^\intercal$, et en particulier par symétrie de $\Omega_T$ on a que $(I_T\cdot \Omega)_{ij}^{-1}=\frac{\mathsf{Com}_{ij}}{|I_T\cdot \Omega |}$.

Donc 
\begin{align}
         &\partial_{\omega_{ij}} \log |I_T\cdot \Omega| = (I_T)_{ij} (I_T\Omega)_{ij}^{-1}
\end{align}
Par symétrie :
\begin{align}
    &\partial_\Omega \log |I_T\cdot \Omega| = 2I_T \cdot \Omega_T^{-1}\\\\
    \iff &\Esp_g \big( \partial_\Omega \log |\Omega_T| \big) = 2\Esp_g (I_T\cdot \Sigma_T) 
\end{align}

\paragraph{\underline{$ \partial_\Omega (Tr(\Omega_T \; Q ))$}:}
\begin{align}
    \partial_\Omega \Esp_{gh}[Tr(\Omega_T \Zbf^\intercal \Zbf)]&=\partial_\Omega Tr(\Esp_g[\Omega_T]Q)\\
    &=\partial_\Omega Tr((P\cdot\Omega)Q)\\
    &=2P\cdot Q
\end{align}
D'où $$ \Esp_g (I_T\cdot \Sigma_T)  = \frac{PQ}{n}$$

Et également 
$$\boxed{n I_T\cdot \hat{\Sigma}_T = I_T\cdot \Esp_h[\Zbf^\intercal \Zbf] = I_T\cdot \widehat{SSD}}$$
\subsection{Calcul du déterminant}
Soit $R=(\rho_{ij})_{ij}$ une matrice de corrélations, et $R^{-1}= \Upsilon$. D'après Lauritzen (5.12):
$$\frac{\upsilon_{\mu\gamma}^2}{\upsilon_{\mu\mu}\upsilon_{\gamma\gamma}} = c_{\mu\gamma}^2 = 1-\frac{|R|\;|R_{\setminus \{\mu\gamma\}}|}{|R_{\setminus\gamma}|\;|R_{\setminus \mu}|} $$
Si R est fidèle à un graphe $\mathcal{G}$ décomposable, alors son déterminant dépend des déterminants des blocs de $R$ défini par les cliques et les séparateurs du graphe, tel que :
$$|R| = \frac{\prod_{C \in \mathcal{C}} |R_C|}{\prod_{S \in \mathcal{S}} |R_S|^{\nu(S)}} $$
où $\nu(S)$ est la multiplicité du séparateur $S$. En particulier si $\mathcal{G}$ est un arbre $T$, les cliques sont les arêtes et les séparateurs les noeuds. Comme $R$ est de diagonale 1, son déterminant s'obtient simplement :
\begin{align*}
    |R| &=  \prod_{kl\in T} (1-\rho_{kl}^2)\\
\end{align*}
Lorsqu'un noeud $\gamma$ est retiré de $T$, $R_{\setminus \gamma}$ devient fidèle à un graphe marginalisé sur $\gamma$, où apparaît une clique $C_\gamma$ formée par l'ensemble des anciens voisins de $\gamma$, et le reste du graphe reste inchangé. L'ensemble des cliques du graphe marginalisé est donc constitué de $C_\gamma$, et des arêtes dont aucune extrémité n'est $\gamma$ dans $T$.
\begin{figure}[h]
 \begin{center}
\begin{tabular}{lcl}
a) && b)\vspace{-0.5cm}\\
\hspace{0.5cm}
	\begin{tikzpicture}	
      \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw]
		\node[basic] (A1) at (0, 0) {};
		\node[basic] (A2) at (0.5*\edgeunit, 0) {};
		\node[] (gam) at (1*\edgeunit, 0) {$\gamma$};
		\node[] (mu) at (1.5*\edgeunit, 0) {$\mu$};
		\node[basic] (A3) at (0.75*\edgeunit, 0.5*\edgeunit ) {};
		\node[basic] (A4) at (1.25*\edgeunit, 0.5*\edgeunit) {};
		\node[basic] (A5) at (1.75*\edgeunit, 0.5*\edgeunit) {};
		\node[basic] (A6) at (1.75*\edgeunit, -0.5*\edgeunit) {};
		\node[basic] (A7) at (1*\edgeunit, -0.5*\edgeunit) {};
		\path (A1) edge [] (A2)
        (A2) edge [] (gam)
        (gam) edge [] (mu)
        (gam) edge [] (A3)
        (gam) edge [] (A4)
        (gam) edge [] (A7)
        (mu) edge [] (A5)
        (mu) edge [] (A6);
	\end{tikzpicture}
&\hspace{1.5cm} &
 \hspace{0.5cm}
	\begin{tikzpicture}	
      \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw]
		\node[basic] (A1) at (0, 0) {};
		\node[basic] (A2) at (0.5*\edgeunit, 0) {};
		\node[] (mu) at (1.5*\edgeunit, 0) {$\mu$};
		\node[basic] (A3) at (0.75*\edgeunit, 0.5*\edgeunit ) {};
		\node[basic] (A4) at (1.25*\edgeunit, 0.5*\edgeunit) {};
		\node[basic] (A5) at (1.75*\edgeunit, 0.5*\edgeunit) {};
		\node[basic] (A6) at (1.75*\edgeunit, -0.5*\edgeunit) {};
		\node[basic] (A7) at (1*\edgeunit, -0.5*\edgeunit) {};
		\node[] (eg) at (2.25*\edgeunit, 0*\edgeunit) {=};
		\path (A1) edge [] (A2)
        (A2) edge [] (mu)
        (A2) edge [] (A3)
        (A2) edge [] (A4)
        (A2) edge [] (A7)
        (A3) edge [] (A4)
        (A3) edge [] (mu)
        (A3) edge [] (A7)
        (A4) edge [] (mu)
        (A4) edge [] (A7)
        (mu) edge [] (A7)
        (mu) edge [] (A5)
        (mu) edge [] (A6);
		\node[basic] (A1b) at (2.75*\edgeunit, 0) {};
		\node[clique] (Cg) at (3.75*\edgeunit, 0) {$C_\gamma$};
		\node[basic] (A5b) at (4.5*\edgeunit, 0.5*\edgeunit) {};
		\node[basic] (A6b) at (4.5*\edgeunit, -0.5*\edgeunit) {};
		\path (A1b) edge [] (Cg)
        (Cg) edge [] (A5b)
        (Cg) edge [] (A6b);
	\end{tikzpicture}
\end{tabular}
 \caption{$a)$ Example of an original tree $T$. $b)$ Graph resulting from the marginalization of node $\gamma$ in $T$. Clique $C_\gamma$ appears, and is separated by nodes from the rest of the graph.}
 \end{center}
\end{figure}
Les séparateurs des cliques restent des noeuds, ce qui permet d'écrire :
$$|R_{\setminus \gamma} | = \prod_{\substack{kl \in T \\ \{k,l\} \neq \gamma }} (1- \rho_{kl}^2) \times |R_{C_\gamma}|$$

Si $T$ est marginalisé sur deux noeuds $\gamma$ et $\mu$ à la fois, une clique $C_{\gamma \mu}$ apparaît, formée par l'union des voisins des deux noeuds dans $T$. Ici $\gamma$ et $\mu$ sont liés dans $T$, sans quoi la corrélation partielle $c_{\gamma \mu}$ est nulle. Il est encore possible d'écrire :
$$| R_{\setminus \{\gamma,\mu\}}| = \prod_{\substack{kl \in T \\ \{k,l\} \notin \{\gamma,\mu\}}} (1-\rho_{kl}^2) \times |R_{C_{\gamma\mu}}|$$
Alors :
\begin{align*}
    \frac{|R|\;|R_{\setminus \{\mu\gamma\}}|}{|R_{\setminus\gamma}|\;|R_{\setminus \mu}|} &= \frac{ \prod_{kl\in T} (1-\rho_{kl}^2)\prod_{\substack{kl\in T\\ \{k,l\} \notin \{\mu, \gamma\}}} (1-\rho_{kl}^2)}{\prod_{\substack{kl\in T \\\{k,l\} \neq \gamma }} (1-\rho_{kl}^2)\prod_{\substack{kl\in T \\ \{k,l\} \neq \mu }} (1-\rho_{kl}^2)} \times \frac{|R_{C_{\gamma \mu}}|}{|R_{C_\gamma}||R_{C_\mu}|}\\
     &=(1-\rho_{\mu\gamma}^2)\times \frac{|R_{C_{\gamma \mu}}|}{|R_{C_\gamma}||R_{C_\mu}|}
\end{align*}
 On obtient:
 $$  c_{\mu\gamma}^2 = 1-(1-\rho_{\mu\gamma}^2)\times \frac{|R_{C_{\gamma \mu}}|}{|R_{C_\gamma}||R_{C_\mu}|} $$

\end{document}