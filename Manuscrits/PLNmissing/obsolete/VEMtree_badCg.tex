\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, mathtools}
\usepackage{amsfonts, dsfont}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{empheq}
\usepackage{bm}
\usepackage[round, sort]{natbib}
\usepackage{tikz}
\usetikzlibrary{shapes,backgrounds,arrows,automata,snakes,shadows,positioning, mindmap}
\newcommand{\argmax}{\arg\!\max}
\newcommand{\argmin}{\arg\!\min}
\newcommand*\widefbox[1]{\fbox{\hspace{3em}#1\hspace{3em}}}

\newcommand*\lesswidefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\newcommand{\edgeunit}{1.5}
\providecommand\given{} % is redefined in \Set
\newcommand\SetSymbol[1][]{\nonscript\:#1\vert\nonscript\:}
%\usepackage[mathcal]{eucal}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\tikzset{%
    observed/.style={%
    scale=0.9,rectangle,draw=white,transform shape,fill=white,font=\Large}
}
\tikzset{%
    basic/.style={%
    scale=0.9,circle,draw=black,transform shape,fill=white,font=\small}
}


\setlength\parindent{0pt}
\author{Raphaelle Momal}
\title{PLN with missing actor}


\newcommand{\Esp}{\mathds{E}}
\newcommand{\entr}{\mathcal{H}}
\begin{document}

\maketitle
\vspace{3cm}
\tableofcontents
\newpage
Previously:
$$ p(Y,Z,T) = p(T)p(Z|T)p(Y|Z)$$
$$ \Esp(\log p(Y,Z,T)|Y) \approx \sum_{1 \leq j < k \leq p} P_{jk} \log\left(\beta_{jk} \hat{\psi}_{jk}\right) - \log B + cst$$

Now adding a hidden layer of unobserved data, indexed by H:

$$ p(Y,Z_O,Z_H,T)$$
\section{PLNmissing peculiarities:}

\subsection{Model}

$$\left\{\begin{array}{rl}
T & \sim\prod_{jk \in T} \beta_{jk}/B \\\\
Z|T& \sim\mathcal{N}(0,\Sigma_T)\\\\
Y|Z&\sim\mathcal{P}( \exp( Z+...) )
\end{array} \right.$$

\paragraph{Dimensions:}
The model is build on matrices $Y$ and $Z$ with the following dimensions:\\
$Y$: $n\times p$\\
$Z_O$: $n\times p$\\
$Z_H$: $n\times r$


\paragraph{Underlying dependencies:} This diagram is the graphical model behind the model, it describes the variables direct denpendencies and how data is simulated. Here, a tree $T$ is first drawn, which controls the dependency structure of parameters $Z = (Z_O,Z_H)$. Count data $Y$ is drawn from a distribution depending only on observed parameters $Z_O$.
\begin{center}
	\begin{tikzpicture}	
      \tikzstyle{every edge}=[-,>=stealth',shorten >=1pt,auto,thin,draw]
		\node[observed] (A1) at (0.625*\edgeunit, 2*\edgeunit) {$T$};
		\node[observed] (A2) at (0*\edgeunit, 1*\edgeunit) {$Z_O$};
		\node[observed] (A3) at (1.25*\edgeunit, 1*\edgeunit) {$Z_H$};
		\node[observed] (A4) at (0*\edgeunit, 0*\edgeunit) {$Y$};
		\path (A1) edge [] (A2)
        (A1) edge [] (A3)
        (A2) edge [] (A3)
        (A2) edge [] (A4);
	\end{tikzpicture}
\end{center}

\subsection{Distributions}
\paragraph{Marginals}
The complete vector of underlying parameters $Z$ follows a mixture of Gaussian laws on the space of all existing spanning trees.
\begin{align*}
(Z_O,Z_H) &\sim \sum_{T \in \mathcal{T}} p(T) \mathcal{N}(0,\Sigma_T) \\
\end{align*}

\paragraph{Conditional on T:}
Conditionally on the dependency structure $T$, $Z$ is a centered Gaussian variable, with covariance matrix $\Sigma_T$ defined by block:
\[
 \Sigma=
  \left( {\begin{array}{cc}
  \Sigma_{TO} &  \Sigma_{TOH}\\\\
  \Sigma_{THO} & \Sigma_{TH}
  \end{array} } \right) =
   \left( {\begin{array}{cc}
  \Omega_{TO} &  \Omega_{TOH}\\\\
  \Omega_{THO} & \Omega_{TH}
  \end{array} } \right)^{-1}
  \]\\
This results in the following distributions:
\begin{align*}
(Z_O,Z_H)|T & \sim\mathcal{N}(0,\Sigma_T)\\
Z_O|T & \sim\mathcal{N}(0,\Omega_{Tm}^{-1})\\
Z_H|T & \sim\mathcal{N}(0,\Omega_{Tm_H}^{-1})
\end{align*}
With $ \Omega_{Tm} =\Sigma_{TO}^{-1} =  \Omega_{TO} - \Omega_{TOH}\Omega_{TH}^{-1}\Omega_{THO}$, and $\Omega_{Tm_H} = \Sigma_{TH}^{-1} = \Omega_{TH} - \Omega_{THO} \Omega_{TO}^{-1} \Omega_{TOH}$. This comes from the Schur's complement which is recalled here:


\begin{align*}
 \Sigma_T=
  \left( {\begin{array}{cc}
  \Omega_{TO} &  \Omega_{TOH}\\\\
  \Omega_{THO} & \Omega_{TH}
  \end{array} } \right)^{-1} &=
  \left( {\begin{array}{cc}
  \Sigma_{TO} =( \Omega_{TO} - \Omega_{THO}\Omega_{TH}^{-1}\Omega_{TOH})^{-1} &  - \Sigma_{TO} \Omega_{TOH}\Omega_{TH}^{-1}\\\\
 -\Omega_{TH}^{-1}\Omega_{THO}\Sigma_{TO} & \Omega_{TH}^{-1}+\Omega_{TH}^{-1}\Omega_{TOH}\Sigma_{TO}\Omega_{THO}\Omega_{TH}^{-1}
  \end{array} } \right)\\\\
  &=   \left( {\begin{array}{cc}
   \Omega_{TO}^{-1}+\Omega_{TO}^{-1}\Omega_{TOH}\Sigma_{TH}\Omega_{THO}\Omega_{TO}^{-1} & -\Omega_{TO}^{-1}\Omega_{TOH}\Sigma_{TH} \\\\
    -\Sigma_{TH}\Omega_{THO}\Omega_{TO}^{-1}&  \Sigma_{TH}= (\Omega_{TH} - \Omega_{THO}\Omega_{TO}^{-1}\Omega_{TOH})^{-1} 
   \end{array} } \right)
\end{align*}
  


\paragraph{Conditional on observed data:\\}


As $Z_O$ decouples $Z_H$ and $Y$, we will never need $Z_H|Y$:
$$ p(Z|Y) = p(Z_O,Z_H | Y) = p(Z_H|Z_O) \: p(Z_O|Y) $$



Model for the hidden part: $$Z_H|Z_O,T \sim \mathcal{N}(\mu_{H|O,T}, \Omega_{H}^{-1})$$ 

\begin{itemize}
\item In a precision matrix of a multivariate Gaussian, the conditional bloc is the marginal bloc:  $\Omega_{TH|O}^{-1} = \Sigma_{TH} -\Sigma_{THO}\Sigma_{TO}^{-1}\Sigma_{TOH} = \Omega_{TH}^{-1}$. Moreover the H block of $\Omega_T$ is diagonal as imposed by identifiability conditions (two hidden nodes cannot be linked with each other), and diagonal terms are assumed to not depend on tree $T$. Hence $\Omega_{TH|O}^{-1}=\Omega_{H}^{-1}$.

\item let's $Z_{Oi}$ denote a realization of $\mathcal{N}(0,\Sigma_{TO}) $.\\
 Then $\displaystyle  \mu_{H|Oi,T} = \Sigma_{THO}\Sigma_{TO}^{-1}(Z_{Oi}-\underbrace{\mu_{Z_Oi|T}}_{0}) 
 = -\Omega_{TH}^{-1}\Omega_{THO} \: Z_{Oi}
$\\
\end{itemize}


\paragraph{Joint likelihood:}
\begin{align*}
p(Y,Z,T)& = p(T) \: p(Z|T) \: p(Y|Z) \\
&= p(T)\: p(Z_O,Z_H|T) \: p(Y|Z_O,Z_H) \\
&= p(T) \: p(Z_O|T) \: p(Z_H | Z_O,T)  \: p(Y|Z_O)
\end{align*} 
\section{Variational EM inference}

This variational EM approximates the conditional distribution $p(T,Z | Y)$ by a product  between the  distributions of tree $T$ and latent variables $Z$ (mean-field approximation).

$$p(T,Z | Y) \approx  g(T)h(Z)$$
Variational approximation:
\begin{itemize}
\item $ h(Z_i) =  \mathcal{N}(m_i,S_i)$ , where $S_i$ is diagonal with diagonal $(s_{i1}^2, ... , s_{i(p+r)}^2)$, meaning that $Z_H$ and $Z_O$ are independant. Marginals are written $h_O$  and  $h_H$. $S=\sum_i S_i$  and $ M =  [m_i]_{1 \leq i \leq (p+r)}$ (so $\Esp_h[Z_{\cdot k}^\intercal Z_{\cdot k}] = \sum_i s_{ik} + m_{\cdot k}^\intercal m_{\cdot k}$). $S$ is also diagonal and its H and O blocs are denoted $S_H$ and $S_O$.
\item $ h(Z) = \prod_i h(Z_i)$ samples are independent 
\item $ g(T) = \left(\prod_{kl} \widetilde{\beta}_{kl} \right) / \widetilde{B}$
\end{itemize}

In the following we use the scalar product on the matrix space defined with the trace operator as: 
$$  Tr(A^\intercal B) = <A,B> .$$
\subsection{Lower bound:}
Starting with the two hidden layers of our model, the lower bound writes:
\begin{align*}
\mathcal{J}(Y; g,h)&=\log p(Y) - KL\left[g(T) h(Z) \middle\vert\middle\vert\ p(T,Z | Y)\right]\\
&= \log p(Y) - \Esp_{gh}[\log( g(T) h(Z)) - \log p(Z,T\mid Y) ]\\
&= \log p(Y) - \Esp_{gh}[\log g(T) + \log h(Z) ] + \Esp_{gh}[\log p(Y,Z,T)] - \Esp_{gh}[\log p(Y)]\\
&= \Esp_{gh} [\log p(Y,Z,T)] + \entr[g(T)] + \entr[h(Z)]\\
&= \Esp_{gh} [\log (p_\beta(T)p_{\Omega_T}(Z\mid T)p_\theta(Y\mid Z))] + \entr[g(T)] + \entr[h(Z)]
\end{align*}

Adding modelization of unobserved covariates:
\begin{align*}
\mathcal{J}(Y; g,h)&= \Esp_{gh}[\log p(T)  p(Z_H| Z_O,T) p(Z_O|T)p(Y|Z_O)] + \entr[g(T)] +\entr[h(Z_H,Z_O)]
\end{align*}

 We develop the entropy of the complete $Z$ vector, to make the entropy of the fully observed part appear:
\begin{align*}
\entr[h(Z_H,Z_O)] &= -\Esp_h[\log h(Z_H,Z_O)]\\
&=-\Esp_h[\log h(Z_H\mid Z_O) h(Z_O)]\\
&=\entr[h(Z_O)] -\Esp_h[\log h(Z_H\mid Z_O)]
\end{align*}

Now using the hypothesis of independence of $g$ and $h$, we can obtain quantities with seperate dependencies:

\begin{align*}
\mathcal{J}(Y; g,h)&=   \Esp_{gh}[\log p(Z_H | Z_O,T)  p(Z_O | T)] +\Esp_g[\log p(T)] + \Esp_h[\log p(Y|Z_O)]-\Esp_h[\log h(Z_H\mid Z_O)]&\\
& \;\; + \entr[g(T)] +\entr[h(Z_O)] 
\end{align*}




%Property of Shannon entropy : $\entr(X,Y) = \entr(X) + \entr(Y|X)$

Finally we obtain:

\begin{align}
\mathcal{J}(Y; g,h)&=  \Esp_{gh}[\log p(Z_O | T)] +\Esp_h[\log p(Y|Z_O)]+\entr[h(Z_O)]& \text{: PLN like} \nonumber\\
& \;\; + \Esp_{gh}[\log p(Z_H | Z_O,T) ]+\Esp_g[\log p(T)] +\entr[g(T)]-\Esp_h[\log h(Z_H\mid Z_O)]&\text{: New}\label{lowerbound}
\end{align}\\

\subsection{Optimizing in $\theta$ and $h(Z_O)$:}
\citet{CMR18} compute the lower bound of a VEM for the PLN, which we write $\mathcal{J}_{PLN}$. This lower bound appears in our computation as well. Let's first detail the following quantity:
\begin{align*}
\Esp_{gh}[\log p(Z_O|T)] &=  \Esp_{gh} \left[\frac{n}{2} \log |\Omega_{Tm}| - \frac{1}{2} \sum_i Z_{Oi}\Omega_{Tm} Z_{Oi}^\intercal  \right]\\
&= \frac{n}{2} \Esp_g [\log |\Omega_{Tm}|] - \frac{1}{2} \Esp_{gh}\left[Tr\left( Z_O^\intercal Z_O \Omega_{Tm}\right)\right].
\end{align*}

The expectation in $gh$ is separable thanks to the independance hypothesis:
\begin{align*}
\Esp_{gh}[\log p(Z_O|T)] &=\frac{n}{2} \Esp_g [\log |\Omega_{Tm}|] - \frac{1}{2}  Tr\left(\Esp_h [Z_O^\intercal Z_O ]\; \Esp_g[\Omega_{Tm}]\right)\\
&=\frac{n}{2} \Esp_g [\log |\Omega_{Tm}|] - \frac{1}{2}<\Esp_h [Z_OZ_O^\intercal ], \underbrace{\Esp_g [\Omega_{Tm}]}_{\overline{\Omega_{Tm}}}> .
\end{align*}
We note $\Esp_g [\Omega_{Tm}] = \overline{\Omega_{Tm}}$, which is a $p\times p$ matrix of expected covariance over the whole spanning-tree space. Introducing the quantity  $\log |\overline{\Omega_{Tm}}| $ in the PLN-like terms of Eq.~(\ref{lowerbound}), the lower bound of the VEM of PLN $\mathcal{J}_{PLN}$ appears:
\begin{align*}
&\Esp_{gh}[\log p(Z_O | T)] +\Esp_h[\log p(Y|Z_O)]+ \entr[h(Z_O)]\\
& =\underbrace{\frac{n}{2} \log |\overline{\Omega_{Tm}}| - \frac{1}{2}<\Esp_h [Z_OZ_O^\intercal ], \overline{\Omega_{Tm}}> + \Esp_{h}[\log p(Y|Z_O)] + \entr[h(Z_O)]}_{\mathcal{J}_{PLN}\text{ with } \widetilde{\Sigma} = \overline{\Omega_{Tm}}} + \underbrace{\frac{n}{2}\left( \Esp_g[\log|\Omega_{Tm}|] - \log|\overline{\Omega_{Tm}}|\right)}_{\leq 0}
\end{align*}


The second brace is a negative quantity, because the log determinent is a concave function. Jensen's inequality gives:
$$\log |\Esp_g[\Omega_{Tm}]| \geq \Esp_g [\log |\Omega_{Tm}|]$$
%$$ \Esp[Z_jZ_k] = -\partial_{\omega_{jk}}( \log |\Omega_{TO}| )$$
Therefore, $$\Esp_{gh}[\log p(Z_O | T)] +\Esp_h[\log p(Y|Z_O)]+ \entr[h(Z_O)] \leq \mathcal{J}_{PLN}$$\\
\begin{align*}
\mathcal{J}_{h}(Y; g,h)&= \mathcal{J}_{PLN}+\frac{n}{2}\left(\Esp_g[\log|\Omega_{Tm}|] - \log|\overline{\Omega_{Tm}}|\right)\\
&\;+ \Esp_{gh}[\log p(Z_H | Z_O,T) ]+\Esp_g[\log p(T)] +\entr[g(T)]-\Esp_h[\log h(Z_H\mid Z_O)]
\end{align*}
The "VE" step of PLNmodels algorithm maximizes $\mathcal{J}_{PLN}$ and gives estimates of $Z_O$ moments of order 1 and 2. So we have easy access to $\widetilde{m}_O, \widetilde{S}_O, \widetilde{\overline{\Omega_{Tm}}}$, which maximize part of $\mathcal{J}(Y;g,h)$ in $h(Z_O)$.  \\

Remarks:
\begin{itemize}
\item Here it shows that assuming a tree structure for the latent layer of parameters $Z$ reveals an average precision matrix over all the precision matrices of possible trees. Recall that these matrices only differ in the position of their zero entries.
\item Not all terms depending on $h(Z_O)$ are in $\mathcal{J}_{PLN}$, so the proposed solution is sub-optimal. After maximizing $\mathcal{J}_{PLN}$ in $h_O$, this density is considered constant and written $h_O^*$.  
\end{itemize}




\subsection{VE step: optimizing in $g$ and $h(Z_H)$}
The VE step minimizes the Küllback-Leibler divergence between the approximated and the aimed distributions :

$$  \argmin_{g,h(Z_H)} KL\left(g(T)h(Z) || p(Z,T|Y)\right)$$


\begin{align*}
KL\left(g(T)h(Z) || p(Z,T|Y)\right) &= \Esp_{gh}\left[\log g(T)h(Z) - (\log p(Y,Z,T) - \log p(Y) ) \right]\\
&=\Esp_{gh}\left[\log g(T) + \log h(Z_O)+ \log h(Z_H|Z_O) - \log p(T)p(Z_H|Z_O,T)p(Z_O|T)p(Y|Z_O)\right. \\
& \;\;\;+ \log p(Y) )\left. \right]
\end{align*}

Under distribution $h$, $Z_H$ and $Z_O$ are independent. Using the partly optimal distribution $h_O^*$ for $h(Z_o)$, we obtain:
\begin{align*}
\argmin_{g,h(Z_H)} KL\left(g(T)h(Z) || p(Z,T|Y)\right) &=\argmin_{g,h(Z_H)} \{ \Esp_g[\log g(T)] - \Esp_g[\log p(T)] + \Esp_h[\log h(Z_H)]  \\
& \text{\hspace{2cm}}   - \Esp_{gh_O^*}[\log p(Z_O|T)] - \Esp_{gh}[ p(Z_H|Z_O,T)]  \}\\
\end{align*}

\subsubsection{Compute $\Esp_{gh}[\log p(Z_H | Z_O,T) ]$}
Recalling $\mu_{H|Oi,T} = -\Omega_H^{-1}\Omega_{THO}Z_{Oi} ^\intercal $ and that $\Omega_H$ is constrained to be diagonal and indenpendent of $T$ for identifiability, we have:
\begin{align*}
\log p(Z_H\mid Z_O,T) &= \frac{n}{2} \log |\Omega_H| -\frac{1}{2} \sum_i (Z_{Hi}-\mu_{H|Oi,T}^\intercal )\Omega_H(Z_{Hi}-\mu_{H|Oi,T}^\intercal )^\intercal \\
&= \frac{n}{2} \log |\Omega_H| -\frac{1}{2} \sum_i \left(Z_{Hi}\Omega_HZ_{Hi}^\intercal  + \mu_{H|Oi,T}^\intercal \Omega_H\mu_{H|Oi,T} - 2\mu_{H|Oi,T}^\intercal \Omega_HZ_{Hi} \right)\\
&= \frac{n}{2} \log |\Omega_H| -\frac{1}{2}Tr(Z_H\Omega_HZ_H^\intercal ) + \frac{1}{2} Tr(Z_O\Omega_{THO}^\intercal (\Omega_H^{-1})^\intercal \Omega_{THO}Z_O^\intercal ) - Tr(Z_O\Omega_{TOH}Z_H^\intercal)\\
&=\frac{n}{2} \log |\Omega_H| -\frac{1}{2}<\Omega_H,Z_H^\intercal Z_H> + \frac{1}{2} <\Omega_{THO}^\intercal (\Omega_H^{-1})^\intercal \Omega_{THO},Z_O^\intercal Z_O>- <\Omega_{TOH},Z_H^\intercal Z_O>
\end{align*}


Now taking the expectation, and by independence of $g$ and $h$:
\begin{align*}
\Esp_{gh}[\log p(Z_H | Z_O,T) ] &=\frac{n}{2} \log |\Omega_H| -\frac{1}{2} <\Omega_H,\Esp_h[Z_H^\intercal Z_H]> +\frac{1}{2} <\Esp_g[\Omega_{THO}^\intercal \Omega_H^{-1}\Omega_{THO}],\Esp_{h_O^*}[Z_O^\intercal Z_O]> \\
&\;\;\; -<\Esp_g[\Omega_{TOH}],\Esp_h[Z_H^\intercal Z_O]>\\
&=\frac{n}{2} \log |\Omega_H|-\frac{1}{2} <\Omega_H,M_H^\intercal M_H + S_H> +\frac{1}{2} <\Esp_g[\Omega_{THO}^\intercal \Omega_H^{-1}\Omega_{THO}],\widetilde{M}_O^\intercal \widetilde{M}_O + \widetilde{S}_O>\\
&\;\;\; -<\Esp_g[\Omega_{TOH}],M_H^\intercal \widetilde{M}_O +  \underbrace{S_{HO}}_{=0}>
\end{align*}\\



We note $\mathcal{C}_g$ the following $p \times p$ expectation matrix:
\begin{align*}
\mathcal{C}_g=\Esp_g[\Omega_{THO}^\intercal \Omega_H^{-1}\Omega_{THO}] &= \sum_{h=1}^r \frac{1}{\omega_{hh}} \Esp_g[\Omega_{TOh} \Omega_{ThO}]\\
&= \sum_{h=1}^r \frac{1}{\omega_{hh}} \left[ \omega_{hk}\omega_{hl}\Esp_g[\mathds{1}\{(hk,hl)\in T\}]\right]_{1\leq k,l\leq p}\\
&=\sum_{h=1}^r \frac{1}{\omega_{hh}}[ \omega_{hk}\omega_{hl}\underbrace{\mathds{P}_g((hk,hl)\in T)}_{P_{gkl}\:^h}]_{1\leq k,l\leq p}
\end{align*}\\
Therefore $\mathcal{C}_g$ term $kl$ is $\mathcal{C}_g[k,l] = \sum_h \frac{\omega_{hk}\omega_{hl}}{\omega_{hh}} P_{gkl}\:^h$.


Compute the probability for two observed covariates to be linked to a hidden one:
\begin{align*}
P_{gkl}\:^h&=\mathds{P}_g((hk,hl)\in T)\\
&=1- \mathds{P}(\{hk \notin T\} \cup \{hl \notin T\})\\
&= 1 - \left[ \mathds{P}_g(hk \notin T)+ \mathds{P}_g(hl \notin T) - \mathds{P}_g( \{hk \notin T\}, \{hl \notin T\}) \right]\\
&=\mathds{P}(hk \in T) + \mathds{P}(hl \in T) + \mathds{P}(\{hk \notin T\} \cap \{hl \notin T\})-1
\end{align*}

Using the Matrix Tree theorem, we can impose two edges to not be in the tree (as two zeros in the weight matrix) and therefore compute $ \mathds{P}(\{hk \notin T\} \cap \{hl \notin T\})$.\\
%Using the Matrix Tree theorem and denoting $\bm{W} = [\widetilde{\beta}_{uv}]_{uv}$:
%\begin{align*}
%\mathds{P}_g((hk,hl)\in T)^{t+1}&= 1-\frac{1}{\widetilde{B}} \left(  |Q_{uv}^*(\bm{W}_{\setminus hk}^t)|+|Q_{uv}^*(\bm{W}_{\setminus hl}^t)|+|Q_{uv}^*(\bm{W}_{\setminus (hk,hl)}^t)| \right) 
%\end{align*}

$\mathcal{C}_g$ is computed with the current $\bm{W}^t$, and is not optimized in $\widetilde{\beta}_{jk}\:^t$.
\paragraph{Derivatives\\}
The only term where $S_H$ appears is the following:
\begin{align*}
\frac{1}{2}<\Omega_H, S_H> &= \frac{1}{2} \sum_h \omega_{hh}\sum_i s_{ih}^2\\
\end{align*}
Let's denote $P_g = [\mathds{P}_g(kl \in T)]_{kl}$ the edge probability matrix under the $g$ distribution. Then:
$$\Esp_g[\Omega_{TOH}] = P_{gOH} \odot \Omega_{OH}$$

Therefore:
\begin{empheq}[box=\widefbox]{align*}
\frac{\partial (-\Esp_{gh}[\log p(Z_H | Z_O,T) ])}{\partial M_H}&=  M_H\Omega_H  - \widetilde{M}_O P_{gOH} \odot \Omega_{OH}\\
\frac{\partial (-\Esp_{gh}[\log p(Z_H | Z_O,T) ]) }{\partial s_{ih}^2}&= \frac{\omega_{hh}}{2}
\end{empheq}

\subsubsection{Compute $\Esp_g[\log p(T)] + \entr[g(T)]$}

\begin{align*}
\Esp_g[\log p(T)] + \entr[g(T)]&=\Esp_g[\log p(T)] -\Esp_g[\log g(T)]\\
&=\Esp_g\left[ \sum_{jk} \mathds{1}\{jk \in T\} \log \beta_{jk} - \log B\right] - \Esp_g\left[\sum_{jk} \mathds{1}\{jk \in T\} \log \widetilde{\beta}_{jk} - \log \widetilde{B}\right]\\
&= \sum_{jk}P_{gjk} \left(\log \frac{\beta_{jk}}{\widetilde{\beta}_{jk}}\right) - \log \frac{B}{\widetilde{B}} 
.\end{align*}

To ensure the identifiability of the weights $\beta_{jk}$, we optimize under the constraint $\sum_{jk} \widetilde{\beta}_{jk} =1 $. Adding the corresponding $\lambda$ penalty, we finally get 
$$\Esp_g[\log p(T)] + \entr[g(T)]=\sum_{jk}P_{gjk} \left(\log \frac{\beta_{jk}}{\widetilde{\beta}_{jk}}\right) - \log \frac{B}{\widetilde{B}}  + \lambda(\sum_{jk} \widetilde{\beta}_{jk} -1 )$$
This is also the (opposite of the) Kullback–Leibler divergence of a tree drawn under $p$ from a tree draw under $g$. This will be used in network comparison.

\paragraph{Derivatives\\}
\begin{align*}
\partial_{\widetilde{\beta}_{jk}} \left(\Esp_g[\log p(T)] + \entr[g(T)]\right) &= \frac{-P_{gjk}}{\widetilde{\beta}_{jk}} + \frac{1}{\widetilde{B}} \times \partial_{\widetilde{\beta}_jk} (\widetilde{B})
\end{align*}

Applying Lemma from Meila, the derivative of $\widetilde{B}$ with respect to $\widetilde{\beta}_{jk}$ is :
$$\partial_{\widetilde{\beta}_{jk}} (\widetilde{B}) = [\bm{M}(\bm{W})]_{jk} \times \widetilde{B} $$

Therefore, we obtain the following gradient: 
\begin{align*}
\partial_{\widetilde{\beta}_{jk}} \left(\Esp_g[\log p(T)] + \entr[g(T)]\right) &= \frac{-P_{gjk}}{\widetilde{\beta}_{jk}} + [\bm{M}(\bm{W})]_{jk} + \lambda =0\\
\iff &= -P_{gjk}+\widetilde{\beta}_{jk} \left( [\bm{M}(\bm{W})]_{jk} + \lambda)\right)
\end{align*}

\subsubsection{Compute $\Esp_h[\log h(Z_H\mid Z_O)]$}
Under $h$, $Z_H$ and $Z_O$ are independent according to the variational approximation. Therefore, $h(Z_H\mid Z_O) = h(Z_H)$ and this terms boils down to the entropy of $h(Z_H)$. More precisely:

\begin{align*}
\Esp_h[\log h(Z_H\mid Z_O)] &= \Esp_{h(Z_O)}\left[\Esp_{h(Z_H\mid Z_O)}[\log h(Z_H\mid Z_O)]\right]\\
&=\Esp_{h(Z_O)}\left[\Esp_{h(Z_H)}[\log h(Z_H)]\right]\\
&=\Esp_{h(Z_H)}[\log h(Z_H)] \\
&=-\entr[h(Z_H)].
\end{align*}


$Z_{Hi} \sim \mathcal{N}(m_{Hi},s_{ih})$ and $h$ is a product law on all $Z_{Hi}$. Therefore: 
\begin{align*}
\entr[h(Z_H)] &= \sum_i \entr[h(Z_{Hi})] \\
&=\frac{1}{2} \sum_i\log |s_{ih}| +n\times  \frac{r}{2}(1+\log(2\pi))\\
\end{align*}


$s_{ih}$ are diagonal. Hence:
\begin{align*}
\frac{1}{2} \sum_i\log |s_{ih}| &= \frac{1}{2} \sum_i \log \left(\prod_{h=1}^r s_{ih}[h,h] \right)\\
&= \frac{1}{2}\sum_{i=1}^n \sum_{h=1}^r \log(s_{ih}^2)
\end{align*}
Finally $$ \Esp_h[\log h(Z_H\mid Z_O)] = -\frac{1}{2}\sum_{i=1}^n \sum_{h=1}^r \log(s_{ih}^2) - \frac{rn}{2}(1+\log(2\pi)).$$

\paragraph{Derivatives}

\begin{empheq}[box=\lesswidefbox]{align*}
\partial_{s_{ih}^2} (\Esp_h[\log h(Z_H\mid Z_O)])&= -\frac{1}{2s_{ih}^2}
\end{empheq}
\subsubsection{Compute $ \Esp_{gh_O^*}[\log p(Z_O|T)]$}
\begin{align*}
 \Esp_{gh_O^*}[\log p(Z_O|T)] &=  \Esp_{gh_O^*}[ \frac{n}{2} \log |\Omega_{Tm}| - \frac{1}{2} Tr( \Omega_{Tm} Z_O^TZ_O) ]\\
 &= \frac{n}{2} \Esp_g[\log |\Omega_{Tm}|] - \frac{1}{2} < \Esp_g[\Omega_{Tm}], \Esp_{h_O^*}[ Z_O^TZ_O] > \\
&= \frac{n}{2} \Esp_g[\log |\Omega_{Tm}|] - \frac{1}{2} < \Esp_g[\Omega_{Tm}], \widetilde{M}_O^T\widetilde{M}_O + \widetilde{S}_O> 
\end{align*}

 $\Omega_{Tm}$ is the Schur complement of $\Sigma_{TO}$. Therefore its expectation can be decomposed as such:
 \begin{align*}
 \Esp_g[\Omega_{Tm}] &= \Esp_g[\Omega_{TO}] - \Esp_g[\Omega_{TOH}\Omega_{H}^{-1}\Omega_{THO}]\\
 &=P_g \odot \Omega_O - \mathcal{C}_g.
 \end{align*}
Because $\Omega_{Tm}$ is tree-structured, its determinent factorizes on the edges of $T$. More precisely as in \citet{robin2019}:
\begin{align*}
|\Omega_{Tm}| &= \prod_{k\in O} \omega_{kk} \prod_{kl \in T} \frac{\omega_{kk}\omega_{ll}-\omega_{kl}^2}{\omega_{kk}\omega_{ll}}\\
\log |\Omega_{Tm}|&= \sum_{k\in O} \log \omega_{kk} + \sum _{kl} \mathds{1}\{kl \in T\} \left[\log\frac{\omega_{kk}\omega_{ll}-\omega_{kl}^2}{\omega_{kk}\omega_{ll}}\right]\\
\Esp_g[\log |\Omega_{Tm}|]&= \sum _{kl} P_{gkl} \log \left[\frac{\omega_{kk}\omega_{ll}-\omega_{kl}^2}{\omega_{kk}\omega_{ll}}\right] + \sum_{k\in O} \log \omega_{kk}
\end{align*}

Finally we obtain:
\begin{align*}
 \Esp_{gh_O^*}[\log p(Z_O|T)] &= \frac{n}{2} \left(\sum _{kl} P_{gkl}  \log\left[\frac{\omega_{kk}\omega_{ll}-\omega_{kl}^2}{\omega_{kk}\omega_{ll}}\right]\right) - \frac{1}{2}< P_g \odot \Omega_O - \mathcal{C}_g, \widetilde{M}_O^T\widetilde{M}_O + \widetilde{S}_O> +\sum_{k\in O} \log \omega_{kk}
\end{align*}

In the end, $\Esp_{gh_O^*}[\log p(Z_O|T)]$ does not depend directly on either $M_H$, $S_H$ or $\widetilde{\beta}$ and thus does not need to be optimized.

\subsubsection{Final derivative and optimal estimators}
\paragraph{$M_H$:}
\begin{align*}
\partial_{M_H} KL &= M_H\Omega_H  + \widetilde{M}_O(P_{gOH}\odot\Omega_{OH}) =0\\
\iff& M_H =- \widetilde{M}_O(P_{gOH}\odot\Omega_{OH})\Omega_H^{-1}
\end{align*}
\paragraph{$S_H$:}
\begin{align*}
\partial_{s_{ih}^2} KL &=  \frac{\omega_{hh}}{2}- \frac{1}{2s_{ih}^2} = 0\\
\iff & s_{ih}^2 = \frac{1}{\omega_{hh}}
\end{align*}
\paragraph{$\widetilde{\beta}$:}
\begin{align*}
\partial_{\widetilde{\beta}} KL &= -P_{gjk}+\widetilde{\beta}_{jk} \left( [\bm{M}(\bm{W})]_{jk} + \lambda)\right) =0\\
\iff & \widetilde{\beta}_{jk}^{t+1} = P_{gjk}^{t+1}/\left([\bm{M}(\bm{W^t})]_{jk} + \lambda\right)
\end{align*}
 
\subsection{M step: optimizing in $(\beta, \Omega)$}
 
$$ \argmax_{\beta, \Omega} \mathcal{J}(Y ; g,h) =\argmax_{\beta, \Omega} \left\{ \Esp_{gh} [\log (p_\beta(T)p_{\Omega_T}(Z\mid T) ]\right\} $$

\subsubsection{General expression}
\begin{align*}
\log (p_\beta(T)p_{\Omega_T}(Z\mid T))  &= \sum_{kl} \mathds{1}\{kl \in T\} \log \beta_{kl} - \log B + \frac{n}{2}\log |\Omega_T| - \frac{1}{2}<\Omega_T,Z^\intercal Z>\\
\Esp_{gh} [\log (p_\beta(T)p_{\Omega_T}(Z\mid T) ] &= \sum_{kl} P_{gkl} \log\beta_{kl} +\frac{n}{2} \Esp_g[\log |\Omega_T|] -\frac{1}{2} <\Esp_g [\Omega_T], \Esp_h[Z^\intercal Z]>- \log B
\end{align*}

$\Omega_T$ is tree-strucutred, so its determinent is decomposable on the edges:
$$|\Omega_{T}| = \prod_{k} \omega_{kk} \prod_{kl \in T} \frac{\omega_{kk}\omega_{ll}-\omega_{kl}^2}{\omega_{kk}\omega_{ll}}$$

And we get

$$\Esp_g[\log |\Omega_{T}|]= \sum _{kl} P_{gkl} \left[ \log\left(\frac{\omega_{kk}\omega_{ll}-\omega_{kl}^2}{\omega_{kk}\omega_{ll}}\right)\right] + \sum_k \log \omega_{kk}$$

Finally, denoting $\psi_{kl} = \left(\frac{\omega_{kk}\omega_{ll}-\omega_{kl}^2}{\omega_{kk}\omega_{ll}}\right)^{n/2}$ we obtain:
\begin{align*}
\Esp_{gh} [\log (p_\beta(T)p_{\Omega_T}(Z\mid T) ] &=\sum_{kl} P_{gkl} \left(\log  \beta_{kl}\psi_{kl}\right) + \frac{n}{2}\sum_k \log \omega_{kk} - \frac{1}{2}<P_g \odot \Omega, \widetilde{M}^\intercal \widetilde{M} + \widetilde{S}>- \log B
\end{align*}

Here again, we add an identifiability constraint on the $\beta_{jk}$ via a $\lambda$ penalty. As the aim is to maximise the function, this time it is preceded by a minus sign:
\begin{align*}
\Esp_{gh} [\log (p_\beta(T)p_{\Omega_T}(Z\mid T) ] &=\sum_{kl} P_{gkl} \left(\log  \beta_{kl}\psi_{kl}\right) + \frac{n}{2}\sum_k \log \omega_{kk} - \frac{1}{2}<P_g \odot \Omega, \widetilde{M}^\intercal \widetilde{M} + \widetilde{S}>\\
& \;\;\;- \log B - \lambda(\sum_{jk} \beta_{jk} - 1 )
\end{align*}
\subsubsection{Update formulas}
\paragraph{Update of $\beta$:}
\begin{align*}
\partial_{\beta_{jk}} \Esp_{gh} [\log (p_\beta(T)p_{\Omega_T}(Z\mid T) ] &= \frac{P_{gjk}}{\beta_{jk}} - \frac{\partial_{\beta_{jk}} B }{B} + \lambda\\
&=\frac{P_{gjk}}{\beta_{jk}}  - [M(W)]_{jk} - \lambda\\
=0 \iff & \boxed{\widehat{\beta}_{jk} = P_{gjk} / ([M(W)]_{jk} + \lambda)}
\end{align*}\\


\paragraph{Update of $\Omega$:}

\[
 \partial_{\omega_{kl}}\left(\sum_{kl} P_{gkl} \left(\log  \beta_{kl}\psi_{kl}\right) +\frac{n}{2} \sum_k \log \omega_{kk}\right) =\begin{cases}
               -n \frac{P_{gkl} }{\omega_{kk}\omega_{ll}-\omega_{kl}^2}\times \omega_{kl} \text{\hspace{1cm}if $k\neq l$}\\\\
              \frac{n}{2} \times\frac{1}{\omega_{kk}} \left(\sum_l P_{gkl} \frac{\omega_{kl}^2}{(\omega_{kk}\omega_{ll}-\omega_{kl}^2)} + 1\right)\text{\hspace{0.5cm}if $k = l$}
            \end{cases}
\]
And
\begin{align*}
- \frac{1}{2} <P_g \odot \Omega, \underbrace{\widetilde{M}^\intercal \widetilde{M} + \widetilde{S}}_{n\widetilde{\Sigma}}> &=- \frac{n}{2}\times 2 \times \sum_{j<k}P_{gjk} \omega_{jk} \widetilde{\sigma}_{jk}- \frac{n}{2}\sum_{ k} \omega_{kk} \widetilde{\sigma}_{kk}
\end{align*}
The "$\times 2$" above comes from differenciating $j$ from $k$.
\begin{itemize}
\item We get the complete derivative for diagonal terms of $\Omega$:
\begin{align*}
 \partial_{\omega_{kk}}\Esp_{gh} [\log (p_\beta(T)p_{\Omega_T}(Z\mid T) ] &=\frac{n}{2} \times\frac{1}{\omega_{kk}} \left(\sum_l P_{gkl} \frac{\omega_{kl}^2}{(\omega_{kk}\omega_{ll}-\omega_{kl}^2)} + 1\right) -\frac{n}{2}\widetilde{\sigma}_{kk} =0\\
 \iff & \boxed{ \frac{1}{\omega_{kk}} \left(\sum_l P_{gkl} \frac{\omega_{kl}^2}{(\omega_{kk}\omega_{ll}-\omega_{kl}^2)} + 1\right) =\widetilde{\sigma}_{kk}}
\end{align*}
Then $\widehat{\omega}_{kk}$ is optained through numeric optimization.\\

\item The complete derivative for off-diagonal terms of $\Omega$ is :
\begin{align*}
 \partial_{\omega_{kl}}\Esp_{gh} [\log (p_\beta(T)p_{\Omega_T}(Z\mid T) ] &=-n \frac{P_{gkl} }{\omega_{kk}\omega_{ll}-\omega_{kl}^2}\times \omega_{kl} - n P_{gjk}  \widetilde{\sigma}_{jk} =0\\
 \iff & \widetilde{\sigma}_{jk} \omega_{kl}^2 - \omega_{kl} - \widetilde{\sigma}_{jk} \omega_{kk}\omega_{ll} = 0 \\
 \iff & \widehat{\omega}_{kl} = (1 \pm \sqrt{1+4\widetilde{\sigma}_{jk}^2 \omega_{kk}\omega_{ll}}) / 2\widetilde{\sigma}_{jk}
\end{align*}

To choose between roots, we use the constraint $\dfrac{\omega_{kl}^2}{\omega_{kk}\omega_{ll}} < 1$ which is equivalent to 
$$ (1 \pm \sqrt{1+4\widetilde{\sigma}_{jk}^2 \omega_{kk}\omega_{ll}}) < 4\widetilde{\sigma}_{jk}^2 \omega_{kk}\omega_{ll}.$$
As $\widetilde{\sigma}_{jk}^2 \omega_{kk}\omega_{ll} > 0$ by positive-definitiveness of $\Omega$, we have:
\begin{align*}
&1 - \sqrt{1+4\widetilde{\sigma}_{jk}^2 \omega_{kk}\omega_{ll}} < 1- \sqrt{4\widetilde{\sigma}_{jk}^2 \omega_{kk}\omega_{ll}}\\
\iff &(1 - \sqrt{1+4\widetilde{\sigma}_{jk}^2 \omega_{kk}\omega_{ll}})^2 < ( \sqrt{4\widetilde{\sigma}_{jk}^2 \omega_{kk}\omega_{ll}}-1)^2 < 4\widetilde{\sigma}_{jk}^2 \omega_{kk}\omega_{ll}
\end{align*}

On the other hand, we also have:
\begin{align*}
&(1 + \sqrt{1+4\widetilde{\sigma}_{jk}^2 \omega_{kk}\omega_{ll}})^2 >4\widetilde{\sigma}_{jk}^2 \omega_{kk}\omega_{ll}.
\end{align*}

Therefore: $$\boxed{ \widehat{\omega}_{kl} = (1 - \sqrt{1+4\widetilde{\sigma}_{jk}^2 \omega_{kk}\omega_{ll}}) / 2\widetilde{\sigma}_{jk}}$$

\end{itemize}


\section*{Remarks}
\begin{itemize}
\item sign inversion of $\lambda$ between VE and M step 
\item test push
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\newpage
\bibliographystyle{apsrev} %tested plainnat
\bibliography{bibi}

\end{document}