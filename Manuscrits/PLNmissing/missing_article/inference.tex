\section{Variational inference}

\subsection{Likelihood approximation}
 \paragraph{Likelihood}
 The joint likelihood of observed and hidden data factorizes as follows:
\begin{align*}
p(\Ybf,\Zbf,T)& = p_{\betabf}(T) \: p_{\Omegabf_T}(\Zbf \mid T) \: p_ {\thetabf}(\Ybf \mid \Zbf) \\
% &= p(T)\: p(Z_O,Z_H \mid T) \: p(\Ybf \mid Z_O,Z_H) \\
&= p_{\betabf}(T) \: p_{\Omegabf_T}(\Zbf_O \mid T) \: p_{\Omegabf_T}(\Zbf_H  \mid  \Zbf_O,T)  \: p_{\thetabf}(\Ybf \mid \Zbf_O)
\end{align*} 
where$\betabf$ is the matrix containing edges weights $\beta_{jk}$. This factorization is suggested by the graphical model on Figure \ref{MG}b).  Obtaining maximum likelihood estimators implies the evaluation of the law of the hidden covariates conditional on the observed data, namely $p(T, \Zbf \mid \Ybf)$. 

\paragraph{Variational approximation.}
 Here $p(T, \Zbf \mid \Ybf)$ has no simple form and we thus adopt a variational approximation which aims to miximise a lower bound of the log-likelihood of observed data $\log p(\Ybf)$, namely
\begin{align*}
    \mathcal{J}(\Ybf; g,h)
    & = \log p(\Ybf) - KL\left[q(T,\Zbf) \middle\vert\middle\vert\ p(T,\Zbf \mid \Ybf)\right]\\
    & =\Esp_{q}\big[\log p_{\thetabf}(\Ybf\mid \Zbf_O)+ \log p_{\Omegabf_T}(\Zbf_O \mid T)\big]-\Esp_q[\log h(\Zbf_O)]  +\Esp_{q}[\log p_{\Omegabf_T}(\Zbf_H \mid \Zbf_O,T)] \\
    & \;\; +\Esp_q\big[\log p_{\betabf}(T) - \log g(T)\big]  -\Esp_q[\log h(\Zbf_H)]
\end{align*}
 where $KL\left[q(T,\Zbf) \middle\vert\middle\vert\ p(T,\Zbf \mid \Ybf)\right]$ is the KÃ¼llback-Leibler  divergence between the approximated distribution  $q(T,\Zbf)$  et the true conditional distribution $p(T, \Zbf \mid \Ybf)$.

\paragraph{Approximated distributions.}
The approximation we adopt relies on factorizing $q$ in the product of two distributions $h$ et $g$ relating respectively to $\Zbf$  and $T$: 
$$p(T,Z | \Ybf) \approx  q(Z,T) = g(T)h(Z).$$

 A consequence of this hypothesis is the contributions of   $\Esp_h[\log p(\Zbf\mid T)]$ and $\log p(T)$ --~only terms involving $T$ in the lower bound~-- writing as sums on the edges. As distribution   $g$ minimizes the KL divergence, it has to factorizes on the edges as well. Hence:
$ g(T) = \left(\prod_{kl} \widetilde{\beta}_{kl} \right) / \widetilde{B}$. \\
 
Moreover, it follows from the independence of the samples that $h$ is a product law: $ h(\Zbf) = \prod_i h_i(\Zbf_i)$.
Our variational approximation consists in assuming that each of its components is a multivariate Gaussian: $h(\Zbf_i) = \Ncal(\Zbf_i; \mbf_i, \Sbf_i)$, where $\Sbf_i$ is diagonal. 

\subsection{Algorithm.}

\paragraph{Optimization in $\thetabf$ and $h(\Zbf_O)$.}
To optimize  the lower bound $\bound(\Ybf; g,h)$, we take advantage of the VEM algorithm implemented by \citet{CMR18,PLNnetwork} in package \url{PLNmodels}. It is indeed possible to write $\bound(\Ybf; g,h)$ such as the lower bound of a simple PLN model appears, denoted $\bound_{PLN}$. We can write:
\begin{align*}
\mathcal{J}(\Ybf; g,h) \leq & \mathcal{J}_{PLN}(\Ybf; h(\Zbf_O), \thetabf) \\
& + \Esp_{gh}[\log p(\Zbf_H \mid \Zbf_O,T) ]+\Esp_g[\log p(T)-\log g(T)]-\Esp_h[\log h(\Zbf_H)].
\end{align*}
The VE step of the \url{PLNmodels} algorithm maximizes $\mathcal{J}_{PLN}$ and provides with moments of first and second order of the observed part of the approximate law $h$: $\mbf_{Oi}$ et $\Sbf_{Oi}$. Taking these estimators for the rest of the optimization allows to miximize $\mathcal{J}(\Ybf;g,h)$ in $\thetabf$ and partly in $h(\Zbf_O)$. There is still a dependence in $\Zbf_O$ in $\Esp_{gh}[\log p(\Zbf_H \mid \Zbf_O,T) ]$, it is therefore a sub-optimal solution in $h(\Zbf_O)$.

\paragraph{VE step: optimizing in $g$ and $h(\Zbf_H)$.}
The VE step minimizes the divergence between the approximate and conditional distribution:
$$  \argmin_{g,h(\Zbf_H)} KL\left(g(T)h(\Zbf) \mid\mid p(\Zbf,T\mid \Ybf)\right)$$

The update of  weights $\widetilde{\beta}_{jk}$ corresponding to edges linking two observed nodes (resp. one observed and one hidden) is obtained by cancelling out the terms of the divergence depending on edges between two observed nodes (resp. one observed and one hidden).
Derivating the expression with parameters of $h(\Zbf_O)$ previously optimized gives the update for  $h(\Zbf_H)$.
 
\paragraph{M step: optimize in $\beta$ and $\Omega_T$.}
The M step maximizes the lower bound  $\mathcal{J}(\Ybf ; g,h)$ in the parameters of $p(\Zbf,T\mid \Ybf)$ which have not yet been optimized, namely in $\betabf$ and $\Omegabf_T$:
$$ \argmax_{\betabf, \Omegabf_T} \mathcal{J}(\Ybf ; g,h) =\argmax_{\betabf, \Omegabf_T} \left\{ \Esp_{gh} [\log (p_{\betabf}(T) p_{\Omegabf_T}(\Zbf\mid T))]\right\} $$

The tree distribution is updated through  $\betabf$ using the formula detailed in  \citet{MRA20}. The update for the off-diagonal terms of   $\Omegabf_T$  is the same as in  \citet{genvieve}; the diagonal is not updated.